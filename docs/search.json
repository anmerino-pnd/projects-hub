[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portafolio de Ciencia de Datos e IA",
    "section": "",
    "text": "Bienvenido a mi portafolio profesional. Soy Angel Alberto Merino Cedeño, estudiante de Maestría en Ciencia de Datos e Ingeniero Químico con una fuerte especialización en el desarrollo de Sistemas de Inteligencia Artificial Generativa, Chatbots RAG y Arquitecturas de Datos.\nAquí encontrarás documentación técnica detallada sobre mis implementaciones más recientes en entornos empresariales reales."
  },
  {
    "objectID": "index.html#proyectos-destacados",
    "href": "index.html#proyectos-destacados",
    "title": "Portafolio de Ciencia de Datos e IA",
    "section": "Proyectos Destacados",
    "text": "Proyectos Destacados\n\n\nHelp Desk Inteligente (CENACE)\nSistema de soporte técnico basado en RAG y Modelos de Lenguaje (LLMs) para el Centro Nacional de Control de Energía. Este proyecto utiliza modelos open-source (Gemma/Ollama) para clasificar incidentes y sugerir soluciones a partir de documentación técnica interna, garantizando la privacidad de datos sensibles.\nTecnologías: Python, LangChain, Ollama, MongoDB, FastAPI.\nVer Documentación Completa\n\n\nChatbot Financiero (Salsas Castillo)\nAsistente conversacional desplegado en Telegram para optimizar el acceso a información de ventas y finanzas. Integra bases de datos SQL (PostgreSQL) para consultas transaccionales en tiempo real y Vector Stores para consultas sobre normativas internas, incluyendo generación automática de reportes PDF.\nTecnologías: OpenAI (GPT-4), PostgreSQL, FAISS, Telegram API.\nVer Documentación Completa\n\n\nRecomendador de Productos (CT Internacional)\nChatbot avanzado para la recomendación de productos de hardware. Implementa un ciclo ETL complejo para sincronizar bases de datos MySQL con un índice vectorial. Incluye un sistema de moderación de usuarios, manejo de “mixed content” mediante proxy PHP y despliegue web mediante Widget personalizado.\nTecnologías: MySQL, Redis, Docker/Podman, JavaScript Widget, RAG.\nVer Documentación Completa"
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "Este proyecto se centra en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información de ventas y finanzas, y mejorar la comunicación interna. El chatbot utiliza un sistema de recuperación aumentada con generación (RAG) y herramientas dinámicas para proporcionar respuestas precisas y relevantes, así como la capacidad de generar reportes financieros.\n\n\n\nAutomatizar consultas: Permitir a los usuarios (operativos y no operativos) obtener información sobre ventas, productos y finanzas de manera rápida y eficiente a través de una interfaz conversacional en Telegram.\nMejorar la toma de decisiones: Proporcionar análisis de datos financieros y de ventas en tiempo real, incluyendo la generación de reportes en PDF.\nOptimizar la gestión de información: Centralizar el acceso a datos transaccionales y consolidados, reduciendo la dependencia de consultas manuales.\nAdaptación a nuevas tecnologías: Implementar un sistema basado en LLMs y bases de datos vectoriales para un enfoque moderno y escalable.\n\n\n\n\n\nIncremento en la eficiencia operativa al reducir el tiempo dedicado a la búsqueda manual de información.\nMejora en la precisión de los datos y análisis disponibles para el personal.\nFacilitación de la toma de decisiones estratégicas basadas en información actualizada."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html#introducción-al-proyecto",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html#introducción-al-proyecto",
    "title": "Documentación",
    "section": "",
    "text": "Este proyecto se centra en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información de ventas y finanzas, y mejorar la comunicación interna. El chatbot utiliza un sistema de recuperación aumentada con generación (RAG) y herramientas dinámicas para proporcionar respuestas precisas y relevantes, así como la capacidad de generar reportes financieros.\n\n\n\nAutomatizar consultas: Permitir a los usuarios (operativos y no operativos) obtener información sobre ventas, productos y finanzas de manera rápida y eficiente a través de una interfaz conversacional en Telegram.\nMejorar la toma de decisiones: Proporcionar análisis de datos financieros y de ventas en tiempo real, incluyendo la generación de reportes en PDF.\nOptimizar la gestión de información: Centralizar el acceso a datos transaccionales y consolidados, reduciendo la dependencia de consultas manuales.\nAdaptación a nuevas tecnologías: Implementar un sistema basado en LLMs y bases de datos vectoriales para un enfoque moderno y escalable.\n\n\n\n\n\nIncremento en la eficiencia operativa al reducir el tiempo dedicado a la búsqueda manual de información.\nMejora en la precisión de los datos y análisis disponibles para el personal.\nFacilitación de la toma de decisiones estratégicas basadas en información actualizada."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "2. Manual de instalación y despliegue",
    "text": "2. Manual de instalación y despliegue\nEsta sección detalla los requisitos, dependencias y pasos para la instalación y despliegue del chatbot de Salsas Castillo.\n\n2.1. Configuraciones importantes\n\nEl backend del chatbot está desarrollado con FastAPI y se espera que se ejecute en un entorno con Python 3.12.\nRequiere conectividad a una instancia de MongoDB para la gestión de sesiones e historial, y a una base de datos PostgreSQL para datos financieros y de ventas.\nUtiliza modelos de lenguaje de OpenAI (requiere OPENAI_API_KEY) y potencialmente modelos open-source como gemma3:4b a través de Ollama para la moderación.\nLas credenciales sensibles se gestionan a través de un archivo .env.\nLa integración con Telegram se realiza mediante webhooks y el telegram_token correspondiente.\n\n\n\n2.2. Requisitos del sistema\n\nPython: Versión 3.x (se recomienda la versión utilizada en el desarrollo, ej., 3.12.9).\nPip/UV: Última versión para la gestión de paquetes.\nOllama: Instalado y en ejecución si se utilizan modelos open-source para moderación.\nMongoDB: Acceso remoto configurado para las colecciones de sesiones e historial de mensajes.\nPostgreSQL: Acceso remoto configurado para las bases de datos historial_facturas y financieroii.\nConexión a Internet: Necesaria para interactuar con las APIs de OpenAI y Telegram.\n\n\n\n2.3. Dependencias principales del sistema\n\nfastapi: Framework web para el backend.\nlangchain: Framework principal para la orquestación del LLM y las herramientas.\nopenai: Cliente Python para la API de OpenAI.\npymongo: Driver para la interacción con MongoDB.\npsycopg2-binary: Adaptador PostgreSQL para Python.\nfpdf: Para la generación de PDFs.\nrequests: Para realizar solicitudes HTTP (ej., a Telegram, OpenAI Whisper).\nuvicorn/gunicorn: Servidor WSGI para despliegue.\npydantic: Para la validación de modelos de datos.\nfaiss-cpu: Para la base de datos vectorial (si aplica para documentos internos).\n\n\n\n2.4. Instalación del Backend (API)\n\nClonar el repositorio:\n\ngh repo fork Macrodata-Analitica/castilloChatbot\ncd castilloChatbot\n\nCrear entorno virtual e instalar dependencias:\n\npip install uv # Si no está instalado\nuv venv\nsource .venv/bin/activate # Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\nuv sync # Sincroniza los modulos de src/\n\nConfigurar variables de entorno (.env):\n\nAsegúrate de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos:\nOPENAI_API_KEY=\"\"\n\nVERIFY_TOKEN=\"\"\nTELEGRAM_BOT_TOKEN=\"\"\nPHONE_NUMBER_ID=\"\"\n\nHOST=\"\"\nPORT=\"\"\nUSER=\"\"\nPASS=\"\"\nDBNAME=\"\"\nSCHEMA=\"\"\n\nLevantar el backend:\n\n\nDesarrollo:\n\nuvicorn salsasllm.API.telegram_main:app --reload\n\nProducción:\n\nnohup gunicorn salsasllm.API.telegram_main:app --workers 4 --bind 0.0.0.0:8000 -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile - --error-logfile - &\n\nConfigurar Webhook de Telegram:\n\nAsegúrate de que Telegram envíe los mensajes a tu endpoint /webhook. Esto se hace una vez a través de la API de Telegram:\ncurl -F \"url=https://&lt;TU_DOMINIO&gt;/tgbot/webhook\" https://api.telegram.org/bot&lt;TU_TELEGRAM_TOKEN&gt;/setWebhook\nReemplaza  y ."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html#documentación-técnica-del-código",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "3. Documentación Técnica del Código",
    "text": "3. Documentación Técnica del Código\nEsta sección describe la estructura modular del proyecto y las funciones clave de sus componentes.\n\n3.1. Estructura de Carpetas y Módulos\nEl proyecto sigue una estructura modular para organizar el código:\n\nsalsasllm/API/telegram_main.py: Archivo principal de la aplicación FastAPI. Configura la aplicación, CORS y registra los endpoints para el chat de Telegram y los logs.\nsalsasllm/API/telegram_chat.py: Contiene la lógica para manejar los webhooks de Telegram, transcribir audio (usando Whisper), enviar mensajes y coordinar con el agente principal.\nsalsasllm/langchain/agent_multitool.py: Implementa la lógica principal del agente conversacional (AgentMultiTools). Gestiona la interacción con herramientas externas (SQL, PDF, búsqueda de información), el historial de conversación en MongoDB y la conexión con el LLM.\nsalsasllm/langchain/vectorstore.py: Implementa la lógica para la creación, carga y consulta de la base de datos vectorial FAISS.\nsalsasllm/tools/search_information.py: Define la herramienta search_information para buscar información relevante en documentos internos (a través del vector store).\nsalsasllm/tools/pdf_tool.py: Define la herramienta generate_financial_report_pdf para crear reportes en PDF a partir de datos tabulares y enviarlos por Telegram.\nsalsasllm/langchain/prompts.py: Contiene las definiciones de los prompts del sistema (prompt_v1, prompt_v2) que guían el comportamiento del LLM.\nsalsasllm/settings/clientes.py: Archivo para la configuración de credenciales (API keys, datos de conexión a DBs, tokens).\nsalsasllm/settings/config.py: Archivo para configuraciones generales del proyecto (ej., rutas de índices FAISS, whitelist de Telegram).\n\n\n\n3.2. Modelos LLM Utilizados\nEl sistema de Salsas Castillo utiliza una combinación de modelos de lenguaje para diferentes propósitos:\n\nGeneración de Respuestas y Razonamiento (OpenAI - gpt-5):\n\nFunción: Es el LLM principal utilizado por AgentMultiTools para interpretar las consultas del usuario, decidir qué herramientas invocar (SQL, búsqueda de información, PDF) y generar las respuestas detalladas.\nVentaja: Ofrece alta capacidad de razonamiento y comprensión contextual para manejar consultas complejas sobre datos financieros y de ventas.\n\nTranscripción de Audio (OpenAI Whisper - whisper-1):\n\nFunción: Utilizado en telegram_chat.py para transcribir mensajes de voz de los usuarios de Telegram a texto, permitiendo que el chatbot procese entradas de audio.\nVentaja: Alta precisión en la transcripción de voz a texto.\n\nGeneración de Análisis para PDF (OpenAI - gpt-5): `\n\nFunción: En pdf_tool.py, este modelo se utiliza para generar un párrafo de análisis conciso a partir de los datos tabulares que se incluirán en el reporte PDF.\nVentaja: Proporciona resúmenes inteligentes y profesionales de los datos.\n\n\n\n\n3.3. Puntos de entrada y funciones clave\nEstos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot:\n\ntelegram_main.py::telegram_webhook_handler:\n\n@app.post(\"/webhook\")\nasync def telegram_webhook_handler(request: Request, background_tasks: BackgroundTasks):\n\nPropósito: Es el endpoint de FastAPI que recibe todos los mensajes y actualizaciones de Telegram. Actúa como el punto de entrada principal para las interacciones del usuario.\nComportamiento: Recibe el payload de Telegram, extrae el chat_id y el mensaje (texto o voz), verifica si el usuario está en la whitelist y delega el procesamiento a handle_message en segundo plano.\ntelegram_chat.py::handle_message:\n\ndef answer(self, question: str = None, session_id: str = None, name: str = None):\n\nPropósito: Es la función central que orquesta la respuesta del chatbot. Recibe la pregunta del usuario, gestiona el historial de la sesión, invoca el AgentExecutor de LangChain y maneja la salida.\nComportamiento: 1. Asegura la existencia de la sesión en MongoDB (ensure_session).\n\n2. Recupera y trunca el historial de la sesión (`get_session_history`, `trim_history`).\n\n3. Invoca al AgentExecutor con la pregunta y el historial, permitiendo que el LLM decida qué herramientas usar (SQL, `search_information`, `pdf_report_tool`).\n\n4. Registra la interacción completa en MongoDB (`add_message`, `add_message_backup`).\n\n5. Si la respuesta es un PDF, coordina su envío a Telegram.\n\npdf_tool.py::generate_financial_report_pdf:\n\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n\nPropósito: Genera un reporte en formato PDF a partir de datos tabulares proporcionados y un análisis generado por un LLM, y lo envía al usuario de Telegram.\nComportamiento: Utiliza fpdf para crear el PDF, get_llm_analysis para obtener un resumen del LLM y enviar_pdf_por_telegram para enviar el archivo.\nvectorstore.py::LangchainVectorStore.create_index:\n\ndef create_index(self, docs):\n\nPropósito: Crea un nuevo índice FAISS a partir de una lista de documentos.\nComportamiento: Utiliza FAISS.from_documents para generar el índice y lo guarda localmente."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "4. Guía de Entrenamiento y Mejora",
    "text": "4. Guía de Entrenamiento y Mejora\nEsta sección aborda cómo se mantiene y mejora la base de conocimientos del chatbot, así como recomendaciones para futuras optimizaciones.\n\n4.1. Generación y Actualización de la Base de Datos Vectorial\nLa herramienta search_information se basa en un vector store FAISS. Este vector store almacena representaciones vectoriales de documentos internos (manuales, reglamentos, etc.) para permitir búsquedas semánticas.\nProceso de Creación: Los documentos internos se convierten en objetos langchain.schema.Document, se generan embeddings utilizando OpenAIEmbeddings, y luego se construye un índice FAISS que se guarda localmente.\nActualización: Para mantener la información actualizada, se debe ejecutar periódicamente el script que reconstruye o actualiza este vector store con cualquier nuevo documento o modificación.\n\n\n4.2. Recomendaciones para Futura Mejora\n\nMonitoreo Avanzado: Implementar un monitoreo más detallado de las interacciones del chatbot, incluyendo el rendimiento de las consultas SQL, el tiempo de respuesta de las herramientas y la calidad de las respuestas generadas por el LLM. Esto puede hacerse analizando los datos en la colección message_backup de MongoDB.\nOptimización de Prompts: Continuar iterando y refinando los prompts (prompts.py) para mejorar la precisión y coherencia de las respuestas, especialmente en casos complejos o ambiguos.\nManejo de Errores Robustos: Mejorar el manejo de errores en las llamadas a APIs externas (OpenAI, Telegram) y a las bases de datos (PostgreSQL, MongoDB) para proporcionar mensajes más informativos al usuario y facilitar la depuración.\nExpansión de Herramientas: Considerar la adición de nuevas herramientas para el agente, como la capacidad de crear gráficos a partir de datos financieros, o interactuar con otros sistemas internos de Salsas Castillo.\nEvaluación Cuantitativa: Si es posible, definir métricas cuantitativas para evaluar la calidad de las respuestas del chatbot (ej., ROUGE, BLEU, o métricas basadas en la satisfacción del usuario) para complementar la evaluación cualitativa.\nEmbeddings Locales (Opcional): Investigar el uso de modelos de embeddings open-source (ej., a través de Ollama) para reducir la dependencia de OpenAI y potencialmente los costos, si la precisión es aceptable para los casos de uso de Salsas Castillo."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/5_documentacion.html#arquitectura",
    "href": "salsas_castillo_chatbot/quarto/5_documentacion.html#arquitectura",
    "title": "Documentación",
    "section": "5. Arquitectura",
    "text": "5. Arquitectura\n\n5.1. Componentes Clave:\n\nUsuario de Telegram: El usuario final que interactúa con el chatbot a través de la aplicación de mensajería.\nBackend del Chatbot (FastAPI): El servicio principal que procesa las consultas de los usuarios.\n\ntelegram_main.py: Punto de entrada de los webhooks de Telegram.\ntelegram_chat.py: Maneja la lógica de Telegram (transcripción de voz, envío de mensajes) y coordina preguntas y respuestas con el agente.\nagent_multitool.py: Contiene el AgentMultiTools que orquesta el LLM y las herramientas.\n\nModelo agéntico: El cerebro principal que utiliza un LLM para generar respuestas, razonar y decidir el uso de herramientas.\nBase de Datos NoSQL (MongoDB): Almacena el historial de sesiones (sessions) y un respaldo completo de mensajes (message_backup) para análisis.\nBase de Datos Relacional (PostgreSQL): Contiene los datos transaccionales de ventas (historial_facturas) y datos financieros consolidados (financieroii).\nBase de Datos Vectorial (FAISS): Almacena los embeddings de documentos internos para búsquedas semánticas (utilizado por search_information).\nHerramientas: Funciones específicas que el LLM puede invocar:\n\nsql_db_query, sql_db_schema, etc. (de SQLDatabaseToolkit): Para interactuar con PostgreSQL.\nsearch_information: Para buscar en el vector store de documentos internos.\npdf_report_tool: Para generar y enviar reportes PDF.\n\n\n\n\n5.2. Flujo de Interacción Principal:\n\nEl Usuario de Telegram envía un mensaje (texto o voz) al chatbot.\nEl mensaje es recibido por la API y enviado al endpoint /webhook del Backend del Chatbot.\ntelegram_chat.py procesa el mensaje. Si es voz, lo envía a OpenAI Whisper API para transcripción.\nEl texto del mensaje se pasa a AgentMultiTools (agent_multitool.py).\nAgentMultiTools gestiona la sesión en MongoDB y consulta el historial.\nEl LLM, guiado por los prompts (prompts.py), analiza la consulta y decide qué Herramientas utilizar:\n\nSi necesita datos de ventas, finanzas, facturas, u otras tablas, invoca herramientas SQL para consultar la base de datos de PostgreSQL.\nSi necesita información de documentos internos, invoca search_information para buscar en la Base de Datos Vectorial (FAISS).\nSi se solicita un reporte, invoca pdf_report_tool, que a su vez puede usar el LLM para análisis y luego envía el PDF a Telegram.\nSi la consulta se trata sobre una visualización de datos, se invoca table_tool, toma los datos devueltos por la consulta SQL, los convierte a tabla, lo guarda temporalmente como imagen y lo envía al usuario.\n\nLa información recuperada por las herramientas se contextualiza y se envía de nuevo al LLM para generar la respuesta final al usuario.\nLa respuesta es enviada de vuelta al Usuario de Telegram a través de la API.\nTodas las interacciones (consultas y respuestas) se registran en las colecciones sessions y message_backup en MongoDB para análisis y auditoría.\n\n\n\n5.3. Diagrama de la Arquitectura\nEl siguiente diagrama ilustra la arquitectura general del sistema del chatbot para Salsas Castillo, mostrando los componentes principales y el flujo de datos.\n\n\n\nArquitectura del sistema"
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/3_modelado.html",
    "href": "salsas_castillo_chatbot/quarto/3_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema del chatbot, integrando modelos de lenguaje, herramientas de acceso a datos y mecanismos de interacción.\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases, facilitando el mantenimiento y la escalabilidad. La librería principal utilizada es LangChain, que permite orquestar la interacción entre el LLM y diversas herramientas.\nLa arquitectura se centra en un modelo agéntico (AgentMultiTools) que actúa como el cerebro del chatbot.\n\n\nEl agente tiene acceso a un conjunto de herramientas dinámicas que le permiten consultar datos actualizados en tiempo real o realizar acciones específicas:\n\nHerramienta RAG: search_info_tool: Realiza búsquedas semánticas en el vector store de documentos internos.\n\n@tool(description=\"Busca información de documentos, manuales, reglamentos, etc. Devuelve un resumen de la información relevante.\")\ndef search_information(query: str) -&gt; str:\n    # ... lógica de búsqueda en vector store ...\n\nHerramientas SQL: Permiten al agente interactuar directamente con la base de datos PostgreSQL. Estas incluyen:\nsql_db_query: Para ejecutar consultas SQL y obtener resultados.\nsql_db_schema: Para obtener el esquema de las tablas y entender su estructura.\nsql_db_list_tables: Para listar las tablas disponibles.\n\n@tool(description=\"Cuando te pidan generar un reporte financiero en PDF a partir de datos generados previamente.\")\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n    # ... lógica de generación de PDF y envío a Telegram ...\nHerramientas de Generación*\n\npdf_report_tool: Genera reportes financieros en PDF a partir de datos tabulares y los envía al usuario.\ntable_tool: Cuando se trate simplemente de una visualización de datos, estos se presentan mediante una tabla convertida en imagen. Facilitando la comprensión y visualización de la información.\n\nEstas herramientas son invocadas automáticamente por el agente cuando el LLM determina que son necesarias para responder a la consulta del usuario.\n\n\n\nEl sistema se alimenta de información de la siguiente manera:\n\nConsultas de Usuario: La pregunta del usuario es el punto de entrada.\nHistorial de Conversación: El historial se gestiona en MongoDB (sessions collection) y se trunca para ajustarse a la ventana de contexto del LLM.\nLLM: Interpreta la consulta y decide qué herramientas usar.\nHerramientas SQL: Si la consulta requiere datos de la base de datos, el LLM genera una consulta SQL que se ejecuta en PostgreSQL. Los resultados se devuelven al LLM.\nHerramienta de Búsqueda de Información: Si la consulta es sobre documentos internos, se busca en el vector store (FAISS) y la información relevante se devuelve al LLM para generar la respuesta.\nHerramienta de PDF: Si se solicita un reporte, se genera el PDF con el análisis del LLM y se envía. Estos análisis pueden incluir gráficas o no.\nGeneración de Respuesta: El LLM sintetiza la información que devuelven las herramientas y genera la respuesta final al usuario.\n\n\n\n\n\nEl LLM opera con los siguientes atributos y contexto:\n\nquestion: La consulta directa del usuario.\nchat_id | session_id: ID de Telegram del usuario, necesario para enviar mensajes y PDFs directamente a Telegram. También es el identificador único de la sesión del usuario, crucial para mantener el historial de conversación.\nname: Nombre del usuario de Telegram, utilizado para personalizar la interacción.\nchat_history: Historial de mensajes previos de la sesión, truncado para optimizar el contexto.\n\n\n\n\nEl sistema de Salsas Castillo utiliza estratégicamente varios modelos de lenguaje:\n\nGPT (OpenAI):\n\nFunción principal: Es el modelo central para el razonamiento del agente, la interpretación de consultas complejas y la generación de respuestas detalladas. Decide cuándo y cómo usar las herramientas SQL, de búsqueda de información y de PDF. También se utiliza para generar el análisis textual en los reportes PDF.\nVentaja: Alta capacidad de comprensión, razonamiento y generación de texto coherente y preciso, fundamental para análisis financiero y de ventas. Actualmente usamos el modelo GPT-5 como modelo central pero también usamos el modelo GPT-4.1 en ciertas herramientas por su gran ventana de contexto que permite manejar mayor cantidad de información.\n\nWhisper-1 (OpenAI):\n\nFunción: Utilizado para la transcripción de mensajes de voz de los usuarios de Telegram a texto.\nVentaja: Excelente precisión en la conversión de audio a texto, lo que permite una interacción más flexible con el chatbot."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/3_modelado.html#modelado",
    "href": "salsas_castillo_chatbot/quarto/3_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema del chatbot, integrando modelos de lenguaje, herramientas de acceso a datos y mecanismos de interacción.\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases, facilitando el mantenimiento y la escalabilidad. La librería principal utilizada es LangChain, que permite orquestar la interacción entre el LLM y diversas herramientas.\nLa arquitectura se centra en un modelo agéntico (AgentMultiTools) que actúa como el cerebro del chatbot.\n\n\nEl agente tiene acceso a un conjunto de herramientas dinámicas que le permiten consultar datos actualizados en tiempo real o realizar acciones específicas:\n\nHerramienta RAG: search_info_tool: Realiza búsquedas semánticas en el vector store de documentos internos.\n\n@tool(description=\"Busca información de documentos, manuales, reglamentos, etc. Devuelve un resumen de la información relevante.\")\ndef search_information(query: str) -&gt; str:\n    # ... lógica de búsqueda en vector store ...\n\nHerramientas SQL: Permiten al agente interactuar directamente con la base de datos PostgreSQL. Estas incluyen:\nsql_db_query: Para ejecutar consultas SQL y obtener resultados.\nsql_db_schema: Para obtener el esquema de las tablas y entender su estructura.\nsql_db_list_tables: Para listar las tablas disponibles.\n\n@tool(description=\"Cuando te pidan generar un reporte financiero en PDF a partir de datos generados previamente.\")\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n    # ... lógica de generación de PDF y envío a Telegram ...\nHerramientas de Generación*\n\npdf_report_tool: Genera reportes financieros en PDF a partir de datos tabulares y los envía al usuario.\ntable_tool: Cuando se trate simplemente de una visualización de datos, estos se presentan mediante una tabla convertida en imagen. Facilitando la comprensión y visualización de la información.\n\nEstas herramientas son invocadas automáticamente por el agente cuando el LLM determina que son necesarias para responder a la consulta del usuario.\n\n\n\nEl sistema se alimenta de información de la siguiente manera:\n\nConsultas de Usuario: La pregunta del usuario es el punto de entrada.\nHistorial de Conversación: El historial se gestiona en MongoDB (sessions collection) y se trunca para ajustarse a la ventana de contexto del LLM.\nLLM: Interpreta la consulta y decide qué herramientas usar.\nHerramientas SQL: Si la consulta requiere datos de la base de datos, el LLM genera una consulta SQL que se ejecuta en PostgreSQL. Los resultados se devuelven al LLM.\nHerramienta de Búsqueda de Información: Si la consulta es sobre documentos internos, se busca en el vector store (FAISS) y la información relevante se devuelve al LLM para generar la respuesta.\nHerramienta de PDF: Si se solicita un reporte, se genera el PDF con el análisis del LLM y se envía. Estos análisis pueden incluir gráficas o no.\nGeneración de Respuesta: El LLM sintetiza la información que devuelven las herramientas y genera la respuesta final al usuario.\n\n\n\n\n\nEl LLM opera con los siguientes atributos y contexto:\n\nquestion: La consulta directa del usuario.\nchat_id | session_id: ID de Telegram del usuario, necesario para enviar mensajes y PDFs directamente a Telegram. También es el identificador único de la sesión del usuario, crucial para mantener el historial de conversación.\nname: Nombre del usuario de Telegram, utilizado para personalizar la interacción.\nchat_history: Historial de mensajes previos de la sesión, truncado para optimizar el contexto.\n\n\n\n\nEl sistema de Salsas Castillo utiliza estratégicamente varios modelos de lenguaje:\n\nGPT (OpenAI):\n\nFunción principal: Es el modelo central para el razonamiento del agente, la interpretación de consultas complejas y la generación de respuestas detalladas. Decide cuándo y cómo usar las herramientas SQL, de búsqueda de información y de PDF. También se utiliza para generar el análisis textual en los reportes PDF.\nVentaja: Alta capacidad de comprensión, razonamiento y generación de texto coherente y preciso, fundamental para análisis financiero y de ventas. Actualmente usamos el modelo GPT-5 como modelo central pero también usamos el modelo GPT-4.1 en ciertas herramientas por su gran ventana de contexto que permite manejar mayor cantidad de información.\n\nWhisper-1 (OpenAI):\n\nFunción: Utilizado para la transcripción de mensajes de voz de los usuarios de Telegram a texto.\nVentaja: Excelente precisión en la conversión de audio a texto, lo que permite una interacción más flexible con el chatbot."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/3_modelado.html#evaluación",
    "href": "salsas_castillo_chatbot/quarto/3_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa evaluación del chatbot de Salsas Castillo se centra en la calidad y precisión de sus respuestas, dada la naturaleza de las consultas financieras y de ventas.\n\n2.1. Criterios de Evaluación\nLa evaluación se realiza mediante un análisis cualitativo, considerando los siguientes criterios:\n\nPrecisión de los Datos: La información proporcionada (cifras de ventas, costos, nombres de productos) debe coincidir exactamente con los datos de las bases de datos SQL.\nCoherencia y Relevancia: Las respuestas deben ser lógicas, directas y pertinentes a la pregunta del usuario, evitando “alucinaciones” o información incorrecta.\nCapacidad de Análisis: Para consultas que requieren análisis (ej., tendencias de ventas, márgenes), la respuesta debe ser comprensible y destacar las conclusiones clave.\nGeneración de Reportes: Los PDFs generados deben ser comprensibles, contener los datos solicitados y el análisis del LLM debe ser claro y profesional.\nManejo de Ambigüedad: La capacidad del chatbot para pedir aclaraciones o proponer interpretaciones cuando la consulta no es clara. También debe ser capaz de generar respuestas generales aún cuando falte especificación de la información.\n\n\n\n2.2. Proceso de Evaluación (Simulación)\nSe simulan una serie de consultas típicas que un usuario de Salsas Castillo podría realizar, cubriendo distintos escenarios:\n\nConsultas de Ventas: “Muéstrame las ventas del producto ‘AMOR 12 / 1000 ML’ en el último mes.”\nConsultas Financieras: “¿Cuál fue el margen de ganancia del segundo trimestre de 2025?”\nGeneración de Reportes: “Genera un reporte de ventas por presentación para el mes de mayo.”\nBúsqueda de Documentos: “Necesito el reglamento de uso de las bodegas.” (Si aplica y se ha cargado un documento de ejemplo)\nConsultas Ambiguas: “Quiero saber sobre las salsas vendidas ayer” (El chatbot debería pedir más detalles o sugerir opciones o debería generar una respuestas general).\n\nLos resultados de estas simulaciones son revisados por expertos con conocimiento del negocio para validar la calidad de las respuestas y el comportamiento general del sistema.\nLos resultados obtenidos hasta ahora sugieren un desempeño prometedor del sistema, con respuestas coherentes y alineadas con las bases de datos. Esto valida la viabilidad de continuar con el despliegue y la iteración en un entorno de pruebas real."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "Salsas Castillo es una empresa dedicada a la producción y comercialización de salsas picantes, soya, chamoy y otros productos. Su portafolio se distribuye a una amplia red de tiendas, restaurantes y mayoristas en todo México. Con un catálogo en constante crecimiento y una operación comercial dinámica, la empresa enfrenta el reto de gestionar grandes volúmenes de información interna de forma ágil y eficiente.\nActualmente, el acceso a datos específicos —como ventas, finanzas o documentación interna— suele ser un proceso manual y demandante, lo que dificulta responder con rapidez a consultas y limita la generación de análisis oportunos para la toma de decisiones. Esto ha impulsado la necesidad de una solución que centralice y automatice el acceso a la información.\nObjetivo del proyecto:\nDesarrollar un chatbot inteligente que funcione como asistente virtual para el personal de Salsas Castillo, permitiendo realizar consultas rápidas y precisas sobre datos de ventas, finanzas, facturas y documentos internos. El propósito es mejorar la eficiencia operativa, elevar la calidad de la toma de decisiones y fortalecer la comunicación interna.\nImpacto:\nLa implementación de este chatbot permitirá a Salsas Castillo:\n\nAgilizar el acceso a la información: Reducir el tiempo que el personal dedica a buscar datos específicos en bases de datos o documentos.\nMejorar la precisión de los datos: Proporcionar respuestas consistentes y actualizadas directamente de las fuentes de datos en tiempo real.\nFomentar la autonomía: Empoderar a los empleados para obtener la información que necesitan sin depender de intermediarios y al alcance de su mano.\nOptimizar la toma de decisiones: Facilitar análisis rápidos y la generación de reportes para una gestión más proactiva."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html#objetivos-de-la-línea-de-investigación",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html#objetivos-de-la-línea-de-investigación",
    "title": "Comprensión del negocio",
    "section": "0.1 Objetivos de la línea de investigación",
    "text": "0.1 Objetivos de la línea de investigación\nEsta iniciativa se enfoca en aprovechar la información existente de Salsas Castillo para crear un sistema de consulta inteligente.\nEl objetivo principal es desarrollar un chatbot en Telegram que funcione como un asistente virtual para la generación de análisis complejos, capaz de:\n\nInterpretar consultas en lenguaje natural: Permitir a los usuarios hacer preguntas complejas sin necesidad de conocimientos técnicos de bases de datos.\nAcceder a datos dinámicos: Conectarse directamente a las bases de datos transaccionales (PostgreSQL) para obtener información actualizada dentro de la base de datos.\nBuscar en documentos internos: Utilizar una base de datos vectorial para recuperar información de documentos como manuales, reglamentos, contratos, etc.\nGenerar reportes: Crear y enviar reportes financieros en formato PDF a través de Telegram.\n\nObjetivos específicos:\n\nIntegrar fuentes de datos: Establecer conexiones robustas con las bases de datos en PostgreSQL.\nDesarrollar herramientas de consulta: Implementar funciones que permitan al LLM interactuar con SQL para extraer y procesar datos.\nConstruir una base de conocimiento vectorial: Procesar documentos internos para crear embeddings y un vector store para búsquedas semánticas.\nImplementar un agente conversacional: Configurar un agente basado en LangChain que orqueste el uso del LLM y las herramientas.\nHabilitar la generación de reportes PDF: Desarrollar la capacidad de crear reportes dinámicos a partir de los datos consultados.\nDesplegar el chatbot en Telegram: Asegurar la funcionalidad completa del chatbot en la plataforma de Telegram, incluyendo la transcripción de audio.\n\nCriterios de éxito:\n\nPrecisión de las respuestas: El chatbot debe proporcionar información precisa, exacta y relevante en al menos el 90% de las consultas cuando se trate de cálculos y uso de formulas.\nFidelidad: Debe ser fiel a la información extraída de las bases de datos sin presentar alucionaciones en al menos el 90% de las consultas.\nCapacidad de análisis: Debe ser capaz de realizar análisis tanto básicos como complejos de datos y generar reportes comprensibles.\nUsabilidad: La interacción a través de Telegram debe ser intuitiva y accesible para usuarios con diferentes niveles de conocimientos técnicos."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html#evaluación-de-la-situación-actual",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html#evaluación-de-la-situación-actual",
    "title": "Comprensión del negocio",
    "section": "0.2 Evaluación de la Situación Actual",
    "text": "0.2 Evaluación de la Situación Actual\nLos recursos disponibles para este proyecto incluyen:\n\nDatos: Acceso a bases de datos PostgreSQL con información de ventas transaccionales y datos financieros consolidados.\nHerramientas: Python, FastAPI, LangChain, OpenAI API, PyMongo, Reportlab, y PostgreSQL.\nEquipo humano: El equipo de Salsas Castillo que provee la información y el contexto necesario para alinear el conocimiento del sistma, y el equipo de desarrollo del proyecto.\n\n\n0.2.1 Requisitos, Supuestos y Restricciones\nRequisitos:\n\nAcceso continuo y estable a las bases de datos PostgreSQL y MongoDB.\nServidor donde se desplegará la API del sistema desarrollado.\nCredenciales válidas para las APIs de OpenAI y el token de Telegram.\nComunicación fluida para la retroalimentación y validación de funcionalidades.\n\nSupuestos:\n\nLas bases de datos de la empresa contienen la información necesaria y están estructuradas de manera que permitan las consultas requeridas sin complicaciones.\nLa API de OpenAI y Telegram mantendrán su disponibilidad y rendimiento.\nEl servidor proveído por la empresa estará siempre disponible para el equipo de desarrollo sin restricciones.\n\nRestricciones:\n\nPosibles limitaciones en la tasa de llamadas a las APIs externas.\nLa complejidad de las consultas SQL puede requerir optimización.\nLa seguridad de la información debe ser una prioridad en todo momento."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html#terminología",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3 Terminología",
    "text": "0.3 Terminología\nAlgunas de las terminologías clave para este proyecto son:\n\nPython: Lenguaje de programación de alto nivel, fundamental para el desarrollo del backend del chatbot.\nFastAPI: Framework de desarrollo web en Python para construir APIs de forma rápida y eficiente.\nLangChain: Herramienta para construir aplicaciones que combinan modelos de lenguaje con fuentes de datos externas y lógica personalizada.\nInteligencia Artificial (IA): Campo de la informática que desarrolla sistemas capaces de realizar tareas que requieren inteligencia humana.\nModelos de Lenguaje Grande (LLM): Modelos de IA entrenados con vastos volúmenes de texto para comprender y generar lenguaje natural. En este proyecto, se utiliza GPT-4.1 de OpenAI.\nSistema de Recuperación Aumentada con Generación (RAG): Técnica que combina LLMs con bases de datos externas para recuperar información relevante y generar respuestas más precisas y contextualizadas.\nRepresentación Vectorial: Proceso de convertir datos textuales en representaciones numéricas (vectores) para facilitar su análisis y búsqueda.\nModelo de Embeddings: Algoritmo que transforma palabras o frases en vectores. Se utilizan OpenAI Embeddings para documentos internos.\nBase de Datos Vectorial (FAISS): Sistema de almacenamiento optimizado para buscar y recuperar información midiendo la similitud entre vectores. Se utiliza para documentos internos.\nBase de Datos SQL (Structured Query Language): Sistema de almacenamiento relacional que organiza los datos en tablas. En este proyecto, PostgreSQL es la fuente de datos transaccionales y financieros.\nAPI (Interfaz de Programación de Aplicaciones): Conjunto de reglas que permite que diferentes sistemas de software se comuniquen entre sí.\nAPI Key: Clave de autenticación utilizada para acceder a servicios protegidos por una API.\nEndpoint (API): Dirección específica dentro de una API donde se accede a una funcionalidad concreta.\nPayload (HTTP): Contenido de los datos enviados en una solicitud HTTP.\nChatbot: Programa que interactúa con los usuarios mediante lenguaje natural.\nTelegram Bot API: La interfaz de programación que permite a los desarrolladores crear bots que interactúan con los usuarios de Telegram.\nWebhook: Un mecanismo que permite a una aplicación recibir información en tiempo real de otra aplicación cuando ocurre un evento específico.\nMongoDB: Base de datos NoSQL utilizada para almacenar el historial de sesiones y los respaldos de mensajes.\nOpenAI Whisper: Modelo de IA para la transcripción de audio a texto.\nReportlab: Librería de Python para la generación de documentos PDF. `"
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html#beneficios",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4 Beneficios",
    "text": "0.4 Beneficios\n\nAcceso Instantáneo a la Información: El personal puede obtener datos clave de ventas y finanzas en segundos, directamente desde Telegram.\nAnálisis de Datos Simplificado: La capacidad de generar reportes en PDF con un análisis generado por el LLM democratiza el acceso a distintas perspectivas del negocio.\nReducción de Carga de Trabajo: Disminuye la necesidad de consultas manuales a las bases de datos y/o la preparación de reportes rutinarios.\nMejora en la Productividad: Permite a los equipos enfocarse en tareas de mayor valor al tener la información al alcance de la mano.\nInnovación Tecnológica: Salsas Castillo se posiciona a la vanguardia en el uso de IA para la gestión empresarial."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/1_comprension.html#costos",
    "href": "salsas_castillo_chatbot/quarto/1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5 Costos",
    "text": "0.5 Costos\n\nTiempo: El desarrollo y la implementación del chatbot requieren una inversión de tiempo significativa del equipo de desarrollo.\nFinancieros: Costos asociados a las suscripciones de APIs (OpenAI) y, potencialmente, a la infraestructura de servidores para el despliegue."
  },
  {
    "objectID": "ct_chatbot/quarto/preparacion.html",
    "href": "ct_chatbot/quarto/preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "ct_chatbot/quarto/preparacion.html#preparación-de-los-datos",
    "href": "ct_chatbot/quarto/preparacion.html#preparación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "ct_chatbot/quarto/preparacion.html#estructura-final-de-los-datos",
    "href": "ct_chatbot/quarto/preparacion.html#estructura-final-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Estructura final de los datos",
    "text": "2. Estructura final de los datos\nUna vez completado el proceso ETL, la información de productos y promociones queda estructurada de la siguiente manera, lista para ser consumida por el chatbot y el sistema de recomendación:\nEn el caso de los productos, la información final queda estructurada de la siguiente manera:\n {\n        \"nombre\": \"Nombre del Producto\",\n        \"clave\": \"Clave\",\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave concatenadas.\",\n        \"fichaTecnica\": {\n            \"Caracteristica1\": \"Valor1\",\n            \"Caracteristica2\": \"Valor2\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto.\",\n            \"LongSummary\": \"Resumen largo.\"\n        }\n    },\n    {...}\nReiteramos que el proceso de limpieza y enriquecimiento aplicado a las promociones fue similar al de los productos. A continuación, se presenta el resultado final de la estructura deseada para las promociones:\n{\n        \"nombre\": \"Nombre de la Promoción\",\n        \"producto\": \"Clave\", # Clave del producto en promoción\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave de la promoción.\",\n        \"fichaTecnica\": {\n            \"CaracteristicaA\": \"ValorA\",\n            \"CaracteristicaB\": \"ValorB\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto de la promoción.\",\n            \"LongSummary\": \"Resumen largo de la promoción.\"\n        }\n    },\n    {...}"
  },
  {
    "objectID": "ct_chatbot/quarto/home.html",
    "href": "ct_chatbot/quarto/home.html",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "ct_chatbot/quarto/home.html#introducción",
    "href": "ct_chatbot/quarto/home.html#introducción",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "ct_chatbot/quarto/despliegue.html",
    "href": "ct_chatbot/quarto/despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "ct_chatbot/quarto/despliegue.html#revisión-del-proceso",
    "href": "ct_chatbot/quarto/despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "ct_chatbot/quarto/despliegue.html#plan-de-implementación",
    "href": "ct_chatbot/quarto/despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2 Plan de implementación",
    "text": "2 Plan de implementación\nLa fase de implementación implica el despliegue de los componentes desarrollados y su integración en el entorno web de pruebas de CT Online. El objetivo es habilitar el widget de chatbot para un grupo controlado de usuarios.\n\n2.1. Arquitectura de Despliegue y Conexión\nLa fase de implementación en el entorno de pruebas requiere el despliegue de la API del chatbot (desarrollada en Python con FastAPI) y la integración del widget (desarrollado en JavaScript, HTML y CSS) en la página web de pruebas de CT Online.\nUn desafío técnico crucial identificado durante la planificación del despliegue fue la política de seguridad de “contenido mixto” (mixed content) impuesta por los navegadores modernos. Dado que la página de pruebas de la empresa se sirve a través de HTTPS para garantizar una conexión segura, el navegador bloquea las peticiones que el código JavaScript del widget intenta realizar a recursos o servicios que no son seguros, como la API FastAPI que opera con el protocolo HTTP. Realizar llamadas directas desde HTTPS a HTTP resulta en un error de “mixed content”, impidiendo la comunicación.\nTambién se evaluó la posibilidad de servir la API directamente a través de HTTPS. Sin embargo, esta alternativa implicaba mayores retos técnicos y operativos, como la gestión de certificados SSL válidos que autentiquen la comunicación segura entre el servidor y los navegadores. Aunque se intentó utilizar un certificado autofirmado, los navegadores modernos no lo reconocen como confiable, lo que resultaba en el bloqueo automático de las conexiones. Además del reto técnico, esta opción implicaba complicaciones administrativas relacionadas con la obtención, configuración y renovación de certificados válidos, lo que aumentaba la complejidad del despliegue inicial.\nPara superar este obstáculo y permitir la comunicación segura entre el frontend en HTTPS y la API, se ha adoptado la siguiente arquitectura de despliegue utilizando un backend proxy:\n\nLa aplicación principal de la empresa, que opera en un entorno seguro con HTTPS, actuará como intermediaria. Dado que esta aplicación utiliza PHP, el proxy se implementará en este lenguaje.\nEl widget, integrado en la página de pruebas (servida en HTTPS), no realizará llamadas directas a la API FastAPI. En su lugar, el JavaScript del widget será configurado para enviar todas sus peticiones a un nuevo endpoint específico en el backend PHP.\nEl backend PHP recibirá estas peticiones entrantes del frontend (en HTTPS).\nEl código PHP, realizará entonces la solicitud real a la API. La comunicación entre el backend PHP y la API no está sujeta a las restricciones de contenido mixto del navegador, o sea, sin importa si la API se sirve en HTTP o HTTPS, el PHP siempre se puede comunicar con el servicio sin problema.\nEl backend PHP recibirá la respuesta de la API y la reenviará de vuelta al frontend del widget (en HTTPS), en otras palabras, se apunta así mismo.\n\nEste enfoque de proxy en el backend PHP resuelve el problema del contenido mixto al asegurar que la comunicación entre el navegador (frontend) y la infraestructura del backend de la empresa siempre se realice a través de HTTPS. El desafío de integrar una fuente HTTP en un entorno HTTPS queda encapsulado en la comunicación de servidor a servidor.\n\n\n2.2 Gestión de Persistencia de Datos con MongoDB\nUna mejora significativa en la arquitectura del chatbot ha sido la migración de la gestión del historial de conversaciones desde archivos JSON locales a una base de datos NoSQL, específicamente MongoDB. Esta decisión se tomó para optimizar el almacenamiento, mejorar el rendimiento y facilitar el análisis de datos, especialmente considerando el volumen y la frecuencia de las interacciones de los usuarios.\nLa implementación en MongoDB se estructura en dos colecciones principales:\n\nsessions: Esta colección está diseñada para mantener los últimos n mensajes de cada sesión de usuario. Su objetivo es asegurar una recuperación de mensajes mínima y rápida, optimizando la experiencia del usuario final al evitar la carga de historiales extensos en cada interacción. Cada vez que se añade un nuevo mensaje, el más antiguo se desplaza si se supera el límite de mensajes configurado, manteniendo la colección ligera y eficiente para las operaciones del chatbot.\nmessage_backup: A diferencia de sessions, esta colección actúa como un histórico completo de todos los mensajes generados. Su propósito principal es el análisis de datos y la alimentación de sistemas de reportes automatizados. El esquema de esta colección está pensado para simular una tabla SQL, lo que facilita la búsqueda y recuperación de información para análisis posteriores. Cada documento en esta colección incluye tanto la consulta del usuario como la respuesta del chatbot, junto con metadatos relevantes como timestamps y detalles de tokens utilizados.\n\nEsta estrategia de persistencia de datos en MongoDB ha permitido superar las limitaciones de los archivos JSON, que no eran escalables para una gran cantidad de usuarios, ofreciendo un camino más robusto y óptimo para almacenar la información de las conversaciones.\n\n\n\nSchema MongoDB\n\n\n\n\n2.3 Desarrollo del widget frontend\nSe desarrolló un widget de chat personalizado e integrable mediante un simple script (sdk.js). Este widget se encarga de inyectar la interfaz de usuario (html) y cargar la lógica de la aplicación (app.js) y los estilos (styles.css) de forma dinámica en cualquier página web.\n\n\n2.4 Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nFrecuencia de uso del widget: Número de aperturas del chat y cantidad de interacciones por usuario.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.5 Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.6 Experiencia de desarrollo\nEl proyecto ha permitido consolidar la experiencia en el ciclo completo de desarrollo de una aplicación basada en modelos de lenguaje, desde la comprensión y preparación de datos complejos, pasando por el prototipado con herramientas como Langchain, hasta el desarrollo de una API robusta con FastAPI y la implementación de una interfaz de usuario dinámica y reusable (widget frontend desarrollado en JS, HTML y CSS). La resolución de desafíos específicos como el manejo de diferentes estructuras de datos para las vector stores y la integración segura de una API a un entorno web real (HTTPS/contenido mixto, CORS, permisos de red) han sido aprendizajes clave con complicaciones y problemas que se pudieron corregir y solucionar. Se han seguido buenas prácticas de desarrollo, enfocándose en la modularidad para facilitar futuras expansiones (ej: integración de LangGraph) y el mantenimiento del código.\n\n\n2.7 Despliegue del chatbot en el sistema de desarrollo\nEl chatbot fue desplegado exitosamente en el entorno de pruebas de CT Online, habilitado específicamente para fines de desarrollo e integración continua. Este entorno permite validar en condiciones casi reales el comportamiento tanto del frontend (widget) como de la API conversacional.\nEl proceso de despliegue consistió en los siguientes pasos:\n\nMontaje del entorno de la API: La API desarrollada con FastAPI se desplegó en un servidor, o ambiente virtual de linux, utilizando Gunicorn como servidor de aplicaciones y conectándose al backend de la página de CT Online.\nIntegración del widget en la página de pruebas: Se inyectó el script del widget directamente en la página, asegurando que se pudieran cargar dinámicamente los recursos necesarios (JS, HTML y CSS) desde un servidor de archivos estáticos. La integración se validó en distintos navegadores modernos para asegurar la compatibilidad y el correcto funcionamiento.\nGestión de versiones y control de cambios: Se utilizó Git para gestionar versiones del código tanto del sistema del Chatbot, la API y del widget. Esto permitió llevar un registro detallado de los cambios realizados y facilitó el proceso de despliegue incremental, en caso de futuras modificaciones o ajustes.\nVerificación funcional: Tras el despliegue inicial, se realizaron pruebas manuales y automatizadas para verificar el correcto funcionamiento del flujo de conversación, el tiempo de respuesta de la API y el comportamiento del widget en diferentes escenarios (errores, entradas no reconocidas, ausencia de resultados, etc.).\nConsideraciones de seguridad: Aunque se trata de un entorno de pruebas, se aseguraron medidas básicas como la validación de origen en CORS, limitación de rutas expuestas en la API, y uso de HTTPS para todas las comunicaciones entre cliente y servidor.\n\nLa imagen a continuación muestra el chatbot funcionando en su entorno de desarrollo, con el widget incrustado en la página de pruebas de CT Online:\n\n\n\nChatbot desplegado en el ambiente de desarrollo\n\n\nEste hito marca un avance significativo hacia la validación en entorno real del sistema conversacional, permitiendo recopilar feedback de usuarios internos antes de considerar un despliegue completo en producción.\n\n\n2.8 Despliegue del chatbot en el sistema de producción\nA partir del despliegue en el ambiente de pruebas, donde recopilamos retroalimentación e hicimos iteraciones sobre el modelo, finalmente pudimos desplegarlo al ambiente de producción. Este despliegue es escalonado, empezando por la sucursal de Hermosillo y con los vendedores de la empresa, recopilando nuevamente retroalimentación pero más pegada a casos de estudio concretos y reales.\n\n\n\nChatbot desplegado en el ambiente de producción"
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html",
    "href": "ct_chatbot/quarto/comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "CT Internacional es una empresa mexicana encargada en distribuir soluciones de Tecnologías de la Información (TI), preferida para hacer negocios de los distribuidores e integradores del mundo de la tecnología. Fundada en 1992 en la ciudad de Hermosillo, esta empresa surgió como una respuesta a la oportunidad de llevar soluciones de TI, principalmente en el noroeste del país (Saúl Rojo). La cual poco a poco se fue expandiendo hasta lograr el alcance que tiene hoy en día, convirtiéndose en una empresa mayorista de alto impacto en el canal de distribución. Actualmente es una de las mejores empresas mexicanas y tiene presencia con 52 sucursales en todos los estados del país. Además cuenta con un canal de distribución integrado por más de 31 mil clientes y aliados de negocio a quiénes proporciona un extenso catálogo de productos y servicios de más de 202 marcas agrupadas en 12 unidades de negocio.\nTeniendo más de 25 años en el mercado, el crecimiento constante del negocio y del mundo de la tecnología, además de la gran cantidad de productos que ofrecen para su distribución; la empresa tiene la oportunidad de aprovechar los avances tecnológicos que se han ido generado hoy en día a su favor. Esto con la intención de ofrecer un mejor servicio a sus clientes y darles un mejor acercamiento de sus productos para que sean capaces de ver un catálogo especial o personalizado en sus necesidades específicas.\nObjetivo del proyecto:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nImpacto:\nLa empresa se estaría adaptando a la nueva era tecnológica, consolidando su liderazgo en el mercado al ofrecer productos que antes podían pasar desapercibidos para el cliente. Con este sistema, los consumidores pueden tomar decisiones ajustadas a sus necesidades, generando valor tanto para ellos como para la empresa."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html#objetivos-de-la-línea-de-investigación",
    "href": "ct_chatbot/quarto/comprension.html#objetivos-de-la-línea-de-investigación",
    "title": "Comprensión del negocio",
    "section": "0.1 Objetivos de la línea de investigación",
    "text": "0.1 Objetivos de la línea de investigación\nEsta línea de investigación se enfoca en aprovechar la información disponible sobre los productos de la empresa, incluyendo características técnicas, precios y disponibilidad, con el objetivo de optimizar su accesibilidad y uso en procesos comerciales.\nEl primer paso es extraer, analizar y evaluar la calidad de los datos almacenados en la base de datos de la empresa para determinar si son suficientes para el desarrollo del sistema o si es necesario complementarlos con información adicional.\nUna vez validada la información, se procederá a su transformación mediante modelos de embeddings, convirtiéndola en representaciones vectoriales para su almacenamiento en una base de datos vectorial.\nEl objetivo final es desarrollar un sistema que permita a los usuarios consultar productos con base en especificaciones detalladas mediante un chatbot, el cual recuperará información relevante desde la base de datos vectorial, mejorando la precisión y relevancia de las recomendaciones.\nActualmente:\nLos productos se ofrecen a través de la plataforma, donde los clientes pueden realizar búsquedas según sus necesidades. Sin embargo, el sistema de búsqueda presenta limitaciones: si los clientes no utilizan ciertas palabras clave que están relacionadas al producto, encontrar lo que buscan puede volverse complicado. Además, la empresa cuenta con un sistema de asistencia por llamada o correo, donde un operador ayuda a los clientes a resolver dudas o encontrar productos específicos.\nObjetivo principal:\nDesarrollar un chatbot inteligente para la recomendación de productos, conectado a una base de datos vectorial para ofrecer respuestas precisas y relevantes a los usuarios, y conectado a SQL para consultar información dinámica. Con la intención de optimizar el proceso de búsqueda de productos que la empresa ofrece.\nObjetivos específicos:\n\nAnalizar y organizar la información disponible sobre un conjunto de los productos para evaluar su calidad y definir los atributos clave que se utilizarán en el sistema de recomendación. Además, identificar si es necesario crear nuevas características o ajustar las existentes para mejorar la precisión de las recomendaciones.\nDesarrollar un modelo de representación vectorial que convierta la información de los productos en representaciones vectoriales para su almacenamiento y recuperación eficiente.\nImplementar un sistema RAG que integre un modelo de lenguaje grande (LLM) con la base de datos vectorial para generar recomendaciones de productos precisas y relevantes.\nValidar el desempeño del sistema mediante pruebas de precisión, relevancia de las recomendaciones y eficiencia en la recuperación de información.\n\nCriterios de éxito:\n\nPrecisión en la recomendación de productos:\n\nAl menos el 90% de las 100 recomendaciones evaluadas deben coincidir con las necesidades descritas por el usuario, dando una respuesta directa, precisa y concisa en la información que proporcionada. La calidad de estas recomendaciones será validada por expertos con conocimiento en los productos.\n\nReproducibilidad y consistencia:\n\nConsultas similares deben producir respuestas coherentes en al menos el 90% de los 100 casos, evitando variaciones innecesaria, además de sugerencia de productos no disponibles o fuera de stock, ni productos que no existan. Este criterio también será evaluado por los mismos expertos quiénes determinarán la calidad de la respuesta.\n\nEscalabilidad:\n\nLa estructura del sistema mantiene su rendimiento al aumentar la cantidad de productos de la base de datos.\nLa calidad de las respuestas y el rendimiento del chatbot se mantiene constante al aumentar la cantidad de usuarios."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html#evaluación-de-la-situación-actual",
    "href": "ct_chatbot/quarto/comprension.html#evaluación-de-la-situación-actual",
    "title": "Comprensión del negocio",
    "section": "0.2 Evaluación de la situación actual",
    "text": "0.2 Evaluación de la situación actual\nLos recursos que se tienen actualmente para este proyecto son:\n\nDatos: Registro de todos los productos de la empresa disponibles para distribución. La información incluye nombre, marca, tipo, modelo, descripción, palabras clave, stock disponible, ficha técnica con especificaciones, características principales, fecha del registro y precio. Toda esta información se tiene en una base de datos SQL, la cual se actualiza periódicamente.\nHerramientas: Python, servicios de OpenAI y bases de datos SQL.\nEquipo humano: Especialistas en el sistema de distribución de la empresa, expertos en la estructura y manejo de la base de datos para facilitar la extracción de datos, y científicos de datos encargados del desarrollo y evaluación del sistema junto con los especialistas.\n\n\n0.2.1 Requisitos, supuestos y restricciones\nRequisitos:\n\nAcceso a los registros de los productos de la empresa y equipo de compúto con características específicas para el desarrollo del proyecto.\nComunicación constante con los expertos de la empresa para constante evaluación y retroalimentación.\nCredenciales para el uso y acceso de herramientas de la empresa.\n\nSupuestos:\n\nLos permisos y el acceso a la base de datos se matendrá a la disposición del equipo siempre que se necesite sin problemas.\nAPI KEY con créditos suficientes disponibles, para evitar problemas con las llamadas de los modelos de OpenAI.\n\nRestricciones:\n\nLimitaciones en el uso del enlace de la conexión de la base de datos ya que se puede saturar.\nConexiones del sistema con la red de la empresa presentan problemas de sseguridad y permisos que se deben verificar y aprobar por las personas encargadas de infraestructura."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html#terminologia",
    "href": "ct_chatbot/quarto/comprension.html#terminologia",
    "title": "Comprensión del negocio",
    "section": "0.3 Terminología",
    "text": "0.3 Terminología\nAlgunas de las terminologías clave para este proyecto son:\n\nPython: Lenguaje de programación de alto nivel, ampliamente utilizado en el desarrollo de software, análisis de datos, inteligencia artificial y automatización.\n\nFastAPI: Framework de desarrollo web en Python que permite construir APIs de forma rápida, sencilla y eficiente.\n\nLangChain: Herramienta para construir aplicaciones que combinan modelos de lenguaje con fuentes de datos externas y lógica personalizada.\n\nInteligencia artificial (IA): Campo de la informática que desarrolla sistemas capaces de realizar tareas que requieren inteligencia humana, como el aprendizaje, la toma de decisiones y el procesamiento del lenguaje natural.\n\nModelos de lenguaje grande (LLM): Modelos de inteligencia artificial entrenados con grandes volúmenes de texto para comprender y generar lenguaje natural.\n\nSistema de recuperación mejorada (RAG): Técnica que combina modelos de lenguaje con bases de datos externas para recuperar información relevante y generar respuestas más precisas.\n\nRepresentación vectorial: Proceso de convertir datos textuales en representaciones numéricas (vectores) para facilitar su análisis y búsqueda.\n\nModelo de embeddings: Algoritmo que transforma palabras o frases en vectores de manera que su similitud semántica pueda medirse matemáticamente.\n\nBase de datos vectorial: Sistema de almacenamiento optimizado para buscar y recuperar información midiendo la similitud entre vectores.\n\nBase de datos SQL (Structured Query Language): Sistema de almacenamiento relacional que organiza los datos en tablas con filas y columnas, usando SQL para consultarlos.\n\nAPI (Interfaz de Programación de Aplicaciones): Conjunto de reglas que permite que diferentes sistemas de software se comuniquen entre sí.\n\nAPI KEY: Clave de autenticación utilizada para acceder a servicios protegidos por una API.\n\nEndpoint (API): Dirección específica dentro de una API donde se accede a una funcionalidad concreta.\n\nPayload (HTTP): Contenido de los datos enviados en una solicitud HTTP, como un formulario o un JSON con información.\n\nChatbot: Programa que interactúa con los usuarios mediante lenguaje natural para responder preguntas o realizar tareas automatizadas.\n\nFrontend: Parte visual o interfaz con la que interactúa el usuario en una aplicación web.\n\nBackend: Parte lógica o del servidor donde se procesa la información y se ejecutan las funciones principales de una aplicación.\n\nHTTP (Hypertext Transfer Protocol): Protocolo para la transferencia de datos en la web.\n\nHTTPS (HTTP Secure): Versión segura del protocolo HTTP que cifra los datos para proteger la comunicación.\n\nContenido mixto (Mixed Content): Problema de seguridad que ocurre cuando una página HTTPS carga recursos desde una fuente HTTP, lo cual puede ser bloqueado por los navegadores.\n\nCertificado SSL/TLS: Archivo digital que autentica la identidad de un sitio web y cifra la información entre el servidor y el navegador.\n\nAutoridad Certificadora (CA): Entidad confiable que emite certificados digitales para garantizar la seguridad en la web.\n\nProxy: Servidor que actúa como intermediario entre un cliente y otro servidor, utilizado para redirigir solicitudes.\n\nPuerto (de red): Punto lógico de conexión en un servidor que permite recibir solicitudes de red.\n\nWidget: Componente visual incrustado en una página web, como un chatbot o formulario."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html#beneficios",
    "href": "ct_chatbot/quarto/comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4 Beneficios",
    "text": "0.4 Beneficios\n\nInnovación en la oferta de productos: Introducir un nuevo enfoque en la recomendación de productos, brindando una experiencia más personalizada y efectiva tanto para la empresa como para los clientes.\nOptimización de la estrategia del mercado: La implementación del sistema abre oportunidades para nuevas estrategias de negocio, mejorando la eficiencia y el alcance en la comercialización de productos."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension.html#costos",
    "href": "ct_chatbot/quarto/comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5 Costos",
    "text": "0.5 Costos\n\nTiempo: El proyecto tiene un plazo estimado de 3 meses para desarrollar una versión funcional y tangible que sirva como punto de partida para futuras mejoras e implementación en el flujo de trabajo.\nFinancieros: Se consideran costos asociados a suscripciones de herramientas de pago necesarias para el desarrollo e implementación del sistema."
  },
  {
    "objectID": "cenace_helpdesk/quarto/5_despliegue.html",
    "href": "cenace_helpdesk/quarto/5_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "cenace_helpdesk/quarto/5_despliegue.html#revisión-del-proceso",
    "href": "cenace_helpdesk/quarto/5_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "cenace_helpdesk/quarto/5_despliegue.html#plan-de-implementación",
    "href": "cenace_helpdesk/quarto/5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación\nEl plan de despliegue se centra en migrar la arquitectura de desarrollo a un entorno de producción escalable de tipo piloto, manteniendo la modularidad del sistema y optimizando el rendimiento.\n\n2.1. Arquitectura de despliegue y conexión\nLa arquitectura final para el despliegue estará compuesta por los siguientes componentes clave, implementados en un entorno de producción como Databricks, Azure o una plataforma similar:\n\nServidor de la API (backend): Implementación de la API desarrollada en FastAPI (main.py, chat.py) en un servidor escalable. Este servidor manejará las peticiones de los usuarios, coordinará las operaciones del RAG y se comunicará con la base de datos y el LLM.\nModelo de lenguaje (LLM): El modelo de lenguaje gemma3:4b se desplegará en un servidor con aceleración por GPU para asegurar un rendimiento óptimo en la generación de respuestas.\nBase de datos vectorial: La base de datos vectorial de FAISS, que almacena los embeddings de los documentos (vectorstore.py), se mantendrá, pero se integrará con un sistema de almacenamiento persistente y escalable en la nube para garantizar la disponibilidad y el rendimiento.\nBase de datos de historial y tickets (MongoDB): La base de datos de MongoDB, utilizada para almacenar el historial de conversaciones y la gestión de tickets, se migrará a un servicio de bases de datos gestionado en la nube para asegurar la persistencia y la seguridad de los datos.\nInterfaz del usuario (frontend): La presentación del sistema se monta de la mano con FastAPI aprovechando su clase interna Jinja2Templates.\n\n\n\n2.2. Desarrollo del frontend\nLa interfaz se desarrolló con ayuda de 3 tipos de archivos básicos para páginas web. Múltiples archivos de JavaScript, un archivo HTML y estilos modernos que mejoraran la experiencia del usuario gracias a un archivo CSS.\n\n2.2.1. Interfaz del usuario final\nA continuación, se presentan capturas de pantalla de las principales secciones de la interfaz de usuario final desplegada, mostrando las funcionalidades clave del sistema:\n\nInicio de sesión: Sección inicial donde el usuario puede acceder a su sesión. Por delimitaciones del proyecto, una sesión más segura no fue implementada. \nPestaña de Chat: Interfaz principal de conversación donde el usuario interactúa con el LLM para resolver incidentes. Muestra el historial de mensajes y las referencias utilizadas por el modelo. \nPestaña de Documentos: Permite a los usuarios cargar nuevos documentos PDF a la base de conocimientos y visualizar los documentos ya procesados. \nPestaña de Soluciones: Muestra las soluciones que han sido marcadas como “útiles” por los usuarios (a través del botón “like”). Desde aquí se pueden procesar estas soluciones para re-indexarlas en la base vectorial. \nPestaña de Tickets: Presenta la lista de tickets registrados, permitiendo visualizar su detalle (título, descripción y categoría) y llevar un ticket específico a una nueva conversación en el chat, iterando hasta llegar a la solución del ticket. \n\n\n\n\n2.3. Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.4. Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.5. Flujo de despliegue en el ambiente virtual\nEl despliegue del sistema en un ambiente virtual o de nube (como el entorno piloto propuesto) implica una transición desde la configuración de desarrollo local hacia una arquitectura gestionada, escalable y robusta. El flujo general se compone de las siguientes etapas conceptuales:\n\nContenerización de la Aplicación: El primer paso consiste en empaquetar la aplicación backend de FastAPI, junto con todas sus dependencias Python, en una imagen de contenedor (utilizando Docker o Podman). Esto garantiza un entorno de ejecución consistente y aislado, independientemente de la infraestructura subyacente. Si se opta por auto-alojar el LLM con Ollama en producción, este también podría ser contenerizado. Los detalles técnicos para construir estas imágenes se basan en la configuración del entorno descrita en el manual de instalación.\nConfiguración de Servicios Gestionados: A diferencia del entorno local, en un ambiente virtual o de nube se aprovecharán servicios gestionados para componentes clave. Esto incluye la configuración de una instancia de MongoDB Atlas (o similar) para la persistencia del historial y los tickets, y potencialmente un servicio de inferencia de modelos con aceleración GPU para hospedar gemma3:4b, asegurando rendimiento y escalabilidad. La conexión a estos servicios se configurará mediante variables de entorno, similar a lo descrito en el archivo .env del manual. La base vectorial de FAISS requerirá un sistema de almacenamiento persistente asociado al contenedor o servicio que la gestione.\nOrquestación de Contenedores: Para gestionar la aplicación contenerizada (y potencialmente el LLM), se utilizará una herramienta de orquestación (como Podman). La orquestación facilitará el despliegue de múltiples instancias de la API para escalabilidad horizontal, gestionará el balanceo de carga entre ellas y asegurará la alta disponibilidad mediante la recuperación automática en caso de fallos.\nImplementación de Monitoreo y Logging: Se configurarán herramientas de monitoreo específicas del entorno de nube para rastrear el rendimiento de la API, la latencia del LLM, el uso de recursos (CPU, GPU, memoria) y el estado de las bases de datos. Se establecerán sistemas de logging centralizados para capturar los registros de la aplicación (como los generados en nohup.out localmente) y facilitar la depuración y el análisis de errores en tiempo real, alineados con las métricas definidas en el plan de monitoreo.\nIntegración y Pruebas Finales: Una vez desplegada y configurada la API en el nuevo entorno, el paso final es la integración con el sistema de Help Desk del CENACE (si aplica para la fase piloto). Se realizarán pruebas funcionales y de carga en este entorno para validar la correcta operación antes de ponerlo a disposición de los usuarios piloto.\n\nPara obtener las instrucciones técnicas detalladas, comandos específicos y configuraciones de ejemplo para cada una de estas etapas, consulte la sección “Manual de instalación y despliegue” en la Documentación Técnica (6_documentacion.qmd)."
  },
  {
    "objectID": "cenace_helpdesk/quarto/3_preparacion.html",
    "href": "cenace_helpdesk/quarto/3_preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "La fase de preparación de datos es crucial para el funcionamiento del sistema RAG. Su objetivo es convertir la información no estructurada, proveniente de los documentos técnicos del CENACE, en un formato que permita una búsqueda eficiente y una recuperación semántica de alta calidad. Este proceso se divide en tres etapas principales: extracción, transformación y carga de los datos."
  },
  {
    "objectID": "cenace_helpdesk/quarto/3_preparacion.html#extracción-de-los-datos",
    "href": "cenace_helpdesk/quarto/3_preparacion.html#extracción-de-los-datos",
    "title": "Preparación de los datos",
    "section": "1. Extracción de los datos",
    "text": "1. Extracción de los datos\nLa principal fuente de información son los documentos técnicos en formato PDF, así como la base de datos de incidentes resueltos. Para la ingesta de documentos, el sistema utiliza el módulo doccollection.py, específicamente la clase DisjointCollection.\n\nIngesta de archivos: Los documentos PDF se cargan y procesan de forma individual utilizando la librería PyPDF2. Se extrae el texto de cada página, junto con sus metadatos inherentes (título, autor, etc.).\nFragmentación de texto (chunking): El texto extraído se divide en fragmentos lógicos o “chunks” para preservar el contexto de la información. El TextSplitter del módulo doccollection está configurado con los siguientes parámetros para optimizar la cohesión del texto:\n\nTamaño del chunk: 1500 caracteres.\nSolapamiento (overlap): 200 caracteres. Este solapamiento asegura que la información contextual clave no se pierda en los límites de cada fragmento.\n\nAsignación de metadatos: A cada chunk se ke asignan metadatos esenciales, como el nombre del archivo de origen (filename), el número de página (page_number), y un identificador único de referencia (reference). Estos metadatos son vitales para referenciar las fuentes en las respuestas del LLM, evitando alucinaciones y permitiendo al usuario validar la información."
  },
  {
    "objectID": "cenace_helpdesk/quarto/3_preparacion.html#transformación-de-los-datos",
    "href": "cenace_helpdesk/quarto/3_preparacion.html#transformación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Transformación de los datos",
    "text": "2. Transformación de los datos\nUna vez que los documentos se han dividido en chunks, se transforman en una representación numérica que la computadora puede entender y procesar eficientemente.\n\nVectorización: Cada chunk de texto es procesado por el modelo de embeddings bge-m3:latest, implementado en la clase OllamaEmbedder.\nRepresentación vectorial: El modelo convierte el texto en un vector numérico de alta dimensión. Estos vectores capturan el significado semántico del texto; los chunks con un significado similar se agrupan en un espacio vectorial. Este enfoque va más allá de la simple búsqueda por palabres clave, ya que permite al sistema encontrar información relevante incluso si la consulta utiliza un vocabulario o una sintaxis diferente."
  },
  {
    "objectID": "cenace_helpdesk/quarto/3_preparacion.html#carga-de-los-datos",
    "href": "cenace_helpdesk/quarto/3_preparacion.html#carga-de-los-datos",
    "title": "Preparación de los datos",
    "section": "3. Carga de los datos",
    "text": "3. Carga de los datos\nLos vectores generados se almacenan en una base de datos vectorial optimizada para la búsqueda de similitud.\n\nBase de datos vectorial: Se utiliza FAISS (Facebook AI Similarity Search), una librería de código abierto para la búsqueda eficiente en grandes conjuntos de vectores. FAISS indexa los vectores de manera que las consultas de similitud se puedan ejecutar en milisegundos.\nPersistencia: La clase FAISSVectorStore es responsable de la carga y el almacenamiento de los vectores. El índice de FAISS se guarda en un archivo local (index.faiss), mientras que los metadatos de los chunks se almacenan en un diccionario (index.pkl).\nBúsqueda semántica: Una vez cargados los datos, la base de datos vectorial está lista para recibir consultas. Cuando un usuario envía una pregunta, esta se convierte en un vector, que luego se utiliza para encontrar los vectores más cercanos (los chunks más relevantes) en el índice de FAISS."
  },
  {
    "objectID": "cenace_helpdesk/quarto/3_preparacion.html#flujo-de-preparación",
    "href": "cenace_helpdesk/quarto/3_preparacion.html#flujo-de-preparación",
    "title": "Preparación de los datos",
    "section": "4. Flujo de preparación",
    "text": "4. Flujo de preparación\nEl proceso completo es orquestado por la clase RAG y es ejecutado como un flujo de trabajo de indexación:\n\nSolicitud de carga: El usuario sube un documento PDF a través del endpoint /documents.\nManejo de la ingesta: El rag.py recibe el archivo y delega su procesamiento al doccollection.\nGeneración de chunks y metadatos: doccollection lee el PDF, extrae el texto y lo divide en chunks. Asigna metadatos como el ID del documento, el nombre del archivo y el número de página a cada uno de ellos.\nVectorización y almacenamiento: Cada chunk y sus metadatos asociados se envían al vectorstore, que a su vez utiliza el embedder para obtener su representación vectorial. Finalmente, los vectores se agregan al índice de FAISS, asegurando que el conocimiento esté disponible para futuras consultas."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html",
    "href": "cenace_helpdesk/quarto/1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html#propuesta-de-solución",
    "href": "cenace_helpdesk/quarto/1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html#objetivos",
    "href": "cenace_helpdesk/quarto/1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html#terminología",
    "href": "cenace_helpdesk/quarto/1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html#beneficios",
    "href": "cenace_helpdesk/quarto/1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "cenace_helpdesk/quarto/1_comprension.html#costos",
    "href": "cenace_helpdesk/quarto/1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mí",
    "section": "",
    "text": "Soy un profesional apasionado por la intersección entre la ingeniería y la ciencia de datos. Actualmente curso la Maestría en Ciencia de Datos en la Universidad de Sonora, enfocándome en el Procesamiento de Lenguaje Natural (NLP) y el desarrollo de Modelos de Lenguaje Grande (LLMs).\nMi experiencia combina la solidez analítica de la ingeniería química con las herramientas modernas de desarrollo de software y análisis de datos."
  },
  {
    "objectID": "about.html#educación",
    "href": "about.html#educación",
    "title": "Sobre Mí",
    "section": "Educación",
    "text": "Educación\nMaestría en Ciencia de Datos | Universidad de Sonora\nAgosto 2023 - Presente Enfoque en Python, SQL, bases de datos NoSQL, procesos ETL y análisis de series de tiempo.\nLicenciatura en Ingeniería Química | Universidad de Sonora\nAgosto 2018 - Junio 2023 Habilidades en simulación y optimización de procesos (MATLAB, ASPEN HYSYS)."
  },
  {
    "objectID": "about.html#experiencia-profesional",
    "href": "about.html#experiencia-profesional",
    "title": "Sobre Mí",
    "section": "Experiencia Profesional",
    "text": "Experiencia Profesional\nCientífico de Datos | Centro Nacional de Control de Energía (CENACE)\nMayo 2024 - Presente Desarrollo de chatbots basados en sistemas RAG para asistir a ingenieros de TI. Integración de sistemas de clasificación de tickets para mejorar la eficiencia de respuesta.\nLaboratorio Químico | Comisión Federal de Electricidad (CFE)\nAbril 2022 - Septiembre 2022 Detección de fallas y medición en equipos eléctricos primarios. Aplicación de normas ASTM."
  },
  {
    "objectID": "about.html#habilidades-técnicas",
    "href": "about.html#habilidades-técnicas",
    "title": "Sobre Mí",
    "section": "Habilidades Técnicas",
    "text": "Habilidades Técnicas\n\nLenguajes: Python (Avanzado), SQL, JavaScript (Básico).\nIA & Data Science: LangChain, RAG, Ollama, Hugging Face, Pandas, Scikit-learn.\nBases de Datos: PostgreSQL, MySQL, MongoDB, Redis, FAISS (Vector Stores).\nDesarrollo & DevOps: FastAPI, Docker/Podman, Git/GitHub.\nIdiomas: Español (Nativo), Inglés (Fluido), Portugués (Fluido), Francés (Fluido)."
  },
  {
    "objectID": "about.html#intereses",
    "href": "about.html#intereses",
    "title": "Sobre Mí",
    "section": "Intereses",
    "text": "Intereses\nMe apasiona el aprendizaje continuo de idiomas y culturas, así como la resolución de problemas basada en datos y la aplicación de tecnología en proyectos innovadores."
  },
  {
    "objectID": "cenace_helpdesk/quarto/0_home.html",
    "href": "cenace_helpdesk/quarto/0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "cenace_helpdesk/quarto/0_home.html#introducción",
    "href": "cenace_helpdesk/quarto/0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "cenace_helpdesk/quarto/2_comprension_datos.html",
    "href": "cenace_helpdesk/quarto/2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "cenace_helpdesk/quarto/2_comprension_datos.html#recolección-de-los-datos",
    "href": "cenace_helpdesk/quarto/2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "cenace_helpdesk/quarto/2_comprension_datos.html#descripción-de-los-datos",
    "href": "cenace_helpdesk/quarto/2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de los datos extraídos, se obtuvo la siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nLas variables se describen a continuación:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "cenace_helpdesk/quarto/2_comprension_datos.html#exploración-de-los-datos",
    "href": "cenace_helpdesk/quarto/2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nLa exploración comenzó con el análisis del número de categorías únicas en los tickets. Esto es crucial, ya que parte del proyecto es desarrollar un modelo capaz de clasificar o asignar una categoría a nuevos tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nAl observar la distribución, se decidió trabajar únicamente con categorías que tuvieran más de 20 incidentes, considerando la cantidad de datos necesaria para los procesos de entrenamiento, validación y prueba. Esta selección resultó en un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección se analiza la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Esto nos permite entender la cantidad de información disponible que puede ser útil para resolver problemas recurrentes y para el desarrollo del modelo de clasificación.\n\n\n\nDistribución de palabras en los títulos\n\n\nA partir de esta gráfica podemos notar que alrededor de 9 palabras promedio son las que se utilizan en los títulos; lo cual es normal dado que tendemos a englobar las problemáticas en pocas palabras. Sin embargo, vemos que hay problemáticas que pueden pasar el promedio, hasta llegar a las 30 palabras aproximadamente.\n\n\n\nDistribución de palabras en las descripciones\n\n\nPara las descripciones vemos que el promedio es mayor, aproximadamente hasta las 88 palabras, aunque en la mayoría de los casos son menos las que se utilizan. Esto también es normal ya que aquí es donde las personas desarrollan los detalles del incidente el cual están enfrentando. También vemos que hay incidentes que pueden tomar tantas palabras hasta llegar a las 400, 500, y hasta las 1000, aunque este último sea poco común.\n\n\n\nDistribución de palabras en las soluciones\n\n\nEn este caso, nosotros esperábamos que en esta sección hubiéramos encontrado una mayor cantidad de palabras, porque en este caso encontramos que en promedio se utilizan 23 palabras, donde frecuentemente son menos. También encontramos que no todos los incidentes contienen una descripción detallada de la solución o de los pasos que se siguieron para resolver la situación; vimos que solo el 41% de los incidentes contienen una explicación de la solución.\n\n\n0.3.3. Análisis de los bigramas más comunes\nSe realizó un análisis de los pares de palabras más frecuentes en los títulos y descripciones para identificar patrones en la forma en que se plantean las problemáticas.\n\n\n\nBigramas de los títulos\n\n\nEn los títulos lo primero que llama la atención es la codificación de ciertas palabras, ya que por detalles confidenciales, se codificaron nombres propios, sistemas, subestaciones, etc. Luego, podemos ver que se mencionan muchas veces los despliegues de distintos sistemas de software y actualizaciones.\n\n\n\nBigramas de las descripciones\n\n\nEn las descripciones vemos que en la gran mayoría de los casos son saludos hacia la persona que se están dirigiendo. Dejando de lado estos saludos, vemos que se mencionan detalles muy técnicos y específicos que solo expertos en el tema podrán entender."
  },
  {
    "objectID": "cenace_helpdesk/quarto/2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "cenace_helpdesk/quarto/2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nA partir de los resultados previos, se determinó que la columna con menos información es la de soluciones. Solo el 41% de los registros contienen una explicación, con un promedio de 23 palabras, aunque en muchos casos la cantidad es menor.\nAl discutir esta situación con los expertos, se descubrió que el desarrollo y la documentación de soluciones no es una práctica común en el departamento, lo que resulta en una pérdida de conocimiento. Esto motivó la búsqueda de una propuesta para mejorar la persistencia de las soluciones."
  },
  {
    "objectID": "cenace_helpdesk/quarto/4_modelado.html",
    "href": "cenace_helpdesk/quarto/4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "cenace_helpdesk/quarto/4_modelado.html#modelado",
    "href": "cenace_helpdesk/quarto/4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "cenace_helpdesk/quarto/4_modelado.html#evaluación",
    "href": "cenace_helpdesk/quarto/4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación\nLa evaluación de un sistema de este tipo es una tarea no trivial. En lugar de métricas tradicionales, se adoptó un enfoque basado en la calidad de la recuperación y la generación, utilizando el marco propuesto por RAGAS. Este enfoque se centra en tres dimensiones clave:\n\nFidelidad: Las respuestas se basan únicamente en el contexto recuperado (los PDF y base de conocimientos). Que no presente alucinaciones en un 95% de los casos. ¿La respuesta se basa fielmente en el contexto recuperado?\nRelevancia de la respuesta: Las respuestas generadas respondan a la consulta del usuario. ¿La respuesta aborda directamente la consulta del usuario?\nRelevancia del contexto: La información recuperada hayan sido relevantes y útiles. ¿La información recuperada es pertinente para la pregunta?\n\nEstas métricas, alineadas con el juicio humano, permiten un ciclo de evaluación robusto y ágil."
  },
  {
    "objectID": "cenace_helpdesk/quarto/6_documentacion.html",
    "href": "cenace_helpdesk/quarto/6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "cenace_helpdesk/quarto/6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "cenace_helpdesk/quarto/6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "cenace_helpdesk/quarto/6_documentacion.html#documentación-técnica-del-código",
    "href": "cenace_helpdesk/quarto/6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\nLa solución se basa en una arquitectura de Recuperación Aumentada con Generación (RAG). La estructura modular del código, organizada en paquetes de Python, permite una clara separación de responsabilidades.\n\n2.1. Estructura de carpetas y módulos\n\nAPI/\n\nchat.py: Contiene los endpoints de FastAPI para interactuar con el chatbot.\nmain.py: Archivo principal que define la aplicación FastAPI y monta los endpoints.\n\nollama/\n\nassistant.py: Clase que encapsula la lógica para generar embeddings utilizando el modelo bge-m3:latest de Ollama.\n\ndoccollection.py: Módulo que maneja la carga y el procesamiento de documentos (PDF’s) para generar fragmentos de texto.\nrag.py: Clase principal del sistema RAG que integra el assistant, el doccollection y el vectorstore.\nvectorstore.py: Módulo que implementa la base de datos vectorial con FAISS.\nsettings/\n\nclients.py: Archivo de configuración que establece la conexión con la base de datos y el cliente de Ollama.\nconfig.py: Define las rutas de directorios para los vectores y los documentos procesados.\n\ntools/\n\nassistant.py: Clase base abstracta para el asistente LLM.\ndoccollection.py: Clase base abstracta para la colección de documentos.\nembedder.py: Clase base abstracta para el generador de embeddings.\nvectorstore.py: Clase base abstracta para almacén de vectores.\n\ntypes.py: Módulo que define modelos de datos con Pydantic para tipado de datos como Text, TextMetadata, Question, etc.\n\n\n\n2.2. Modelos LLM utilizados\nEl flujo de información en el sistema RAG sigue dos rutas principales:\n\nIndexación de documentos:\n\n\nLos archivos PDF son cargados y procesados por el módulo doccollection.py.\ndoccollection divide cada documento en fragmentos.\nCada fragmento es enviado al embedder.py para generar su representación vectorial.\nLos vectores resultantes se almacenan en la base de datos vectorial de FAISS, implementada en vectorstore.py, junto con sus metadatos.\n\n\nProceso de consulta (QA):\n\n\nUna consulta de usuario llega el endpoint de chat.py.\nLa consulta es vectorizada por el embedder.\nEl vectorstore realiza una búsqueda de similitud semántica para recuperar los fragmentos de documento más relevantes.\nEstos fragmentos se envían al assistant.py, que los utiliza como contexto.\nEl assistant utiliza el LLM (gemma3:4b) para generar una respuesta coherente y contextualizada.\nLa respuesta es devuelta al usuario a través del chat.py y el main.py.\n\n\n\n2.3. Puntos de entrada y funciones clave\n\nGestión de conversaciones: El módulo assistant.py gestiona el historial de conversación en MongoDB, permitiendo que el chatbot mantenga un contexto limitado con el usuario.\nGestión de tickets: Las funciones add_ticket y update_ticket_metadata en rag.py y sus respectivos endpoints en chat.py demuestran la capacidad del sistema para interactuar y actualizar una base de datos de tickets.\nBucle de retroalimentación: La funcionalidad has_liked_solution_in_conversation permite identificar y potencialmente re-indexar soluciones validadas por los usuarios, mejorando continuamente la base de conocimientos."
  },
  {
    "objectID": "cenace_helpdesk/quarto/6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "cenace_helpdesk/quarto/6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\nLa base de conocimientos del chatbot se construye a partir de un proceso que comprende la extracción, segmentación y vectorización del contenido textual proveniente de documentos en formato PDF.\nLos vectores resultantes son posteriormente indexados y almacenados en una base de datos vectorial, la cual constituye el núcleo de la recuperación de información relevante durante las interacciones con el chatbot.\n\n\n3.2. Flujo de la interacción\nEl usuario debe acceder a la pestaña Documentos, donde podrá seleccionar los archivos que desea incorporar a la base de conocimientos del sistema. Una vez elegidos, los documentos se suben a la carpeta correspondiente dentro del entorno donde se encuentra desplegado el sistema (backend). Posteriormente, estos archivos son procesados siguiendo el flujo descrito en el apartado anterior, dando como resultado la creación de la base vectorial o base de conocimientos del sistema.\n\n\n\nGeneración de la base de datos vectorial\n\n\n\n\n3.3. Recomendaciones para futura mejora\n\nLectura de documentos escaneados\n\nActualmente, el sistema no puede extraer información de documentos escaneados. Sería recomendable integrar un módulo de Reconocimiento Óptico de Caracteres (OCR) para ampliar la capacidad de análisis, o de igual manera, Modelo Multimodales que pudieran extraer la información y almacenarla en documentos PDF que sean posteriormente vectorizados.\n\nSistema de seguridad para el inicio de sesión\n\nEl mecanismo de inicio de sesión actual es básico, pues solo requiere ingresar el nombre del usuario.\nAunque esta simplicidad se ajusta al alcance inicial del proyecto, se sugiere incorporar un sistema de autenticación más robusto, que garantice la seguridad de acceso y manejo de información.\n\nSistema de corrección de ortografía\n\nDurante el desarrollo de los modelos de clasificación, se identificó que la falta de ortografía en los tickets afectaba la calidad del análisis.\nSe propuso el desarrollo de un sistema tipo journalist capaz de identificar las 5 W’s y reconstruir el contexto completo del texto, corrigiendo iterativamente los errores ortográficos al momento de cargar los datos."
  },
  {
    "objectID": "cenace_helpdesk/quarto/6_documentacion.html#arquitectura-del-sistema",
    "href": "cenace_helpdesk/quarto/6_documentacion.html#arquitectura-del-sistema",
    "title": "Documentación",
    "section": "4. Arquitectura del sistema",
    "text": "4. Arquitectura del sistema\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial.\n\n\n\nArquitectura del sistema\n\n\n\n4.1. Componentes clave de conversación y chat\n\nPOST /chat/stream: Endpoint principal para la interacción conversacional. Recibe una consulta y un conversation_id, y devuelve una respuesta generada por el LLM en tiempo real a través de un stream.\nGET /chat/history/{user_id}/{conversation_id}: Recupera el historial de mensajes de una conversación específica.\nPOST /conversations: Crea una nueva conversación, generando un conversation_id único.\nGET /conversations/{user_id}: Lista todas las conversaciones de un usuario, incluyendo sus títulos y la fecha de la última actualización.\nDELETE /conversations: Elimina una conversación específica y su historial de la base de datos.\nPATCH /message-metadata: Permite actualizar los metadatos de un mensaje, utilizado para la funcionalidad de “gustar” una solución.\n\n\n\n4.2. Componentes clave de documentos y soluciones\n\nPOST /documents: Permite cargar nuevos archivos (en formato PDF) a la base de datos vectorial para expandir la base de conocimientos.\nGET /documents: Lista todos los documentos que han sido procesados y están disponibles para la consulta.\nDELETE /documents: Elimina un documento específico de la base de datos vectorial, eliminando también su referencia y los fragmentos asociados.\nPOST /solutions: Procesa y re-indexa soluciones “gustadas” por los usuarios, agregándolas como nuevos documentos a la base de datos vectorial para mejorar la precisión del sistema.\nDELETE /solutions: Elimina una solución específica de la base de datos vectorial.\n\n\n\n4.3. Componentes clave de tickets\n\nGET /tickets: Recupera una lista de todos los tickets almacenados en la base de datos de MongoDB.\nPOST /tickets: Permite añadir un nuevo ticket a la base de datos, con campos como título, descripción y categoría.\nPATCH /tickets/{ticket_reference}: Actualiza los metadatos de un ticket existente, como su estado de solución (is_solved)."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension_datos.html",
    "href": "ct_chatbot/quarto/comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n['nombre', 'clave', 'categoria', 'categoria', 'marca', 'tipo',\n       'modelo', 'descripcion', 'descripcion_corta', 'palabrasClave']\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension_datos.html#recolección-de-los-datos",
    "href": "ct_chatbot/quarto/comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n['nombre', 'clave', 'categoria', 'categoria', 'marca', 'tipo',\n       'modelo', 'descripcion', 'descripcion_corta', 'palabrasClave']\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension_datos.html#descripción-de-los-datos",
    "href": "ct_chatbot/quarto/comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2 Descripción de los datos",
    "text": "0.2 Descripción de los datos\nAl combinar la información extraída de SQL con las fichas técnicas en formato XML, obtenemos las siguientes variables:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\nNombre\nNombre del producto tal como aparece en la página web o catálogo.\nTexto\n\n\nClave\nCódigo único que distingue al producto de otros en el sistema.\nTexto\n\n\nCategoría\nClasificación o tipo de producto al que pertenece.\nTexto\n\n\nMarca\nNombre de la empresa que fabrica o distribuye el producto.\nTexto\n\n\nTipo\nEspecificación del tipo de producto (por ejemplo, cable, bateria, etc.).\nTexto\n\n\nModelo\nIdentificación del modelo específico del producto.\nTexto\n\n\nDetalles\nDescripción completa y detallada del producto.\nTexto\n\n\nFicha técnica\nInformación técnica detallada sobre el producto.\nTexto\n\n\nResumen\nResumen general del producto y sus características principales.\nTexto"
  },
  {
    "objectID": "ct_chatbot/quarto/comprension_datos.html#exploración-de-los-datos",
    "href": "ct_chatbot/quarto/comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de productos\nPara comenzar, examinaremos la distribución de categorías dentro del catálogo, identificando cuáles son las más representativas entre un total de 247 categorías distintas.\nEl siguiente análisis nos permitirá visualizar las 10 categorías más frecuentes, lo que nos dará una mejor comprensión de la composición del inventario.\n\n\n\nTop 10 categorías\n\n\nVemos que los tóners destacan como la categoría predominante, seguidos por las aplicaciones de seguridad, aunque en este último caso, la diferencia con el resto de las categorías es menos marcada en comparación con la primera.\nDel mismo modo, exploraremos la distribución de marcas en los productos y visualizaremos las 10 más comunes dentro de un total de 195 marcas registradas.\n\n\n\nTop 10 marcas\n\n\nEn este análisis, observamos que la marca BROBOTIX sobresale como la más frecuente en el catálogo. Le sigue MANHATTAN, con una diferencia más reducida respecto a las siguientes marcas, en un patrón similar al que se observó en las categorías.\n\n\n0.3.2 Distribución de las palabras asociadas a los productos\nEn esta sección, analizaremos la cantidad de palabras utilizadas en diferentes descripciones de los productos. Esto nos permitirá entender cómo se estructuran los nombres, descripciones y palabras clave dentro del catálogo.\nPara ello, compararemos la distribución de palabras en los siguientes atributos:\n\nNombre del producto\nDescripción completa\nDescripción corta\nPalabras clave asociadas Este análisis nos ayudará a identificar patrones en la longitud de las descripciones y su posible impacto en la categorización y búsqueda de los productos.\n\n\n\n\nComparación de distribuciones\n\n\nAl analizar las distribuciones, observamos que muchas de las instancias de la descripción corta comienzan con 0, pero luego la distribución se aproxima a una distribución quasi-normal, con un promedio de 5 palabras por instancia. En el caso de las descripciones, aunque no todas las instancias comienzan con 0, la distribución de palabras muestra un promedio de 1 palabra. Finalmente, las palabras en los nombres siguen una distribución aparentemente normal, con un promedio de 8 palabras por instancia."
  },
  {
    "objectID": "ct_chatbot/quarto/comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "ct_chatbot/quarto/comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nObservando el comportamiento de las distribuciones de la gráfica pasada, observamos que debe haber presencia de varios datos nulos, además de una distribución poco común para la variable de descripción ya que el promedio indica que es 1, algo que no se esperaría en una variable de este estilo.\nSi observamos la información de los datos:\nRangeIndex: 6526 entries, 0 to 6525\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   idProductos        6526 non-null   int64 \n 1   nombre             6526 non-null   object\n 2   clave              6526 non-null   object\n 3   categoria          6526 non-null   object\n 4   marca              6526 non-null   object\n 5   tipo               6526 non-null   object\n 6   modelo             6526 non-null   object\n 7   descripcion        6526 non-null   object\n 8   descripcion_corta  763 non-null    object\n 9   palabrasClave      6268 non-null   object\n 10  detalles_precio    6526 non-null   object\ndtypes: int64(1), object(10)\nmemory usage: 561.0+ KB\nAquí vemos que la razón por la cual la distribución de descripción corta empezaba en 0, era porque alrededor del 89% son datos nulos. Sin embargo vemos que la descripción no tiene datos nulos, pero aún así sigue siendo curioso que la cantidad de palabras en promedio sea 1. Para esto analizaremos esta columna, contando sus valores únicos.\ndescripcion\n0                                                                                                                   4983\n                                                                                                                    1460\nTipo: Limpiador& Función: Para computadoras& Características: Limpieza profunda y protección antiestática              1\nColor: Negro& Compatible: L200                                                                                         1\nTipo: Vertical sencillo&#38; Compatible: Para rack de 42U&#38; Ducto: 4x4 pulgadas&#38; Color: Negro texturizado       1\nName: count, dtype: int64\nSi observamos en los datos, la gran mayoría de los datos tienen escrito el valor 0 (en tipo string). Y el segundo valor más frecuente son un espacio en blanco."
  },
  {
    "objectID": "ct_chatbot/quarto/documentacion.html",
    "href": "ct_chatbot/quarto/documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\nPodman: Gestión y ejecución de los contenedores de la aplicación (API, frontend y otros microservicios) que consumen dichas bases de datos.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nIP=\nPORT=\nUSER=\nPSSWD=\nDB=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nSUCURSALES_URL=\"\"  # Url de la sección de sucursales\nRELOAD_VECTORS_POST= \"https://localhost:8000/internal/reload_vectorstores\"\nURL=''           # Url del servicio de fichas tecnicas\nTOKEN_API=\nTOKEN_CT=\nCONTENT_TYPE=\nCOOKIE=\nDOMINIO=\nBOUNDARY=\n\n# Conexión a MongoDB\nMONGO_URI=\"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB=\"\"\nMONGO_COLLECTION_SESSIONS=\"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP=\"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS=\"tbl_productos\"\nMONGO_COLLECTION_SALES=\"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS=\"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones que están dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nPara cada promoción o promociones nuevas que fueron cargadas después del inicio del mes, este método busca promociones faltantes y los agrega a la base de datos.\npython3 -c \"from ct.ETL.pipeline import update_sales; update_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (recomendación opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo update_products.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/update_products.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/update_products.sh\n\n# Probar manualmente \n~/proyectoCT/update_products.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/update_products.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/update_products.sh\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/update_products.log\nPara actualizar las ofertas, que cada mes se cargan, solo es necesario agregar la siguiente tarea:\nchmod +x ~/proyectoCT/reload_sales.sh\n\n~/proyectoCT/reload_sales.sh\n\ncrontab -e\n\n# Cada 1° y 2° de cada mes se ejecuta la tarea a las 9 en punto (Hermosillo)\n0 9 1,2 * * ~/proyectoCT/reload_sales.sh\n\ncrontab -l\n\ncat ~/proyectoCT/logs/reload_sales.log\nPara agregar ofertas que se vayan agregando a lo largo de la semana:\nchmod +x ~/proyectoCT/update_sales.sh\n\n~/proyectoCT/update_sales.sh\n\n# Agregar la tarea\ncrontab -e\n\n# A partir del 2 de cada mes hasta que acabe el mes, agrega promociones que se hayan subido después de la fecha inicial, de 10 a 7 pm (Hermosillo)\n0 10-19 2-31 * 1-6 ~/proyectoCT/update_sales.sh \n\ncrontab -l\n\ncat ~/proyectoCT/logs/update_sales.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nInsertar/editar tareas\nI\n\n\nGuardar las tareas\n:wq\n\n\nNo hacer ningún cambio\n:q!\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarias para que no hayan problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "ct_chatbot/quarto/documentacion.html#manual-de-instalación-y-despliegue.",
    "href": "ct_chatbot/quarto/documentacion.html#manual-de-instalación-y-despliegue.",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\nPodman: Gestión y ejecución de los contenedores de la aplicación (API, frontend y otros microservicios) que consumen dichas bases de datos.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nIP=\nPORT=\nUSER=\nPSSWD=\nDB=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nSUCURSALES_URL=\"\"  # Url de la sección de sucursales\nRELOAD_VECTORS_POST= \"https://localhost:8000/internal/reload_vectorstores\"\nURL=''           # Url del servicio de fichas tecnicas\nTOKEN_API=\nTOKEN_CT=\nCONTENT_TYPE=\nCOOKIE=\nDOMINIO=\nBOUNDARY=\n\n# Conexión a MongoDB\nMONGO_URI=\"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB=\"\"\nMONGO_COLLECTION_SESSIONS=\"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP=\"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS=\"tbl_productos\"\nMONGO_COLLECTION_SALES=\"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS=\"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones que están dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nPara cada promoción o promociones nuevas que fueron cargadas después del inicio del mes, este método busca promociones faltantes y los agrega a la base de datos.\npython3 -c \"from ct.ETL.pipeline import update_sales; update_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (recomendación opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo update_products.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/update_products.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/update_products.sh\n\n# Probar manualmente \n~/proyectoCT/update_products.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/update_products.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/update_products.sh\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/update_products.log\nPara actualizar las ofertas, que cada mes se cargan, solo es necesario agregar la siguiente tarea:\nchmod +x ~/proyectoCT/reload_sales.sh\n\n~/proyectoCT/reload_sales.sh\n\ncrontab -e\n\n# Cada 1° y 2° de cada mes se ejecuta la tarea a las 9 en punto (Hermosillo)\n0 9 1,2 * * ~/proyectoCT/reload_sales.sh\n\ncrontab -l\n\ncat ~/proyectoCT/logs/reload_sales.log\nPara agregar ofertas que se vayan agregando a lo largo de la semana:\nchmod +x ~/proyectoCT/update_sales.sh\n\n~/proyectoCT/update_sales.sh\n\n# Agregar la tarea\ncrontab -e\n\n# A partir del 2 de cada mes hasta que acabe el mes, agrega promociones que se hayan subido después de la fecha inicial, de 10 a 7 pm (Hermosillo)\n0 10-19 2-31 * 1-6 ~/proyectoCT/update_sales.sh \n\ncrontab -l\n\ncat ~/proyectoCT/logs/update_sales.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nInsertar/editar tareas\nI\n\n\nGuardar las tareas\n:wq\n\n\nNo hacer ningún cambio\n:q!\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarias para que no hayan problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "ct_chatbot/quarto/documentacion.html#documentación-técnica-del-código",
    "href": "ct_chatbot/quarto/documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2 Documentación técnica del código",
    "text": "2 Documentación técnica del código\n\n2.1 Estructura de carpetas y módulos\nEl proyecto sigue una estructura modular para facilitar la gestión y el mantenimiento. A continuación, se detalla el propósito de los módulos principales y algunas de sus funciones clave:\nct/langchain/tool_agent.py\nEste archivo contiene la lógica principal del agente conversacional, incluyendo la interacción con herramientas externas, gestión del historial de conversación, uso de MongoDB, y conexión con el modelo que se esté utilizando en el momento a través de LangChain y OpenAI.\n\nClases y funciones clave:\n\nclass ToolAgent:\n\n__init__:\n\nPropósito: Inicializa el agente, configurando el modelo LLM, conectando a MongoDB para la persistencia de sesiones e historial, definiendo el prompt principal del sistema y registrando las herramientas disponibles.\nComportamiento:\n\nEstablece self.model a “gpt-4.1” (O cualquier modelo disponible).\nInicializa las conexiones a las colecciones de MongoDB (sessions, message_backup).\nDefine self.prompt como un ChatPromptTemplate que guía el comportamiento del agente, incluyendo instrucciones de formato de respuesta y el manejo del historial (chat_history).\nDefine self.tools como una lista de objetos Tool y StructuredTool, que el agente puede invocar. Estas herramientas incluyen search_information_tool, inventory_tool y sales_rules_tool, cada una con su descripción y esquema de argumentos (args_schema) cuando aplica.\nself.executor se inicializa a None y se construye bajo demanda.\n\n\nclear_session_history(self, session_id: str) -&gt; bool:\n\nPropósito: Limpia el historial de mensajes (last_messages) para una sesión de usuario específica en la base de datos de MongoDB. Limpia el historial de mensajes para una sesión en particular.\nParámetros:\n\nsession_id (str): El identificador único de la sesión cuyo historial se desea borrar.\n\nRetorna: bool: True si la operación fue exitosa, False en caso de error.\nComportamiento: Actualiza el documento de la sesión en MongoDB, estableciendo last_messages como una lista vacía. Maneja excepciones de PyMongo y otras.\n\nensure_session(self, session: str) -&gt; dict:\n\nPropósito: Garantiza que exista una entrada para la sesion_id en la colección sessions de MongoDB. Si no existe, la crea; si existe, actualiza la marca de tiempo de la última actividad.\nParámetros:\n\nsession_id (str): El identificador de la sesión.\n\nRetorna: dict: El documento de la sesión actualizado o recién creado.\nComportamiento: Utiliza update_one con $setOnInsert y $set para manejar la lógica de upsert y actualización de actividad.\n\nbuild_executor(self):\n\nPropósito: Construye el AgentExecutor de LangChain, que es el componente principal que orquesta la interacción entre el LLM, las herramientas y el prompt.\nParámetros: Ninguno (usa atributos de la clase).\nComportamiento:\n\nCrea un ChatOpenAI LLM con el modelo y la configuración de streaming.\nCrea un agente de funciones de OpenAI (create_openai_functions_agent) vinculando el LLM, las herramientas y el prompt.\nInicializa self.executor como una instancia de AgentExecutor, configurándolo para ser verbose=False y con un max_iterations para controlar la profundidad de la ejecución del agente.\n\n\nrun:\n\nasync def run(\n session_id: str, \n question: str, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta una consulta del usuario a través del agente, gestiona el historial de chat, recopila métricas y transmite la respuesta en tiempo real.\nParámetros:\n\nsession_id (str): ID de la sesión del usuario.\nquestion (str): La pregunta del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario, usado en el prompt del LLM.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta a medida que se generan.\nComportamiento:\n\nRecupera el historial completo de la sesión (get_session_history).\nTrunca el historial (trim_messages) para ajustarse a la ventana de contexto del LLM, priorizando los mensajes más recientes.\nInicializa un TokenCostProcess y CostCalcAsyncHandler para el seguimiento de tokens y costos.\nSi el executor no está construido, llama a build_executor().\nDefine los inputs para el executor, incluyendo la query, chat_history, listaPrecio y session_id.\nUtiliza self.executor.astream() para obtener la respuesta en streaming.\nAcumula los fragmentos de la respuesta completa.\nEn el bloque finally, calcula la duración y los metadatos de la interacción.\nPersiste los mensajes del usuario y del asistente en las colecciones sessions y message_backup de MongoDB.\n\nget_session_history(self, session_id: str) -&gt; list[BaseMessage]:\n\nPropósito: Recupera el historial de mensajes de una sesión específica desde MongoDB y lo convierte a objetos BaseMessage de LangChain.\nParámetros:\n\nsession_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: list[baseMessage]: Una lista de objetos HumanMessage y AIMessage que representan el historial de conversación.\nComportamiento: Consulta la colección sessions en MongoDB para el session_id dado y mapea los mensajes almacenados a los tipos de mensaje de LangChain.\n\nadd_message\n\ndef add_message(\n session_id: str, \n message_type: str, \n content: str, \n metadata: dict = None): \n\nPropósito: Añade un nuevo mensaje (de usuario o asistente) al historial de last_messages de una sesión en MongoDB, manteniendo un tamaño fijo para optimizar el rendimiento.\nParámetros:\n\nsession_id (str): ID de la sesión.\nmessage_type (str): Tipo de mensaje, puede ser “human” o “assistant”.\ncontent (str): Contenido textual del mensaje.\n\nComportamiento:\n\nCrea un diccionario short_msg con el tipo, contenido y timestamp.\nUtiliza $push con $each, $sort y $slice para añadir el nuevo mensaje y truncar la lista last_messages a los últimos 24 mensajes (configurable).\n\nadd_message_backup\n\ndef add_message_backup(\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un respaldo completo de cada interacción (pregunta del usuario y respuesta completa del asistente) junto con métricas detalladas en la colección message_backup de MongoDB para análisis posterior.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta original del usuario.\nfull_answer (str): La respuesta completa generada por el asistente.\nmetadata (dict): Diccionario con metadatos adicionales (tokens, costo, duración, modelo utilizado).\n\nComportamiento: Inserta un nuevo documento en message_backup con toda la información relevante para análisis posterior.\nadd_irrelevant_message\n\ndef add_irrelevant_message(\n self,\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un mensaje etiquetado como “irrelevante” en la colección message_backup. Esto es útil para el monitoreo y posible re-entrenamiento del clasificador.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta del usuario clasificada como irrelevante.\nfull_answer (str): La respuesta generada por el moderador para consultas irrelevantes.\n\nComportamiento: Inserta un nuevo documento en message_backup con el campo label establecido en False.\nmake_metadata\n\ndef make_metadata(\n self,\n token_cost_process: TokenCostProcess,\n duration: float = None) -&gt; dict : \n\nPropósito: Genera un diccionario con metadatos de la interacción, incluyendo información sobre el costo, los tokens utilizados y el tiempo de procesamiento.\nParámetros:\n\ntoken_cost_process (TokenCostProcess): Objeto que contiene información de los tokens.\nduration (float): Duración de la ejecución en segundos.\n\nRetorna: dict: Diccionario con metadatos.\n\n\n\nct/tools/search_information.py\nEste módulo define la herramienta search_information_tool, que permite al chatbot realizar búsquedas semánticas en las bases de datos vectoriales de productos y promociones para encontrar elementos relevantes.\n\nClases y funciones clave:\n\nvectorstore:\n\nPropósito: Una instancia global de LangchainVectorStore que carga el vector store combinado de productos y promociones desde SALES_PRODUCTS_VECTOR_PATH.\n\nretriever_productos:\n\nPropósito: Un retriever configurado para buscar similitudes en el vector store, filtrando específicamente por documentos de la colección “productos”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"productos\"}.\n\nretriever_promociones:\n\nPropósito: Un retriever configurado de manera similar, pero filtrando por documentos de la colección “promociones”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"promociones\"}.\n\nparse_page_content (content):\n\nPropósito: Una función auxiliar interna que parsea el page_content de un documento de LangChain (que es una cadena de texto concatenada) de nuevo a un diccionario de clave-valor.\nParámetros:\n\ncontent (str): La cadena de texto del page_content del documento.\n\nRetorna: dict: Un diccionario con las características del producto/promoción.\nComportamiento: Utiliza expresiones regulares para dividir la cadena por . y luego por : para extraer las claves y valores.\n\nsearch_information_tool(query) -&gt; dict:\n\nPropósito: Busca productos y promociones relevantes en las bases de datos vectoriales utilizando la búsqueda semántica.\nParámetros:\n\nquery (str): La consulta de búsqueda del usuario.\n\nRetorna: dict: Un diccionario que contiene dos listas: \"Promociones\" y \"Productos\", donde cada lista contiene diccionarios de los resultados encontrados.\nComportamiento:\n\nInvoca retriever_promociones.invoke(query) y retriever_productos.invoke(query) para obtener los documentos más relevantes de cada colección.\nUtiliza parse_page_content() para transformar el page_content de cada documento recuperado en un formato de diccionario estructurado.\n\n\n\n\nct/tools/inventory.py\nEste módulo define la herramienta inventory_tool, que permite al chatbot consultar la disponibilidad, precio y moneda de un producto específico en la base de datos MySQL.\n\nClases y funciones clave:\n\nclass InventoryInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave única del producto a consultar.\nlistaPrecio (int): El ID de la lista de precios a considerar para la consulta.\n\n\ninventory_tool(clave: str, listaPrecio: int) -&gt; str:\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\n\nRetorna: str: Una cadena de texto formateada con la información del producto (claave, precio original, moneda, existencias, si está en promoción) o un mensaje de promoción no encontrada.\nComportamiento:\n\nConstruye una consulta SQL que une las tablas productos, existencias, precio y promociones.\nSe conecta a MySQL, ejecuta la consulta con los parámetros proporcionados.\nFormatea el resultado para indicar la moneda (MXN/USD) y el estado de promoción (si el producto en cuestión está o no en promoción).\nIncluye manejo de errores para problemas de conexión a la base de datos o errores inesperados.\n\n\n\n\nct/tools/sales_rules_tool.py\nEste módulo define la herramienta sales_rules_tool, que permite al chatbot aplicar reglas de promoción y calcular el precio final de un producto, considerando la lista de precios y la sucursal del usuario.\n\nClases y funciones clave:\n\nSUCURSALES:\n\nPropósito: Un diccionario global cargado desde un archivo JSON (ID_SUCURSAL) que mapea nemónicos de sucursal a sus IDs.\n\nclass SalesInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta sales_rule_tool utilizando Pydantic.\nAtributos:\n\nclave (str): La clave única del producto en promoción.\nlistaPrecio (int): El ID de la lista de precios a considerar.\nsession_id (str): El ID de la sesión del usuario, utilizado para inferir la sucursal.\n\n\nobtener_id_sucursal(session_id: str) -&gt; str:\n\nPropósito: Extrae el ID de la sucursal a partir del session_id del usuario, utilizando patrones predefinidos (e.g., “XXCTIN” o nemónicos como “HMO”).\nParámetros:\n\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: El ID de la sucursal como una cadena.\nComportamiento: Utiliza expresiones regulares para extraer el nemónico o ID de la sucursal del session_id y lo busca en el diccionario SUCURSALES. Lanza ValueError si no puede extraer o encontrar la sucursal.\n\nquery_sales():\n\nPropósito: Retorna la consulta SQL para obtener los detalles de la promoción más relevante para un producto, lista de precios y sucursal específicos.\nParámetros: Ninguno.\nRetorna: str: La cadena de la consulta SQL.\nComportamiento: La consulta filtra por promociones activas, producto, lista de precios y sucursal, ordenando por fecha de inicio para obtener la promoción más reciente.\n\nsales_rules_tool(clave: str, listaPrecio: int, session_id: str) -&gt; str:\n\nPropósito: Aplica las reglas de promoción para un producto dado, calculando el precio final y generando un mensaje descriptivo para el usuario.\nParámetros:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: Una cadena de texto formateada que describe la promoción aplicada (precio, final, descuento, condiciones, vigencia) o un mensaje si el producto no está en promoción.\nComportamiento:\n\nObtiene el id_sucursal usando obtener_id_sucursal().\nSe conecta a MySQL y ejecuta query_sales() para obtener los detalles de la promoción.\nEvalúa diferentes tipos de promociones (precio de oferta, descuento porcentual, “en compra de X recibe Y”).\nCalcula el precio_final y construye un mensaje descriptivo.\nManeja casos donde la promoción no está vigente o el producto no se encuentra en promoción.\nIncluye manejo de errores para problemas de base de datos o errores inesperados.\n\n\n\n\nct/tools/status.py\nEste módulo define la herramienta status_tool, que permite al chatbot obtener el estado actual de un pedido a través de su número de factura.\n\nClases y funciones clave:\n\nclass StatusInput:\n\nPropósito: Modelo de datos Pydantic para validar y describir el argumento de entrada factura requerido por la herramienta.\nComportamiento: Asegura que el número de factura sea una cadena de texto.\n\nstatus_tool(factura: str) -&gt; str:\n\nPropósito: Consulta una base de datos de MongoDB para encontrar el estado de un pedido específico usando su número de factura.\nParámetros: factura (str): El número de factura del pedido.\nRetorna: str: Una descripción textual del estado del pedido (ej. “Pedido entregado al domicilio”, “Pedido en generación”).\nComportamiento:\n\nConecta a la base de datos de MongoDB. Realiza una consulta para encontrar el documentos del pedido por su folio (factura).\nRealiza una consulta para encontrar el documento del pedido por su folio (factura).\nSi el pedido es encontrado, navega al último estado registrado en el historial de estados (estatus).\nUtiliza una declaración match para mapear los estados de la base de datos (ej. ‘Pendiente’, ‘Enviado’, ‘Entregado’) a descripciones amigables para el usuario.\nManeja casos especiales como el estado de ‘Transito’ para formatear la fecha y hora de manera legible.\nDevuelve un mensaje apropiado si el pedido no es encontrado.\n\n\n\n\nct/moderation/query_moderator.py\nEste módulo se encarga de clasificar las consultas del usuario (relevante, irrelevante, inapropiado) y de gestionar el comportamiento inapropiado, incluyendo la aplicación de sanciones temporales.\n\nClases y funciones claves:\n\nclass QueryModerator:\n\n__init__(model: str = \"gemma3:4b\", assistant : ToolAgent = None):\n\nPropósito: Inicializa el moderador de consultas, configurando el modelo LLM para clasificación y una referencia al ToolAgent para interactuar con la base de datos de sesiones.\nParámetros:\n\nmodel (str): Nombre del modelo de Ollama a utilizar para la clasificación de consultas (por defecto “gemma3:4b”).\nassistant (ToolAgent): Instancia del ToolAgent para acceder a la gestión de sesiones en MongoDB.\n\n\nclassify_query(query: str) -&gt; str\n\nPropósito: Clasifica la consulta del usuario como ‘relevante’, ‘irrelevante’ o ‘inapropiado’ utilizando un modelo de lenguaje.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\n\nRetorna: Un str con una de las clasificaciones relevante, irrelevante, o inapropiado.\nComportamiento:\n\nUtiliza ollama.generate() con un system_prompt predefinido (_classification_prompt) para guiar la clasificación del modelo.\nConfigura opciones del modelo como temperature = 0 para un comportamiento determinista.\n\n\n_classification_prompt(self) -&gt; str:\n\nPropósito: Retorna el system prompt utilizado por el modelo de clasificación para categorizar las consultas del usuario.\nParámetros: Ninguno.\nRetorna: str: La cadena de texto del system prompt.\nComportamiento: Define las reglas y ejemplos para que el LLM clasifique las consultas en las tres categorías.\n\npolite_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida y amigable cuando la consulta del usuario es clasificada como ‘irrelevante’.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con la respuesta cortés.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y confiabilidad en la respuesta.\n\nban_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida que advierte al usuario sobre el uso de lenguaje inapropiado y las posibles consecuencias.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con el mensaje de advertencia.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y control de tono.\n\nevaluate_inappropriate_behavior(self, session: dict, query: str):\n\nPropósito: Evalúa el comportamiento inapropiado del usuario, incrementa el contador de intentos y determina la duración de una posible sanción (baneo progresivo).\nParámetros:\n\nsession (dict): El documento de la sesión del usuario, que contiene el historial de intentos inapropiados y el estado de baneo.\nquery (str): La consulta inapropiada actual del usuario.\nRetorna: tuple[str, int, Optional[datetime]]: Una tupla que contiene:\n\nmsg (str): El mensaje de sanción a mostrar al usuario.\ntries (int): El número actualizado de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la cual el usuario estará baneado (o None si es solo una advertencia).\n\nComportamiento:\n\nImplementa una lógica de escalamiuento progresivo de sanciones (advertencia, 1 min, 3 min, 10 min, 1 hora, 1 día, 7 días) basada en el número de intentos.\nReinicia el contador de intentos si ha pasado suficiente tiempo desde el último incidente.\n\n\n\ncheck_if_banned(self, session: dict) -&gt; Optional[str]:\n\nPropósito: Verifica si el usuario asociado a una sesión está actualmente baneado. Si el baneo ha expirado, limpia el estado de baneo en la base de datos. Verifica si el usuario está actualmente baneado.\nParámetros:\n\nsession (dict): El documento de la sesión del usuario.\n\nRetorna: Optional[str]: Un mensaje de baneo si el usuario está actualmente restringido, o None si no está baneado o si el baneo ha expirado.\nComportamiento: Compara la fecha y hora actual con la fecha banned_until de la sesión. Si el baneo ha expirado, actualiza la sesión en MongoDB para eliminar el campo banned_until.\n\nupdate_inappropriate_session(self, session, tries, banned_until):\n\nPropósito: Actualiza los campos relacionados con el comportamiento inapropiado (inappropiate_tries, last_inappropiate, banned_until) en el documento de la sesión del usuario en MongoDB.\nParámetros:\n\nsession_id (str): ID de la sesión.\ntries (int): El número de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la que el usuario está baneado (o None).\n\nComportamiento: Realiza una operación de update_one en la colección sessions para establecer los campos especificados.\n\n\n\n\nct/langchain/moderated_tool_agent.py\nEste módulo orquesta el flujo completo de una consulta de usuario, incluyendo la moderación de contenido y la delegación de la consulta al agente principal de herramientas (ToolAgent) si es relevante.\n\nClases y funciones claves:\n\nclass ModeratedToolAgent:\n\n__init__(self):\n\nPropósito: Inicializa el ModeratedToolAgent, creando instancias de ToolAgent y QueryModerator y vinculándolos para coordinar el flujo de la conversación.\nComportamiento:\n\nCrea self.tool_agent para manejar la lógica principal del chatbot y la interración con herramientas.\nCrea self.moderator para la clasificación y gestión de comportamiento, pasándole self.tool_agent para que el moderador pueda actualizar el estado de la sesión.\n\n\nrun\n\nasync def run(\n query: str, \n session_id: str = None, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta el flujo completo de una consulta del usuario, desde la verificación de baneo y la clasificación, hasta la generación de la respuesta (relevante, irrelevante, inapropiado) y su streaming.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\nsession_id (str): ID de la sesión del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta final del chatbot.\nComportamiento:\n\nAsegura la existencia de la sesión (tool_agent.ensure_session).\nVerifica si el usuario está baneado (moderator.check_if_banned). Si lo está, cede el mensaje de baneo y termina.\nClasifica la query (moderator.classify_query).\nBasado en la label de clasificación:\n\nSi es relevante, delega la ejecución al tool_agent.run() para obtener una respuesta detallada con herramientas.\nSi es irrelevante, genera una respuesta cortés (moderator.polite_answer()) y la registra.\nSi es inapropiado, evalúa el comportamiento (moderator.evaluate_inappropriate_behavior()), actualiza la sesión (moderator.update_inappropriate_session()) y cede el mensaje de sanción.\nSi la clasificación no es reconocida, cede un mensaje de error genérico.\n\n\n\n\n\nct/chat.py\nEste módulo define los endpoints de la API FastAPI para la interacción del chat y la gestión del historial de sesiones, sirviendo como la interfaz principal entre el frontend y la lógica del chatbot.\n\nFunciones clave:\n\nassistant = ModeratedToolAgent():\n\nPropósito: Inicializa una instancia global de ModeratedToolAgent que será utilizada por todos los endpoints del chat.\nComportamiento: Se crea una única instancia para mantener el estado y las conexiones a la base de datos.\n\nget_chat_history(user_id: str) -&gt; list[dict[str, str]]:\n\nPropósito: Devuelve el historial de chat de un usuario específico en un formato JSON amigable para el frontend. Recupera el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: List[Dict[str, str]]: Una lista de diccionarios, donde cada diccionario representa un mensaje con las claves role (user o bot) y content (el texto del mensaje). Retorna una lista vacía si no hay historial.\nComportamiento: Llama a assistant.tool_agent.get_session_history() para obtener el historial de LangChain y lo transforma al formato JSON deseado.\n\nasync_chat_generator(request: QueryRequest) -&gt; AsyncGenerator[str, None]:\n\nPropósito: Un generador asíncrono que envuelve la función run de la clase ModeratedToolAgent para permitir el streaming de respuestas a los clientes de la API.\nParámetros:\n\nrequest (QueryRequest): Un objeto Pydantic que contiene la consulta del usuario (user_query), el ID del usuario (user_id), y la lista de precios (listaPrecio).\n\nRetorna: AsyncGenerator[str, None]: Cede los fragmentos de respuesta directamente desde el assistant.run().\n\nasync_chat_endpoint(request: QueryRequest) -&gt; StreamingResponse:\n\nPropósito: El endpoint HTTP POST principal para recibir nuevas consultas de chat de los usuarios y devolver respuestas en streaming (Server-Sent Events).\nParámetros\n\nrequest (QueryRequest): Objeto de solicitud Pydantic con la consulta del usuario, ID de usuario y lista de precios.\n\nRetorna: StreamingResponse: Una respuesta HTTP que permite al cliente recibir los fragmentos de la respuesta en tiempo real a medida que se generan.\nComportamiento: Envuelve el async_chat_generator en una StreamingResponse con media_type=\"text/event-stream\".\n\ndelete_chat_history_endpoint(user_id: str) -&gt; str:\n\nPropósito: Endpoint HTTP DELETE para eliminar el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se va a eliminar.\n\nRetorna: str: Una cadena “success” si la eliminación fue exitosa.\nComportamiento: Llama a assistant.tool_agent.clear_session_history() para borrar el historial. Maneja excepciones y levanta una HTTPException en caso de error interno.\n\n\n\nct/main.py\nEs el archivo principal de la aplicación FastAPI. Configura la aplicación, habilita CORS y registra los endpoints definidos en ct.chat.\n\nFunciones clave:\n\napp = FastAPI: Inicializa la aplicación FastAPI.\napp.add_middleware(CORSMiddleware, ...): Configura el middleware de CORS para permitir solicitudes desde cualquier origen, métodos y cabeceras, lo cual es crucial para la integración del widget en diferentes dominios.\n@app.get(\"/history/{user_id}\"): Decorador que mapea la ruta GET /history/{user_id} a la función handle_history.\nhandle_history(user_id: str): Llama a get_chat_history de ct.chat para obtener y retornar el historial.\n@app.post(\"/chat\"): Decorador que mapea la ruta POST /chat a la función handle_chat.\nhandle_chat(request: QueryRequest): Llama a async_chat_endpoint de ct.chat para manejar la solicitud de chat y el streaming de la respuesta.\n@app.delete(\"/history/{user_id}\"): Decorador que mapea la ruta DELETE /history/{user_id} a la función handle_delete_history.\nhandle_delete_history(user_id: str): Llama a delete_chat_history_endpoint de ct.chat para borrar el historial de un usuario.\nif __name__ == \"__main__\":: Bloque de ejecución principal para correr la aplicación con Uvicorn en desarrollo. En producción con Gunicorn.\n\n\nct/ETL/extraction.py:\nMódulo de la capa de Extracción. Responsable de conectarse a la base de datos MySQL para extraer información de productos y promociones, y de interactuar con el servicio externo para obtener fichas técnicas.\n\nClases y funciones clave:\n\nclass Extraction:\n\n__init__(): Inicializa la clase con los parámetros de conexión a MySQL y configura un cloudscraper para la extracción de fichas técnicas.\n\nParámetros: Ninguno explícito, lee de ct.clients.\nComportamiento: Establece scraper con headers personalizados para los tokens de la API y cookies.\n\nids_query() -&gt; str: Retorna la consulta SQL para obtener IDs de productos válidos (con existencias y precios).\nget_valid_ids() -&gt; list: Ejecuta la consulta ids_query y retorna una lista de IDs de productos válidos.\n\nRetorna: list: Lista de IDs de productos.\nComportamiento: Se conecta a MySQL, ejecuta la consulta y maneja errores de conexión.\n\nproduct_query(id) -&gt; str: Retorna la consulta SQL para obtener detalles de un producto específico por ID. Incluye detalles de precios por lista, categoría, marca, etc.\nget_products() -&gt; pd.DataFrame: Extrae la información de todos los productos válidos desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de los productos.\nComportamientos: Itera sobre los IDs válidos, ejecuta product_query para cada uno y consolida los resultados en un DataFrame.\n\ncurrent_sales_query() -&gt; str: Retorna la consulta SQL para obtener las promociones vigentes.\nget_current_sales() -&gt; pd.DataFrame: Extrae las promociones vigentes desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de las promociones.\n\nget_specifications_cloudscraper:\n\nget_specifications_cloudscraper(\n    claves: List[str],\n    max_retries: int = 3,\n    sleep_seconds: float = 0.15\n) -&gt; Dict[str, dict]\nIntenta obtener las fichas técnicas para una lista de claves de productos desde un servicio externo, utilizando cloudscraper para manejar posibles protecciones como Cloudflare.\n\nParámetros:\n\nclaves (List[str]): Lista de claves de productos.\nmax_retries (int): Número máximo de reintentos por cada clave.\nsleep_seconds (float): Tiempo inicial de espera entre reintentos (con backoff exponencial).\n\nRetorna: Dict[str, dict]: Un diccionario donde la clave es la claveProducto y el valor es la ficha técnica en formato JSON.\nComportamiento: Realiza solicitudes POST al url del servicio de fichas técnicas. Implementa lógica de reintentos con backoff controlado y maneja diversos errores HTTP (ej., 403 Forbidden) y errores de JSON/red.\n\n\n\nct/ETL/transform.py\nMódulo de la capa de Transformación. Se encarga de limpiar, unificar y normalizar los datos extraídos, y de persistir las fichas técnicas en MongoDB.\n\nClases y funciones claves:\n\nclass Transform:\n\n__init__(): Inicializa la clase Transform, creando una instancia de Extraction y configurando la conexión a la colección de especificaciones de producto.\n\nParámetros:\n\nspecifications (dict): El diccionario que contiene los datos de la ficha técnica de un producto.\n\nRetorna: Un diccionario estructurado con fichaTecnica (pares nombre-valor de características) y resumen (descripciones cortas y largas).\nComportamiento: Navega a través de la estructura anidada de ProductFeature y SummaryDescription para extraer la información.\n\ntransform_specifications(specs: dict) -&gt; dict: Transforma múltiples especificaciones brutas (obtenidas del servicio externo) en un formato limpio.\n\nParámetros:\n\nspecs (dict): Diccionario de fichas técnicas brutas, donde la clave es la claveProducto.\n\nRetorna: Diccionario con fichas técnicas transformadas y limpias, listas para ser usadas o guardadas.\n\ntransform_products() -&gt; pd.DataFrame: Transforma los datos brutos de productos obtenidos de MySQL en un DataFrame limpio y estandarizado.\n\nRetorna: Un DataFrame con columnas relevantes, detalles concatenados y detalles_precio parseado de JSON.\n\nclean_products() -&gt; dict: Limpia los datos de productos y los enriquece con las fichas técnicas. Primero busca en MongoDB y si no encuentra, las extrae y las guarda.\n\nRetorna: Un diccionario de productos limpios y enriquecidos, listos para la carga final.\nComportamiento: Identifica las claves de productos para las que faltan fichas técnicas en MongoDB, las extrae usando self.data.get_specifications, las transforma y las guarda en la colección specifications. Finalmente, combina las fichas técnicas existentes y nuevas con los datos de productos.\n\ntransform_sales() -&gt; pd.DataFrame: Transforma los datos brutos de promociones (ventas) en un DataFrame limpio.\n\nRetorna: Un DataFrame con información de promociones, fechas formateadas y descuentos con símbolo de porcentaje.\n\nclean_sales() -&gt; dict: Limpia los datos de promociones y los enriquece con fichas técnicas de manera similar a clean_products().\n\nRetorna: Un diccionario de promociones limpias y enriquecidas con fichas técnicas.\n\n\n\n\nct/ETL/load.py\nMódulo de la capa de Carga. Maneja la inserción de los datos transformados en las colecciones de MongoDB y la construcción de la base de datos vectorial.\n\nClases y funciones clave:\n\nclass Load:\n\n__init__(): Inicializa la clase Load, creando una instancia de Transform y configurando las conexiones a las colecciones de MongoDB (products, sales, specifications) y el embedder de OpenAI.\nbuild_content(product: dict, product_features: list) -&gt; str: Contruye el contenido textual de un documento a partir de un diccionario de producto y una lista de características.\n\nParámetros:\n\nproduct (dict): Un diccionario con los datos de un producto.\nproduct_features (list): Lista de claves de características a incluir en el contenido.\n\nRetorna: Una cadena de texto concatenada que resume el producto, adecuada para la vectorización.\n\nmongo_products(): Carga las promociones limpias (procesadas por Transform) en la colección products de MongoDB, realizando upserts.\nmongo_sales(): Carga las promociones limpias (procesadas por Transform) en la colección sales de MongoDB, realizando upserts.\nload_products() -&gt; List[Document]: Carga los productos desde la colección products de MongoDB y los convierte en objetos langchain.schema.Document.\n\nRetorna: Una instancia del índice FAISS.\n\nproducts_vs(): Crea o actualiza el vector store para productos.\n\nComportamiento: Carga los productos desde MongoDB, los vectoriza en lotes (batch_size = 250) para optimizar el uso de memoria, y guarda el índice FAISS localmente en PRODUCTS_VECTOR_PATH.\n\nsales_products_vs(): Crea o actualiza el vector store para ventas/ofertas.\n\nComportamiento: Carga las ofertas desde MongoDB. Luego, carga el vector store de productos existente desde PRODUCTS_VECTOR_PATH y añade las ofertas a este índice (también en lotes), guardando el índice combinado en SALES_PRODUCTS_VECTOR_PATH.\n\n\n\n\nct/ETL/pipeline.py\nEste módulo actúa como el orquestador principal del proceso ETL (Extracción, Transformación, Carga). Centraliza la ejecución de las etapas de obtención, limpieza, enriquecimiento y carga de datos, asegurando que los vector stores de productos y promociones estén siempre actualizados.\n\nClases y funciones clave:\n\nrun_etl_pipeline():\n\nPropósito: Función principal que coordina la ejecución secuencial de todas las fases del pipeline ETL.\nComportamiento:\n\nInicializa una instancia de la clase Load.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.load_sales() para extraer, transformar y preparar los datos de promociones.\nInvoca load.sales_products_vs() para crear o actualizar el vector store de promociones, integrándolos con el vector store de productos existente,\n\nUso: Diseñada para ser el punto de entrada para la actualización programada o manual de la base de conocimientos del chatbot.\n\n\n\n\n\n2.2 Modelos LLM utilizados\nEl sistema utiliza una combinación estratégica de modelos de lenguaje para optimizar la funcionalidad y los costos:\n\nClasificación de consultas y respuestas iniciales (Ollama - gemma3:4b):\n\nFunción: Este modelo open-source, cargado localmente a través de Ollama, es el primer punto de contacto. Su función principal es clasificar las consultas de los usuarios como relevantes (productos que se ofrecen en la empresa), irrelevantes (cualquier producto o tema fuera del ámbito de negocio), o inapropiado (lenguaje ofensivo).\nVentaja: Permite una gestión eficiente de consultas no relacionadas con el negocio sin incurrir en costos de API’s comerciales, y es ideal para respuestas de “baneo” o corteses.\n\nGeneración de respuestas relevantes (OpenAI - gpt-41):\n\nFunción: Este modelo avanzado de OpenAI es el encargado de generar las respuestas detalladas y contextualizadas para las consultas clasificadas como relevantes. Trabaja en conjunto con la cadena RAG para integrar la información recuperada de la base de datos vectorial.\nVentaja: Ofrece alta calidad y precisión en las respuestas, especialmente en el manejo de precios, promociones complejas y detalles de productos, lo que fue validad en pruebas comparativas.\n\n\n\n\n2.3 Puntos de entrada y funciones clave\nEstos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot. Es crucial que aquí se documenten las funciones y clases directamente expuestas o que inician un flujo principal.\n\nModeratedToolAgent.run:\n\nModeratedToolAgent.run(\n  query: str, \n  session_id: str = None, \n  listaPrecio: str = None\n  ) -&gt; AsyncGenerator[str, None]:\n\nEs la función principal que orquesta el glujocompleto de una consulta de usuario.\nParámetros:\n\nquery (str): La pregunta o entrada del usuario.\nsession_id (str): Un identificador único para la sesión del usuario, utilizado para mantener el historial correspondiente.\nlistaPrecio (str): El nivel o clave de precio específico del cliente, que se pasa al LLM para asegurar la precisión de los precios en las consultas dinámicas (ej. precios y promociones).\n\nComportamiento:\n\nVerifica si el usuario está actualmente baneado (usando QueryModerator.check_if_banned). Si es así, retorna un mensaje de baneo.\nClasifica la query (usando QueryModerator.classify_query) en una de las categorías: relevante, irrelevante, o inapropiado.\nBasado en la clasificación:\n\n\nSi es relevante, delega la ejecución al ToolAgent.run() para obtener una respuesta detallada con el uso de herramientas y la base de datos vectorial.\nSi es irrelevante, retorna una respuesta predefinida y cortés (QueryModerator.polite_answer()) y registra la interacción.\nSi es inapropiado, evalúa el comportamiento (QueryModerator.evaluate_inappropriate_behavior()), actualiza el estado de baneo en la sesión del usuario (QueryModerator.update_inappropriate_session()) y retorna el mensaje de sanción apropiado.\n\n\nCede fragmentos de la respuesta en tiempo real (streaming).\nRegistra la interacción completa, incluyendo la pregunta, respuesta y metadatos relevantes para análisis futuro.\n\nToolAgent.run:\n\nToolAgent().run(\n session_id: str, \n question: str, \n listaPrecio: str\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Responde a consultas relevantes de productos, utilizando el agente de LangChain con acceso a herramientas y memoria de conversación. Es la función que el ModeratedToolAgent invoca cuando una consulta es clasificada como relevante.\nParámetros:\n\nsession_id (str): ID de la sesión para recuperar/actualizar el historial.\nquestion (str): La pregunta original del usuario.\nlistaPrecio (str): El nivel de precios del cliente.\n\nComportamiento:\n\nAsegura la existencia de la sesión en MongoDB (ensure_session).\nRecupera el historial de la sesión (get_session_history) y lo trunca para ajustarse a la ventana de contexto del LLM (trim_messages).\nInicializa el AgentExecutor si no ha sido construido.\nEjecuta la query a través del AgentExecutor de LangChain, que orquesta el uso del LLM y las herramientas (search_information_tool, inventory_tool, sales_rules_tool) según la necesidad de la consulta.\nTransmite la respuesta en fragmentos (astream()).\nRegistra la interacción completa (pregunta, respuesta, métricas como tokens y costo) en las colecciones sessions y message_backup de MongoDB.\n\nload.py::products_vs() y load.py::sales_products_vs():\n\nPropósito: Funciones clave para la creación y actualización incremental de las bases de datos vectoriales (FAISS) de productos y ofertas, respectivamente. Orquestan el proceso de carga de documentos desde MongoDB y su vectorización.\nComportamiento:\n\nproducts_vs(): Carga todos los productos de la colección products de MongoDB, los convierte a langchain.schema.Document y los vectoriza en lotes (batch_size=250) utilizando OpenAIEmbeddings. Finalmente, guarda el índice FAISS resultante en PRODUCTS_VECTOR_PATH.\nsales_products_vs(): Carga las ofertas de la colección sales de MongoDB. Luego, carga el índice de productos existente (desde PRODUCTS_VECTOR_PATH) y añade las ofertas a este mismo índice, también en lotes (batch_size=200). Finalmente, guarda el índice combinado (productos + ofertas) en SALES_PRODUCTS_VECTOR_PATH. Esta estrategia asegura que las ofertas se integren sin necesidad de re-vectorizar todo el catálogo de productos, optimizando tiempo y recursos."
  },
  {
    "objectID": "ct_chatbot/quarto/documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "ct_chatbot/quarto/documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3 Guía de entrenamiento y mejora",
    "text": "3 Guía de entrenamiento y mejora\nFlujo de Datos (ETL)\nLa información que alimenta la base de conocimientos del chatbot sigue un proceso ETL (Extracción, Transformación, Carga) estructurado que garantiza que el modelo tenga acceso a datos actualizados, limpios y ricos en contexto semántico.\nPara una descripción detallada de cada etapa del proceso ETL (Extracción de MySQL y servicio externo de fichas técnicas, Transformación para limpieza, unificación y estructuración, y Carga en MongoDB y la base de datos vectorial FAISS), por favor, consulta el documento “Preparación de los datos”.\n\n3.1 Generación de la base de datos vectorial\nLa base de datos vectorial FAISS es el corazón del sistema RAG, almacenando las representaciones vectoriales de la información de productos y promociones para búsquedas de similitud eficientes. Su generación y actualización son parte integral del proceso ETL, el cual asegura que el modelo tenga acceso a una base de conocimiento robusta y actualizada.\nLos detalles sobre el proceso de creación del índice vectorial, el uso de embeddings, la estrategia de procesamiento en lotes y la inclusión incremental de ofertas se encuentran descritos exhaustivamente en el documento “Preparación de los datos”.\n\n\n3.2 Recomendaciones para futura mejora\n\nEmbeddings locales (Ollama):\n\nPara reducir los costos asociados con los embeddings de OpenAI y disminuir la dependencia de servicios externos, se recomienda realizar pruebas con los modelos de embeddings disponibles a través de Ollama (u otras librerías de embeddings locales).\n\nIndexado incremental:\n\nActualmente, la actualización de la base vectorial puede implicar procesar grandes lotes. Para un mantenimiento más eficiente, especialmente si solo cambian algunos productos o sus precios/promociones, se podría implementar una función de actualización a nivel de producto.\n\nMonitoreo avanzado de rendimiento:\n\nAunque ya se registran métricas de costo y duración, se puede profundizar en el monitoreo.\n\nOptimización del manejo de promociones complejas:\n\nLas promociones tipo “en compra X lleva Y” aún presenta desafíos. Podría ser necesario enriquecer el contexto del embedding para estos productos específicos, reglas más explícitas dentro del LLM o creación de plantillas de prompts más específicos para estas promociones."
  },
  {
    "objectID": "ct_chatbot/quarto/documentacion.html#diagrama-de-arquitectura",
    "href": "ct_chatbot/quarto/documentacion.html#diagrama-de-arquitectura",
    "title": "Documentación",
    "section": "4 Diagrama de arquitectura",
    "text": "4 Diagrama de arquitectura\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial. Se ha actualizado para reflejar la implementación de MongoDB y los diferentes flujos.\n\n4.1 Componentes clave\n\nInterfaz de usuario (widget del chatbot): El componente frontal incrustado en la página web de CT Internacional, permitiendo la interacción directa del usuario.\nServicio intermediario PHP: Actúa como un puente seguro entre el frontend (HTTPS) y la API del chatbot (HTTPS pero con certificado autofirmado, o sea, no seguro) para resolver problemas de contenido mixto. Si el backend ya está en en HTTPS con un certificado SSL seguro, este componente puede ser obviado.\nAPI del chatbot (FastAPI): El servicio backend principal que procesa las consultas de los usuarios, orquesta la recuperación de información y se comunica con los modelos de lenguaje.\nBase de datos de productos y promociones (MySQL): Almacena la información transaccional y de precios de los productos y promociones de CT Internacional. Es la fuente original de los datos.\nServicio de fichas técnicas: Fuente de datos para la información detallada y semi-estructurada (XML) de los productos.\nMódulo ETL: Proceso automatizado que:\n\nExtrae datos de MySQL y el servicio de fichas técnicas.\nLimpia, unifica, y transforma los datos, persistiendo las fichas técnicas en MongoDB.\nCarga los datos limpios en colecciones de MongoDB (products, sales, specifications).\n\nBase de datos NoSQL (MongoDB): Almacena las fichas técnicas (specifications), así como el historial de las interacciones de los usuarios (sessions, message_backup).\n\nsessions: Mantiene los últimos n mensajes de cada usuario para recuperación rápida y una experiencia de usuario fluida.\nmessage_backup: Almacena un historial completo de todas las consultas y respuestas con métricas detalladas para fines de análisis y reportes.\n\nModelo de embeddings: Componente encargado de transformar tanto las consultas de los usuarios como la información de los productos en representaciones vectoriales numéricas (usando OpenAI Embeddings).\nBase de datos vectorial (FAISS): Almacena las representaciones vectoriales de la información de productos y ofertas, permitiendo búsquedas de similitud eficientes. Se actualiza con datos del ETL.\nClasificador de consultas (Ollama - gemma3:4b): Componente central que, para consultas relevantes, coordina la búsqueda de información en la base de datos vectorial y contextualiza esta información con la consulta del usuario.\nLLM: El motor principal del chatbot para consultas relevantes, responsable de generar respuestas coherentes y detalladas en base a la consulta contextualizada por el RAG.\nSistema de reportes automatizados: Procesa los datos del message_backup en MongoDB para generar insights sobre el uso del chatbot, intereses de los clientes, costos y rendimiento.\n\n\n\n4.2 Flujo de interacción principal\n\nEl usuario interactúa con el widget del chatbot en la página web.\nLa consulta se envía a la API del chatbot (potencialmente vía el servicio intermediario PHP)\nLa API crea o continúa una sesión de conversación, y la consulta es pasada al asistente conversacional.\nSi la consulta es relevante:\n\n\nsearch_information_tool: busca productos relevantes.\ninventory_tool: obtiene precios, existencias y moneda por clave de producto.\nsales_rules_tool: calcula el precio final considerando promociones o reglas de negocio.\nLa información recuperada se contextualiza y se envía al LLM para generar la respuesta al usuario.\n\n\nSi la consulta es irrelevante o inapropiada, el clasificador genera una respuesta adecuada (cortés o de advertencia) directamente al usuario.\nTodas las interacciones (consultas y respuestas) se registran en las colecciones sessions y message_backup en MongoDB.\nEl sistema de reportes automatizados accede a message_backup para generar análisis de datos.\n\n\n\n4.3 Flujo de datos\n\nEl módulo ETL extrae datos de MySQL y el servicio de fichas técnicas.\nLos datos se transforman y las fichas técnicas se cargan en MongoDB (colección specifications).\nLos datos transformados de products y sales (incluyendo las fichas técnicas obtenidas de MongoDB) son utilizados por el módulo ETL para construir y actualizar la base de datos vectorial.\n\n\n\n\nArquitectura del sistema"
  },
  {
    "objectID": "ct_chatbot/quarto/modelado.html",
    "href": "ct_chatbot/quarto/modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "ct_chatbot/quarto/modelado.html#modelado",
    "href": "ct_chatbot/quarto/modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "ct_chatbot/quarto/modelado.html#evaluación",
    "href": "ct_chatbot/quarto/modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2 Evaluación",
    "text": "2 Evaluación\nCon base en las respuestas generadas durante la etapa de modelado, se llevó a cabo una evaluación cualitativa para analizar la coherencia, relevancia y precisión de las recomendaciones de cada modelo. Este análisis nos permitió identificar oportunidades de mejora en el sistema, así como validar si el comportamiento del modelo es adecuado para continuar con su implementación o si requiere ajustes adicionales.\nA continuación, se presentan las respuestas generadas por el sistema para una serie de consultas simuladas por un usuario. Estas imágenes muestran el resultado del mejor modelo seleccionado (GPT 4o) ante cada solicitud:\n\nConsulta: “¡Hola! Me interesan computadoras de oficina”\n\nConsulta: “También me gustaría ver monitores de 27 pulgadas arriba de 75Hz”\n\nConsulta: “Y un no break gamer”\n\nConsulta: “Y una extensión doméstica”\n\n\nLos resultados obtenidos reflejan un desempeño sólido por parte del sistema. En todos los casos evaluados, las respuestas del chatbot fueron coherentes, alineadas con la base de datos y cumplieron con los criterios definidos:\n\nLas ofertas y promociones fueron correctamente identificadas y presentadas.\nLos precios y descripciones de los productos coincidieron con los datos reales.\nNo se observaron errores de alucinación ni pérdidas de coherencia en la conversación.\n\nEsto sugiere que el modelo es capaz de generar respuestas confiables y útiles para los usuarios, por lo que se considera viable continuar con las siguientes etapas del proyecto o bien escalar el sistema hacia una versión de prueba."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/0_home.html",
    "href": "salsas_castillo_chatbot/quarto/0_home.html",
    "title": "GlorIA: Análisis empresarial desde un enfoque con IA",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información crítica de ventas, finanzas, facturas y más. A través de este asistente conversacional, se busca empoderar al personal de toma de decisiones, proporcionando respuestas rápidas y precisas a sus consultas, y facilitando la toma de decisiones basadas en datos actualizados que cambian en tiempo real.\nEl chatbot se basa en un modelo agéntico MCP (Protocolo de Contexto del Modelo), que combina modelos de lenguaje avanzados con acceso directo a bases de datos relacionales (PostgreSQL) y vectoriales (FAISS). Además de la capacidad de generar reportes en PDF y generación de tablas para una visualización más amigable. La interacción principal se realiza a través de la plataforma de mensajería Telegram.\nEl proyecto se divide en las siguientes fases clave:\n\nComprensión del Negocio: Definición de los objetivos estratégicos y el contexto operativo de Salsas Castillo, enfocados en mejorar la eficiencia en el acceso a la información, el correcto manejo y manipulación de los datos para una generación eficiente de respuestas, y la comunicación interna.\nPreparación: Diseño e implementación de la arquitectura del chatbot, incluyendo la integración de bases de datos relacionales y vectoriales, el desarrollo de herramientas personalizadas, y su flujo de trabajo.\nModelado y Evaluación: La configuración de modelos de lenguaje y la validación de su desempeño a través de consultas controladas.\nDespliegue: Integración del chatbot en el entorno de producción de Telegram y la infraestructura de Salsas Castillo, asegurando su operatividad y accesibilidad.\n\nA través de estas fases, buscamos proporcionar una solución innovadora que transforme la manera en que Salsas Castillo accede y utiliza su información de negocio, impulsando la eficiencia y la agilidad en sus operaciones diarias."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/0_home.html#introducción",
    "href": "salsas_castillo_chatbot/quarto/0_home.html#introducción",
    "title": "GlorIA: Análisis empresarial desde un enfoque con IA",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información crítica de ventas, finanzas, facturas y más. A través de este asistente conversacional, se busca empoderar al personal de toma de decisiones, proporcionando respuestas rápidas y precisas a sus consultas, y facilitando la toma de decisiones basadas en datos actualizados que cambian en tiempo real.\nEl chatbot se basa en un modelo agéntico MCP (Protocolo de Contexto del Modelo), que combina modelos de lenguaje avanzados con acceso directo a bases de datos relacionales (PostgreSQL) y vectoriales (FAISS). Además de la capacidad de generar reportes en PDF y generación de tablas para una visualización más amigable. La interacción principal se realiza a través de la plataforma de mensajería Telegram.\nEl proyecto se divide en las siguientes fases clave:\n\nComprensión del Negocio: Definición de los objetivos estratégicos y el contexto operativo de Salsas Castillo, enfocados en mejorar la eficiencia en el acceso a la información, el correcto manejo y manipulación de los datos para una generación eficiente de respuestas, y la comunicación interna.\nPreparación: Diseño e implementación de la arquitectura del chatbot, incluyendo la integración de bases de datos relacionales y vectoriales, el desarrollo de herramientas personalizadas, y su flujo de trabajo.\nModelado y Evaluación: La configuración de modelos de lenguaje y la validación de su desempeño a través de consultas controladas.\nDespliegue: Integración del chatbot en el entorno de producción de Telegram y la infraestructura de Salsas Castillo, asegurando su operatividad y accesibilidad.\n\nA través de estas fases, buscamos proporcionar una solución innovadora que transforme la manera en que Salsas Castillo accede y utiliza su información de negocio, impulsando la eficiencia y la agilidad en sus operaciones diarias."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/2_preparacion.html",
    "href": "salsas_castillo_chatbot/quarto/2_preparacion.html",
    "title": "Preparación, modelado y evaluación de los datos",
    "section": "",
    "text": "A diferencia de un proceso ETL tradicional con extracción y transformación masiva de datos a un destino intermedio, el proyecto con Salsas Castillo se enfoca en la conexión de datos en tiempo real a través de herramientas SQL, y la preparación de un vector store para documentos internos.\n\n\nLas fuentes de información principales para el chatbot son:\n\nBase de Datos PostgreSQL: Contiene la información transaccional de ventas, facturas, datos financieros consolidados, entra otras tablas. Estos datos se acceden directamente mediante consultas SQL generadas por el LLM.\nDocumentos Internos (Vector Store): Archivos como manuales, reglamentos o cualquier otra información textual estática que se haya procesado para búsquedas semánticas.\n\n\n\n\n\n\nLos datos de ventas y finanzas se acceden directamente desde PostgreSQL en tiempo real. La preparación aquí, se centra en cómo el sistema interactúa con la base de datos:\n\nConexión: Se utiliza psycopg2 para establecer la conexión con la base de datos empresarial. Las credenciales se gestionan de forma segura mediante variables de entorno.\nGeneración de Consultas: El LLM es capaz de generar consultas SQL dinámicamente, basándose en la pregunta del usuario y el esquema de la base de datos. Se han definido reglas específicas en el prompt del sistema (settings/prompts.py) para asegurar la sintaxis correcta.\nManejo de Columnas: El LLM está instruido para inferir el significado de las columnas y, si es necesario, consultar el esquema de la base de datos (sql_db_schema tool) antes de generar una consulta. Esto minimiza errores por nombres de columnas desconocidos.\nMapeo de Productos: Se incluye una lista de nombres de productos (dicts.py) para ayudar al LLM a hacer matches precisos con las presentaciones de productos en la base de datos, mejorando la relevancia de las respuestas.\n\n\n\n\n\nPara la información estática contenida en documentos internos (que no están en las bases de datos SQL), se construye un vector store para permitir búsquedas semánticas:\n\nDocumentos: Los documentos textuales se cargan y se convierten en objetos langchain.schema.Document.\nEmbeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas.\nÍndice FAISS: Se crea un índice FAISS (LangchainVectorStore en langchain/vectorstore.py) a partir de estos embeddings. Este índice se guarda localmente para su uso eficiente.\nActualización: Aunque no se describe un ETL formal, la actualización de este vector store implicaría volver a procesar los documentos modificados o nuevos para regenerar el índice."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/2_preparacion.html#preparación-de-los-datos",
    "href": "salsas_castillo_chatbot/quarto/2_preparacion.html#preparación-de-los-datos",
    "title": "Preparación, modelado y evaluación de los datos",
    "section": "",
    "text": "A diferencia de un proceso ETL tradicional con extracción y transformación masiva de datos a un destino intermedio, el proyecto con Salsas Castillo se enfoca en la conexión de datos en tiempo real a través de herramientas SQL, y la preparación de un vector store para documentos internos.\n\n\nLas fuentes de información principales para el chatbot son:\n\nBase de Datos PostgreSQL: Contiene la información transaccional de ventas, facturas, datos financieros consolidados, entra otras tablas. Estos datos se acceden directamente mediante consultas SQL generadas por el LLM.\nDocumentos Internos (Vector Store): Archivos como manuales, reglamentos o cualquier otra información textual estática que se haya procesado para búsquedas semánticas.\n\n\n\n\n\n\nLos datos de ventas y finanzas se acceden directamente desde PostgreSQL en tiempo real. La preparación aquí, se centra en cómo el sistema interactúa con la base de datos:\n\nConexión: Se utiliza psycopg2 para establecer la conexión con la base de datos empresarial. Las credenciales se gestionan de forma segura mediante variables de entorno.\nGeneración de Consultas: El LLM es capaz de generar consultas SQL dinámicamente, basándose en la pregunta del usuario y el esquema de la base de datos. Se han definido reglas específicas en el prompt del sistema (settings/prompts.py) para asegurar la sintaxis correcta.\nManejo de Columnas: El LLM está instruido para inferir el significado de las columnas y, si es necesario, consultar el esquema de la base de datos (sql_db_schema tool) antes de generar una consulta. Esto minimiza errores por nombres de columnas desconocidos.\nMapeo de Productos: Se incluye una lista de nombres de productos (dicts.py) para ayudar al LLM a hacer matches precisos con las presentaciones de productos en la base de datos, mejorando la relevancia de las respuestas.\n\n\n\n\n\nPara la información estática contenida en documentos internos (que no están en las bases de datos SQL), se construye un vector store para permitir búsquedas semánticas:\n\nDocumentos: Los documentos textuales se cargan y se convierten en objetos langchain.schema.Document.\nEmbeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas.\nÍndice FAISS: Se crea un índice FAISS (LangchainVectorStore en langchain/vectorstore.py) a partir de estos embeddings. Este índice se guarda localmente para su uso eficiente.\nActualización: Aunque no se describe un ETL formal, la actualización de este vector store implicaría volver a procesar los documentos modificados o nuevos para regenerar el índice."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/4_despliegue.html",
    "href": "salsas_castillo_chatbot/quarto/4_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del chatbot para Salsas Castillo ha seguido un enfoque iterativo, centrado en la integración de modelos de lenguaje con las bases de datos existentes y la plataforma de Telegram. Los principales retos se concentraron en asegurar la interacción fluida y precisa del LLM con las bases de datos SQL para consultas dinámicas, así como la correcta gestión del historial de conversación y la generación de reportes en PDF.\nEl objetivo principal de optimizar el acceso a la información de productos y finanzas se ha mantenido constante a lo largo del proyecto, adaptándose a las particularidades de Salsas Castillo.\n\n\nConsiderando los avances en el desarrollo y las pruebas internas, se ha decidido priorizar el despliegue en un entorno de pruebas real para validar el comportamiento del chatbot en condiciones operativas y recopilar feedback directo de los usuarios.\n\n\n\nLa decisión es proceder con la implementación del chatbot en un entorno de pruebas de Telegram. Esto permitirá:\n\nValidar la integración completa con la API de Telegram.\nProbar la conectividad y el rendimiento con las bases de datos PostgreSQL y MongoDB en un entorno real.\nRecopilar feedback de usuarios internos para identificar mejoras en la experiencia de usuario y la precisión de las respuestas.\nAsegurar la robustez y estabilidad del sistema antes de una posible implementación a mayor escala."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/4_despliegue.html#revisión-del-proceso",
    "href": "salsas_castillo_chatbot/quarto/4_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del chatbot para Salsas Castillo ha seguido un enfoque iterativo, centrado en la integración de modelos de lenguaje con las bases de datos existentes y la plataforma de Telegram. Los principales retos se concentraron en asegurar la interacción fluida y precisa del LLM con las bases de datos SQL para consultas dinámicas, así como la correcta gestión del historial de conversación y la generación de reportes en PDF.\nEl objetivo principal de optimizar el acceso a la información de productos y finanzas se ha mantenido constante a lo largo del proyecto, adaptándose a las particularidades de Salsas Castillo.\n\n\nConsiderando los avances en el desarrollo y las pruebas internas, se ha decidido priorizar el despliegue en un entorno de pruebas real para validar el comportamiento del chatbot en condiciones operativas y recopilar feedback directo de los usuarios.\n\n\n\nLa decisión es proceder con la implementación del chatbot en un entorno de pruebas de Telegram. Esto permitirá:\n\nValidar la integración completa con la API de Telegram.\nProbar la conectividad y el rendimiento con las bases de datos PostgreSQL y MongoDB en un entorno real.\nRecopilar feedback de usuarios internos para identificar mejoras en la experiencia de usuario y la precisión de las respuestas.\nAsegurar la robustez y estabilidad del sistema antes de una posible implementación a mayor escala."
  },
  {
    "objectID": "salsas_castillo_chatbot/quarto/4_despliegue.html#plan-de-implementación",
    "href": "salsas_castillo_chatbot/quarto/4_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de Implementación",
    "text": "2. Plan de Implementación\nLa fase de implementación implica el despliegue del backend del chatbot y su integración con la plataforma de Telegram.\n\n2.1. Arquitectura de Despliegue y Conexión\nEl chatbot de Salsas Castillo se despliega como un servicio de backend basado en FastAPI, que interactúa con la API de Telegram mediante webhooks. La persistencia de datos se maneja con MongoDB y las consultas a datos transaccionales se realizan en PostgreSQL.\n\nBackend del Chatbot (FastAPI): La aplicación principal se despliega en un servidor (o ambiente virtual Linux) utilizando Gunicorn para producción o Uvicorn para desarrollo.\nConexión con Telegram: La comunicación se establece a través de webhooks. Telegram envía las actualizaciones de mensajes al endpoint /webhook de la API de FastAPI. El chatbot, a su vez, utiliza la API de Telegram para enviar respuestas y documentos (PDFs).\nBases de Datos:\n\nPostgreSQL: Se establece una conexión directa desde el backend del chatbot para las consultas SQL.\nMongoDB: Se utiliza para almacenar el historial de sesiones (sessions) y un respaldo completo de mensajes (message_backup).\n\n\n\n\n2.2. Gestión de Persistencia de Datos con MongoDB\nLa gestión del historial de conversaciones es crucial para un chatbot. En Salsas Castillo, se utiliza MongoDB con dos colecciones principales:\n\nsessions: Almacena los últimos mensajes de cada sesión de usuario para mantener el contexto de la conversación. Se configura para mantener un tamaño fijo (ej., los últimos 24 mensajes) para optimizar el rendimiento.\nmessage_backup: Actúa como un histórico completo de todas las interacciones (preguntas del usuario, respuestas del chatbot, metadatos). Es fundamental para el análisis de datos, auditorías y futuras mejoras del modelo.\n\n\n\n2.3. Plan de Monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema:\n\nTiempo de Respuesta: Latencia de las respuestas del chatbot, incluyendo el tiempo de ejecución de las consultas SQL y las llamadas a la API de OpenAI.\nTasa de Éxito/Error: Monitoreo de las peticiones a la API del chatbot y a las bases de datos.\nCalidad de las Respuestas: Evaluación manual de la precisión, coherencia y relevancia de las respuestas, especialmente en escenarios complejos o con datos numéricos.\nUso del Chatbot: Frecuencia de interacciones por usuario, tipos de consultas más comunes.\nErrores en Logs: Revisión de los logs del servidor para identificar excepciones o problemas en el backend.\n\n\n\n2.4. Plan de Mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema:\n\nActualización de Dependencias: Revisión y actualización regular de las librerías de Python (FastAPI, LangChain, PyMongo, etc.).\nRevisión de Logs: Monitoreo activo de los logs del servidor y de las bases de datos para identificar y solucionar problemas.\nAuditoría de Datos y Respuestas: Evaluación periódica de la calidad de los datos en PostgreSQL y MongoDB, y verificación de la precisión de las respuestas del chatbot a lo largo del tiempo.\nOptimización de Consultas: Refinamiento continuo de las consultas SQL generadas por el LLM para mejorar el rendimiento.\nActualización del Vector Store: Si se añaden nuevos documentos internos, se programará la actualización del vector store FAISS.\n\n\n\n2.5. Experiencia de Desarrollo\nEl proyecto ha permitido consolidar la experiencia en el desarrollo de un chatbot completo, desde la integración con plataformas de mensajería (Telegram) hasta la orquestación de LLMs con bases de datos relacionales y vectoriales. Los aprendizajes clave incluyen:\n\nManejo de la interacción entre LLMs y bases de datos SQL para consultas dinámicas.\nImplementación de la persistencia de sesiones y el historial de mensajes en MongoDB.\nDesarrollo de herramientas personalizadas (ej., generación de PDFs) y su integración en el flujo del agente.\nGestión de la transcripción de audio para una experiencia de usuario más inclusiva.\nAdherencia a buenas prácticas de desarrollo modular y escalable.\n\n\n\n2.6. Despliegue del Chatbot en el Sistema de Pruebas\nEl chatbot será desplegado en un entorno de pruebas de Telegram, accesible para un grupo controlado de usuarios internos de Salsas Castillo. Este despliegue permitirá validar el sistema en condiciones casi reales.\nEl proceso de despliegue consistirá en:\n\nMontaje del Entorno de la API: Despliegue de la API de FastAPI en un servidor dedicado, asegurando la conectividad con PostgreSQL y MongoDB.\nConfiguración del Webhook de Telegram: Establecer el webhook para que Telegram envíe las actualizaciones de mensajes al endpoint de la API.\nVerificación Funcional: Realizar pruebas exhaustivas para verificar el flujo de conversación, la precisión de las respuestas, la generación de PDFs y el manejo de la transcripción de audio.\nConsideraciones de Seguridad: Asegurar la autenticación de usuarios (mediante whitelist de Telegram IDs), el manejo seguro de credenciales y la protección de datos.\n\nEste hito marca un avance significativo hacia la validación en entorno real del sistema conversacional, permitiendo recopilar feedback de usuarios internos antes de considerar un despliegue completo en producción."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "Este proyecto se centra en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información de ventas y finanzas, y mejorar la comunicación interna. El chatbot utiliza un sistema de recuperación aumentada con generación (RAG) y herramientas dinámicas para proporcionar respuestas precisas y relevantes, así como la capacidad de generar reportes financieros.\n\n\n\nAutomatizar consultas: Permitir a los usuarios (operativos y no operativos) obtener información sobre ventas, productos y finanzas de manera rápida y eficiente a través de una interfaz conversacional en Telegram.\nMejorar la toma de decisiones: Proporcionar análisis de datos financieros y de ventas en tiempo real, incluyendo la generación de reportes en PDF.\nOptimizar la gestión de información: Centralizar el acceso a datos transaccionales y consolidados, reduciendo la dependencia de consultas manuales.\nAdaptación a nuevas tecnologías: Implementar un sistema basado en LLMs y bases de datos vectoriales para un enfoque moderno y escalable.\n\n\n\n\n\nIncremento en la eficiencia operativa al reducir el tiempo dedicado a la búsqueda manual de información.\nMejora en la precisión de los datos y análisis disponibles para el personal.\nFacilitación de la toma de decisiones estratégicas basadas en información actualizada."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html#introducción-al-proyecto",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html#introducción-al-proyecto",
    "title": "Documentación",
    "section": "",
    "text": "Este proyecto se centra en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información de ventas y finanzas, y mejorar la comunicación interna. El chatbot utiliza un sistema de recuperación aumentada con generación (RAG) y herramientas dinámicas para proporcionar respuestas precisas y relevantes, así como la capacidad de generar reportes financieros.\n\n\n\nAutomatizar consultas: Permitir a los usuarios (operativos y no operativos) obtener información sobre ventas, productos y finanzas de manera rápida y eficiente a través de una interfaz conversacional en Telegram.\nMejorar la toma de decisiones: Proporcionar análisis de datos financieros y de ventas en tiempo real, incluyendo la generación de reportes en PDF.\nOptimizar la gestión de información: Centralizar el acceso a datos transaccionales y consolidados, reduciendo la dependencia de consultas manuales.\nAdaptación a nuevas tecnologías: Implementar un sistema basado en LLMs y bases de datos vectoriales para un enfoque moderno y escalable.\n\n\n\n\n\nIncremento en la eficiencia operativa al reducir el tiempo dedicado a la búsqueda manual de información.\nMejora en la precisión de los datos y análisis disponibles para el personal.\nFacilitación de la toma de decisiones estratégicas basadas en información actualizada."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "2. Manual de instalación y despliegue",
    "text": "2. Manual de instalación y despliegue\nEsta sección detalla los requisitos, dependencias y pasos para la instalación y despliegue del chatbot de Salsas Castillo.\n\n2.1. Configuraciones importantes\n\nEl backend del chatbot está desarrollado con FastAPI y se espera que se ejecute en un entorno con Python 3.12.\nRequiere conectividad a una instancia de MongoDB para la gestión de sesiones e historial, y a una base de datos PostgreSQL para datos financieros y de ventas.\nUtiliza modelos de lenguaje de OpenAI (requiere OPENAI_API_KEY) y potencialmente modelos open-source como gemma3:4b a través de Ollama para la moderación.\nLas credenciales sensibles se gestionan a través de un archivo .env.\nLa integración con Telegram se realiza mediante webhooks y el telegram_token correspondiente.\n\n\n\n2.2. Requisitos del sistema\n\nPython: Versión 3.x (se recomienda la versión utilizada en el desarrollo, ej., 3.12.9).\nPip/UV: Última versión para la gestión de paquetes.\nOllama: Instalado y en ejecución si se utilizan modelos open-source para moderación.\nMongoDB: Acceso remoto configurado para las colecciones de sesiones e historial de mensajes.\nPostgreSQL: Acceso remoto configurado para las bases de datos historial_facturas y financieroii.\nConexión a Internet: Necesaria para interactuar con las APIs de OpenAI y Telegram.\n\n\n\n2.3. Dependencias principales del sistema\n\nfastapi: Framework web para el backend.\nlangchain: Framework principal para la orquestación del LLM y las herramientas.\nopenai: Cliente Python para la API de OpenAI.\npymongo: Driver para la interacción con MongoDB.\npsycopg2-binary: Adaptador PostgreSQL para Python.\nfpdf: Para la generación de PDFs.\nrequests: Para realizar solicitudes HTTP (ej., a Telegram, OpenAI Whisper).\nuvicorn/gunicorn: Servidor WSGI para despliegue.\npydantic: Para la validación de modelos de datos.\nfaiss-cpu: Para la base de datos vectorial (si aplica para documentos internos).\n\n\n\n2.4. Instalación del Backend (API)\n\nClonar el repositorio:\n\ngh repo fork Macrodata-Analitica/castilloChatbot\ncd castilloChatbot\n\nCrear entorno virtual e instalar dependencias:\n\npip install uv # Si no está instalado\nuv venv\nsource .venv/bin/activate # Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\nuv sync # Sincroniza los modulos de src/\n\nConfigurar variables de entorno (.env):\n\nAsegúrate de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos:\nOPENAI_API_KEY=\"\"\n\nVERIFY_TOKEN=\"\"\nTELEGRAM_BOT_TOKEN=\"\"\nPHONE_NUMBER_ID=\"\"\n\nHOST=\"\"\nPORT=\"\"\nUSER=\"\"\nPASS=\"\"\nDBNAME=\"\"\nSCHEMA=\"\"\n\nLevantar el backend:\n\n\nDesarrollo:\n\nuvicorn salsasllm.API.telegram_main:app --reload\n\nProducción:\n\nnohup gunicorn salsasllm.API.telegram_main:app --workers 4 --bind 0.0.0.0:8000 -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile - --error-logfile - &\n\nConfigurar Webhook de Telegram:\n\nAsegúrate de que Telegram envíe los mensajes a tu endpoint /webhook. Esto se hace una vez a través de la API de Telegram:\ncurl -F \"url=https://&lt;TU_DOMINIO&gt;/tgbot/webhook\" https://api.telegram.org/bot&lt;TU_TELEGRAM_TOKEN&gt;/setWebhook\nReemplaza  y ."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html#documentación-técnica-del-código",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "3. Documentación Técnica del Código",
    "text": "3. Documentación Técnica del Código\nEsta sección describe la estructura modular del proyecto y las funciones clave de sus componentes.\n\n3.1. Estructura de Carpetas y Módulos\nEl proyecto sigue una estructura modular para organizar el código:\n\nsalsasllm/API/telegram_main.py: Archivo principal de la aplicación FastAPI. Configura la aplicación, CORS y registra los endpoints para el chat de Telegram y los logs.\nsalsasllm/API/telegram_chat.py: Contiene la lógica para manejar los webhooks de Telegram, transcribir audio (usando Whisper), enviar mensajes y coordinar con el agente principal.\nsalsasllm/langchain/agent_multitool.py: Implementa la lógica principal del agente conversacional (AgentMultiTools). Gestiona la interacción con herramientas externas (SQL, PDF, búsqueda de información), el historial de conversación en MongoDB y la conexión con el LLM.\nsalsasllm/langchain/vectorstore.py: Implementa la lógica para la creación, carga y consulta de la base de datos vectorial FAISS.\nsalsasllm/tools/search_information.py: Define la herramienta search_information para buscar información relevante en documentos internos (a través del vector store).\nsalsasllm/tools/pdf_tool.py: Define la herramienta generate_financial_report_pdf para crear reportes en PDF a partir de datos tabulares y enviarlos por Telegram.\nsalsasllm/langchain/prompts.py: Contiene las definiciones de los prompts del sistema (prompt_v1, prompt_v2) que guían el comportamiento del LLM.\nsalsasllm/settings/clientes.py: Archivo para la configuración de credenciales (API keys, datos de conexión a DBs, tokens).\nsalsasllm/settings/config.py: Archivo para configuraciones generales del proyecto (ej., rutas de índices FAISS, whitelist de Telegram).\n\n\n\n3.2. Modelos LLM Utilizados\nEl sistema de Salsas Castillo utiliza una combinación de modelos de lenguaje para diferentes propósitos:\n\nGeneración de Respuestas y Razonamiento (OpenAI - gpt-5):\n\nFunción: Es el LLM principal utilizado por AgentMultiTools para interpretar las consultas del usuario, decidir qué herramientas invocar (SQL, búsqueda de información, PDF) y generar las respuestas detalladas.\nVentaja: Ofrece alta capacidad de razonamiento y comprensión contextual para manejar consultas complejas sobre datos financieros y de ventas.\n\nTranscripción de Audio (OpenAI Whisper - whisper-1):\n\nFunción: Utilizado en telegram_chat.py para transcribir mensajes de voz de los usuarios de Telegram a texto, permitiendo que el chatbot procese entradas de audio.\nVentaja: Alta precisión en la transcripción de voz a texto.\n\nGeneración de Análisis para PDF (OpenAI - gpt-5): `\n\nFunción: En pdf_tool.py, este modelo se utiliza para generar un párrafo de análisis conciso a partir de los datos tabulares que se incluirán en el reporte PDF.\nVentaja: Proporciona resúmenes inteligentes y profesionales de los datos.\n\n\n\n\n3.3. Puntos de entrada y funciones clave\nEstos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot:\n\ntelegram_main.py::telegram_webhook_handler:\n\n@app.post(\"/webhook\")\nasync def telegram_webhook_handler(request: Request, background_tasks: BackgroundTasks):\n\nPropósito: Es el endpoint de FastAPI que recibe todos los mensajes y actualizaciones de Telegram. Actúa como el punto de entrada principal para las interacciones del usuario.\nComportamiento: Recibe el payload de Telegram, extrae el chat_id y el mensaje (texto o voz), verifica si el usuario está en la whitelist y delega el procesamiento a handle_message en segundo plano.\ntelegram_chat.py::handle_message:\n\ndef answer(self, question: str = None, session_id: str = None, name: str = None):\n\nPropósito: Es la función central que orquesta la respuesta del chatbot. Recibe la pregunta del usuario, gestiona el historial de la sesión, invoca el AgentExecutor de LangChain y maneja la salida.\nComportamiento: 1. Asegura la existencia de la sesión en MongoDB (ensure_session).\n\n2. Recupera y trunca el historial de la sesión (`get_session_history`, `trim_history`).\n\n3. Invoca al AgentExecutor con la pregunta y el historial, permitiendo que el LLM decida qué herramientas usar (SQL, `search_information`, `pdf_report_tool`).\n\n4. Registra la interacción completa en MongoDB (`add_message`, `add_message_backup`).\n\n5. Si la respuesta es un PDF, coordina su envío a Telegram.\n\npdf_tool.py::generate_financial_report_pdf:\n\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n\nPropósito: Genera un reporte en formato PDF a partir de datos tabulares proporcionados y un análisis generado por un LLM, y lo envía al usuario de Telegram.\nComportamiento: Utiliza fpdf para crear el PDF, get_llm_analysis para obtener un resumen del LLM y enviar_pdf_por_telegram para enviar el archivo.\nvectorstore.py::LangchainVectorStore.create_index:\n\ndef create_index(self, docs):\n\nPropósito: Crea un nuevo índice FAISS a partir de una lista de documentos.\nComportamiento: Utiliza FAISS.from_documents para generar el índice y lo guarda localmente."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "4. Guía de Entrenamiento y Mejora",
    "text": "4. Guía de Entrenamiento y Mejora\nEsta sección aborda cómo se mantiene y mejora la base de conocimientos del chatbot, así como recomendaciones para futuras optimizaciones.\n\n4.1. Generación y Actualización de la Base de Datos Vectorial\nLa herramienta search_information se basa en un vector store FAISS. Este vector store almacena representaciones vectoriales de documentos internos (manuales, reglamentos, etc.) para permitir búsquedas semánticas.\nProceso de Creación: Los documentos internos se convierten en objetos langchain.schema.Document, se generan embeddings utilizando OpenAIEmbeddings, y luego se construye un índice FAISS que se guarda localmente.\nActualización: Para mantener la información actualizada, se debe ejecutar periódicamente el script que reconstruye o actualiza este vector store con cualquier nuevo documento o modificación.\n\n\n4.2. Recomendaciones para Futura Mejora\n\nMonitoreo Avanzado: Implementar un monitoreo más detallado de las interacciones del chatbot, incluyendo el rendimiento de las consultas SQL, el tiempo de respuesta de las herramientas y la calidad de las respuestas generadas por el LLM. Esto puede hacerse analizando los datos en la colección message_backup de MongoDB.\nOptimización de Prompts: Continuar iterando y refinando los prompts (prompts.py) para mejorar la precisión y coherencia de las respuestas, especialmente en casos complejos o ambiguos.\nManejo de Errores Robustos: Mejorar el manejo de errores en las llamadas a APIs externas (OpenAI, Telegram) y a las bases de datos (PostgreSQL, MongoDB) para proporcionar mensajes más informativos al usuario y facilitar la depuración.\nExpansión de Herramientas: Considerar la adición de nuevas herramientas para el agente, como la capacidad de crear gráficos a partir de datos financieros, o interactuar con otros sistemas internos de Salsas Castillo.\nEvaluación Cuantitativa: Si es posible, definir métricas cuantitativas para evaluar la calidad de las respuestas del chatbot (ej., ROUGE, BLEU, o métricas basadas en la satisfacción del usuario) para complementar la evaluación cualitativa.\nEmbeddings Locales (Opcional): Investigar el uso de modelos de embeddings open-source (ej., a través de Ollama) para reducir la dependencia de OpenAI y potencialmente los costos, si la precisión es aceptable para los casos de uso de Salsas Castillo."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/5_documentacion.html#arquitectura",
    "href": "2_salsas_chatbot/quarto/5_documentacion.html#arquitectura",
    "title": "Documentación",
    "section": "5. Arquitectura",
    "text": "5. Arquitectura\n\n5.1. Componentes Clave:\n\nUsuario de Telegram: El usuario final que interactúa con el chatbot a través de la aplicación de mensajería.\nBackend del Chatbot (FastAPI): El servicio principal que procesa las consultas de los usuarios.\n\ntelegram_main.py: Punto de entrada de los webhooks de Telegram.\ntelegram_chat.py: Maneja la lógica de Telegram (transcripción de voz, envío de mensajes) y coordina preguntas y respuestas con el agente.\nagent_multitool.py: Contiene el AgentMultiTools que orquesta el LLM y las herramientas.\n\nModelo agéntico: El cerebro principal que utiliza un LLM para generar respuestas, razonar y decidir el uso de herramientas.\nBase de Datos NoSQL (MongoDB): Almacena el historial de sesiones (sessions) y un respaldo completo de mensajes (message_backup) para análisis.\nBase de Datos Relacional (PostgreSQL): Contiene los datos transaccionales de ventas (historial_facturas) y datos financieros consolidados (financieroii).\nBase de Datos Vectorial (FAISS): Almacena los embeddings de documentos internos para búsquedas semánticas (utilizado por search_information).\nHerramientas: Funciones específicas que el LLM puede invocar:\n\nsql_db_query, sql_db_schema, etc. (de SQLDatabaseToolkit): Para interactuar con PostgreSQL.\nsearch_information: Para buscar en el vector store de documentos internos.\npdf_report_tool: Para generar y enviar reportes PDF.\n\n\n\n\n5.2. Flujo de Interacción Principal:\n\nEl Usuario de Telegram envía un mensaje (texto o voz) al chatbot.\nEl mensaje es recibido por la API y enviado al endpoint /webhook del Backend del Chatbot.\ntelegram_chat.py procesa el mensaje. Si es voz, lo envía a OpenAI Whisper API para transcripción.\nEl texto del mensaje se pasa a AgentMultiTools (agent_multitool.py).\nAgentMultiTools gestiona la sesión en MongoDB y consulta el historial.\nEl LLM, guiado por los prompts (prompts.py), analiza la consulta y decide qué Herramientas utilizar:\n\nSi necesita datos de ventas, finanzas, facturas, u otras tablas, invoca herramientas SQL para consultar la base de datos de PostgreSQL.\nSi necesita información de documentos internos, invoca search_information para buscar en la Base de Datos Vectorial (FAISS).\nSi se solicita un reporte, invoca pdf_report_tool, que a su vez puede usar el LLM para análisis y luego envía el PDF a Telegram.\nSi la consulta se trata sobre una visualización de datos, se invoca table_tool, toma los datos devueltos por la consulta SQL, los convierte a tabla, lo guarda temporalmente como imagen y lo envía al usuario.\n\nLa información recuperada por las herramientas se contextualiza y se envía de nuevo al LLM para generar la respuesta final al usuario.\nLa respuesta es enviada de vuelta al Usuario de Telegram a través de la API.\nTodas las interacciones (consultas y respuestas) se registran en las colecciones sessions y message_backup en MongoDB para análisis y auditoría.\n\n\n\n5.3. Diagrama de la Arquitectura\nEl siguiente diagrama ilustra la arquitectura general del sistema del chatbot para Salsas Castillo, mostrando los componentes principales y el flujo de datos.\n\n\n\nArquitectura del sistema"
  },
  {
    "objectID": "2_salsas_chatbot/quarto/3_modelado.html",
    "href": "2_salsas_chatbot/quarto/3_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema del chatbot, integrando modelos de lenguaje, herramientas de acceso a datos y mecanismos de interacción.\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases, facilitando el mantenimiento y la escalabilidad. La librería principal utilizada es LangChain, que permite orquestar la interacción entre el LLM y diversas herramientas.\nLa arquitectura se centra en un modelo agéntico (AgentMultiTools) que actúa como el cerebro del chatbot.\n\n\nEl agente tiene acceso a un conjunto de herramientas dinámicas que le permiten consultar datos actualizados en tiempo real o realizar acciones específicas:\n\nHerramienta RAG: search_info_tool: Realiza búsquedas semánticas en el vector store de documentos internos.\n\n@tool(description=\"Busca información de documentos, manuales, reglamentos, etc. Devuelve un resumen de la información relevante.\")\ndef search_information(query: str) -&gt; str:\n    # ... lógica de búsqueda en vector store ...\n\nHerramientas SQL: Permiten al agente interactuar directamente con la base de datos PostgreSQL. Estas incluyen:\nsql_db_query: Para ejecutar consultas SQL y obtener resultados.\nsql_db_schema: Para obtener el esquema de las tablas y entender su estructura.\nsql_db_list_tables: Para listar las tablas disponibles.\n\n@tool(description=\"Cuando te pidan generar un reporte financiero en PDF a partir de datos generados previamente.\")\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n    # ... lógica de generación de PDF y envío a Telegram ...\nHerramientas de Generación*\n\npdf_report_tool: Genera reportes financieros en PDF a partir de datos tabulares y los envía al usuario.\ntable_tool: Cuando se trate simplemente de una visualización de datos, estos se presentan mediante una tabla convertida en imagen. Facilitando la comprensión y visualización de la información.\n\nEstas herramientas son invocadas automáticamente por el agente cuando el LLM determina que son necesarias para responder a la consulta del usuario.\n\n\n\nEl sistema se alimenta de información de la siguiente manera:\n\nConsultas de Usuario: La pregunta del usuario es el punto de entrada.\nHistorial de Conversación: El historial se gestiona en MongoDB (sessions collection) y se trunca para ajustarse a la ventana de contexto del LLM.\nLLM: Interpreta la consulta y decide qué herramientas usar.\nHerramientas SQL: Si la consulta requiere datos de la base de datos, el LLM genera una consulta SQL que se ejecuta en PostgreSQL. Los resultados se devuelven al LLM.\nHerramienta de Búsqueda de Información: Si la consulta es sobre documentos internos, se busca en el vector store (FAISS) y la información relevante se devuelve al LLM para generar la respuesta.\nHerramienta de PDF: Si se solicita un reporte, se genera el PDF con el análisis del LLM y se envía. Estos análisis pueden incluir gráficas o no.\nGeneración de Respuesta: El LLM sintetiza la información que devuelven las herramientas y genera la respuesta final al usuario.\n\n\n\n\n\nEl LLM opera con los siguientes atributos y contexto:\n\nquestion: La consulta directa del usuario.\nchat_id | session_id: ID de Telegram del usuario, necesario para enviar mensajes y PDFs directamente a Telegram. También es el identificador único de la sesión del usuario, crucial para mantener el historial de conversación.\nname: Nombre del usuario de Telegram, utilizado para personalizar la interacción.\nchat_history: Historial de mensajes previos de la sesión, truncado para optimizar el contexto.\n\n\n\n\nEl sistema de Salsas Castillo utiliza estratégicamente varios modelos de lenguaje:\n\nGPT (OpenAI):\n\nFunción principal: Es el modelo central para el razonamiento del agente, la interpretación de consultas complejas y la generación de respuestas detalladas. Decide cuándo y cómo usar las herramientas SQL, de búsqueda de información y de PDF. También se utiliza para generar el análisis textual en los reportes PDF.\nVentaja: Alta capacidad de comprensión, razonamiento y generación de texto coherente y preciso, fundamental para análisis financiero y de ventas. Actualmente usamos el modelo GPT-5 como modelo central pero también usamos el modelo GPT-4.1 en ciertas herramientas por su gran ventana de contexto que permite manejar mayor cantidad de información.\n\nWhisper-1 (OpenAI):\n\nFunción: Utilizado para la transcripción de mensajes de voz de los usuarios de Telegram a texto.\nVentaja: Excelente precisión en la conversión de audio a texto, lo que permite una interacción más flexible con el chatbot."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/3_modelado.html#modelado",
    "href": "2_salsas_chatbot/quarto/3_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema del chatbot, integrando modelos de lenguaje, herramientas de acceso a datos y mecanismos de interacción.\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases, facilitando el mantenimiento y la escalabilidad. La librería principal utilizada es LangChain, que permite orquestar la interacción entre el LLM y diversas herramientas.\nLa arquitectura se centra en un modelo agéntico (AgentMultiTools) que actúa como el cerebro del chatbot.\n\n\nEl agente tiene acceso a un conjunto de herramientas dinámicas que le permiten consultar datos actualizados en tiempo real o realizar acciones específicas:\n\nHerramienta RAG: search_info_tool: Realiza búsquedas semánticas en el vector store de documentos internos.\n\n@tool(description=\"Busca información de documentos, manuales, reglamentos, etc. Devuelve un resumen de la información relevante.\")\ndef search_information(query: str) -&gt; str:\n    # ... lógica de búsqueda en vector store ...\n\nHerramientas SQL: Permiten al agente interactuar directamente con la base de datos PostgreSQL. Estas incluyen:\nsql_db_query: Para ejecutar consultas SQL y obtener resultados.\nsql_db_schema: Para obtener el esquema de las tablas y entender su estructura.\nsql_db_list_tables: Para listar las tablas disponibles.\n\n@tool(description=\"Cuando te pidan generar un reporte financiero en PDF a partir de datos generados previamente.\")\ndef generate_financial_report_pdf(table_data: str, title: str, chat_id: int) -&gt; dict:\n    # ... lógica de generación de PDF y envío a Telegram ...\nHerramientas de Generación*\n\npdf_report_tool: Genera reportes financieros en PDF a partir de datos tabulares y los envía al usuario.\ntable_tool: Cuando se trate simplemente de una visualización de datos, estos se presentan mediante una tabla convertida en imagen. Facilitando la comprensión y visualización de la información.\n\nEstas herramientas son invocadas automáticamente por el agente cuando el LLM determina que son necesarias para responder a la consulta del usuario.\n\n\n\nEl sistema se alimenta de información de la siguiente manera:\n\nConsultas de Usuario: La pregunta del usuario es el punto de entrada.\nHistorial de Conversación: El historial se gestiona en MongoDB (sessions collection) y se trunca para ajustarse a la ventana de contexto del LLM.\nLLM: Interpreta la consulta y decide qué herramientas usar.\nHerramientas SQL: Si la consulta requiere datos de la base de datos, el LLM genera una consulta SQL que se ejecuta en PostgreSQL. Los resultados se devuelven al LLM.\nHerramienta de Búsqueda de Información: Si la consulta es sobre documentos internos, se busca en el vector store (FAISS) y la información relevante se devuelve al LLM para generar la respuesta.\nHerramienta de PDF: Si se solicita un reporte, se genera el PDF con el análisis del LLM y se envía. Estos análisis pueden incluir gráficas o no.\nGeneración de Respuesta: El LLM sintetiza la información que devuelven las herramientas y genera la respuesta final al usuario.\n\n\n\n\n\nEl LLM opera con los siguientes atributos y contexto:\n\nquestion: La consulta directa del usuario.\nchat_id | session_id: ID de Telegram del usuario, necesario para enviar mensajes y PDFs directamente a Telegram. También es el identificador único de la sesión del usuario, crucial para mantener el historial de conversación.\nname: Nombre del usuario de Telegram, utilizado para personalizar la interacción.\nchat_history: Historial de mensajes previos de la sesión, truncado para optimizar el contexto.\n\n\n\n\nEl sistema de Salsas Castillo utiliza estratégicamente varios modelos de lenguaje:\n\nGPT (OpenAI):\n\nFunción principal: Es el modelo central para el razonamiento del agente, la interpretación de consultas complejas y la generación de respuestas detalladas. Decide cuándo y cómo usar las herramientas SQL, de búsqueda de información y de PDF. También se utiliza para generar el análisis textual en los reportes PDF.\nVentaja: Alta capacidad de comprensión, razonamiento y generación de texto coherente y preciso, fundamental para análisis financiero y de ventas. Actualmente usamos el modelo GPT-5 como modelo central pero también usamos el modelo GPT-4.1 en ciertas herramientas por su gran ventana de contexto que permite manejar mayor cantidad de información.\n\nWhisper-1 (OpenAI):\n\nFunción: Utilizado para la transcripción de mensajes de voz de los usuarios de Telegram a texto.\nVentaja: Excelente precisión en la conversión de audio a texto, lo que permite una interacción más flexible con el chatbot."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/3_modelado.html#evaluación",
    "href": "2_salsas_chatbot/quarto/3_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa evaluación del chatbot de Salsas Castillo se centra en la calidad y precisión de sus respuestas, dada la naturaleza de las consultas financieras y de ventas.\n\n2.1. Criterios de Evaluación\nLa evaluación se realiza mediante un análisis cualitativo, considerando los siguientes criterios:\n\nPrecisión de los Datos: La información proporcionada (cifras de ventas, costos, nombres de productos) debe coincidir exactamente con los datos de las bases de datos SQL.\nCoherencia y Relevancia: Las respuestas deben ser lógicas, directas y pertinentes a la pregunta del usuario, evitando “alucinaciones” o información incorrecta.\nCapacidad de Análisis: Para consultas que requieren análisis (ej., tendencias de ventas, márgenes), la respuesta debe ser comprensible y destacar las conclusiones clave.\nGeneración de Reportes: Los PDFs generados deben ser comprensibles, contener los datos solicitados y el análisis del LLM debe ser claro y profesional.\nManejo de Ambigüedad: La capacidad del chatbot para pedir aclaraciones o proponer interpretaciones cuando la consulta no es clara. También debe ser capaz de generar respuestas generales aún cuando falte especificación de la información.\n\n\n\n2.2. Proceso de Evaluación (Simulación)\nSe simulan una serie de consultas típicas que un usuario de Salsas Castillo podría realizar, cubriendo distintos escenarios:\n\nConsultas de Ventas: “Muéstrame las ventas del producto ‘AMOR 12 / 1000 ML’ en el último mes.”\nConsultas Financieras: “¿Cuál fue el margen de ganancia del segundo trimestre de 2025?”\nGeneración de Reportes: “Genera un reporte de ventas por presentación para el mes de mayo.”\nBúsqueda de Documentos: “Necesito el reglamento de uso de las bodegas.” (Si aplica y se ha cargado un documento de ejemplo)\nConsultas Ambiguas: “Quiero saber sobre las salsas vendidas ayer” (El chatbot debería pedir más detalles o sugerir opciones o debería generar una respuestas general).\n\nLos resultados de estas simulaciones son revisados por expertos con conocimiento del negocio para validar la calidad de las respuestas y el comportamiento general del sistema.\nLos resultados obtenidos hasta ahora sugieren un desempeño prometedor del sistema, con respuestas coherentes y alineadas con las bases de datos. Esto valida la viabilidad de continuar con el despliegue y la iteración en un entorno de pruebas real."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html",
    "href": "2_salsas_chatbot/quarto/1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "Salsas Castillo es una empresa dedicada a la producción y comercialización de salsas picantes, soya, chamoy y otros productos. Su portafolio se distribuye a una amplia red de tiendas, restaurantes y mayoristas en todo México. Con un catálogo en constante crecimiento y una operación comercial dinámica, la empresa enfrenta el reto de gestionar grandes volúmenes de información interna de forma ágil y eficiente.\nActualmente, el acceso a datos específicos —como ventas, finanzas o documentación interna— suele ser un proceso manual y demandante, lo que dificulta responder con rapidez a consultas y limita la generación de análisis oportunos para la toma de decisiones. Esto ha impulsado la necesidad de una solución que centralice y automatice el acceso a la información.\nObjetivo del proyecto:\nDesarrollar un chatbot inteligente que funcione como asistente virtual para el personal de Salsas Castillo, permitiendo realizar consultas rápidas y precisas sobre datos de ventas, finanzas, facturas y documentos internos. El propósito es mejorar la eficiencia operativa, elevar la calidad de la toma de decisiones y fortalecer la comunicación interna.\nImpacto:\nLa implementación de este chatbot permitirá a Salsas Castillo:\n\nAgilizar el acceso a la información: Reducir el tiempo que el personal dedica a buscar datos específicos en bases de datos o documentos.\nMejorar la precisión de los datos: Proporcionar respuestas consistentes y actualizadas directamente de las fuentes de datos en tiempo real.\nFomentar la autonomía: Empoderar a los empleados para obtener la información que necesitan sin depender de intermediarios y al alcance de su mano.\nOptimizar la toma de decisiones: Facilitar análisis rápidos y la generación de reportes para una gestión más proactiva."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html#objetivos-de-la-línea-de-investigación",
    "href": "2_salsas_chatbot/quarto/1_comprension.html#objetivos-de-la-línea-de-investigación",
    "title": "Comprensión del negocio",
    "section": "0.1 Objetivos de la línea de investigación",
    "text": "0.1 Objetivos de la línea de investigación\nEsta iniciativa se enfoca en aprovechar la información existente de Salsas Castillo para crear un sistema de consulta inteligente.\nEl objetivo principal es desarrollar un chatbot en Telegram que funcione como un asistente virtual para la generación de análisis complejos, capaz de:\n\nInterpretar consultas en lenguaje natural: Permitir a los usuarios hacer preguntas complejas sin necesidad de conocimientos técnicos de bases de datos.\nAcceder a datos dinámicos: Conectarse directamente a las bases de datos transaccionales (PostgreSQL) para obtener información actualizada dentro de la base de datos.\nBuscar en documentos internos: Utilizar una base de datos vectorial para recuperar información de documentos como manuales, reglamentos, contratos, etc.\nGenerar reportes: Crear y enviar reportes financieros en formato PDF a través de Telegram.\n\nObjetivos específicos:\n\nIntegrar fuentes de datos: Establecer conexiones robustas con las bases de datos en PostgreSQL.\nDesarrollar herramientas de consulta: Implementar funciones que permitan al LLM interactuar con SQL para extraer y procesar datos.\nConstruir una base de conocimiento vectorial: Procesar documentos internos para crear embeddings y un vector store para búsquedas semánticas.\nImplementar un agente conversacional: Configurar un agente basado en LangChain que orqueste el uso del LLM y las herramientas.\nHabilitar la generación de reportes PDF: Desarrollar la capacidad de crear reportes dinámicos a partir de los datos consultados.\nDesplegar el chatbot en Telegram: Asegurar la funcionalidad completa del chatbot en la plataforma de Telegram, incluyendo la transcripción de audio.\n\nCriterios de éxito:\n\nPrecisión de las respuestas: El chatbot debe proporcionar información precisa, exacta y relevante en al menos el 90% de las consultas cuando se trate de cálculos y uso de formulas.\nFidelidad: Debe ser fiel a la información extraída de las bases de datos sin presentar alucionaciones en al menos el 90% de las consultas.\nCapacidad de análisis: Debe ser capaz de realizar análisis tanto básicos como complejos de datos y generar reportes comprensibles.\nUsabilidad: La interacción a través de Telegram debe ser intuitiva y accesible para usuarios con diferentes niveles de conocimientos técnicos."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html#evaluación-de-la-situación-actual",
    "href": "2_salsas_chatbot/quarto/1_comprension.html#evaluación-de-la-situación-actual",
    "title": "Comprensión del negocio",
    "section": "0.2 Evaluación de la Situación Actual",
    "text": "0.2 Evaluación de la Situación Actual\nLos recursos disponibles para este proyecto incluyen:\n\nDatos: Acceso a bases de datos PostgreSQL con información de ventas transaccionales y datos financieros consolidados.\nHerramientas: Python, FastAPI, LangChain, OpenAI API, PyMongo, Reportlab, y PostgreSQL.\nEquipo humano: El equipo de Salsas Castillo que provee la información y el contexto necesario para alinear el conocimiento del sistma, y el equipo de desarrollo del proyecto.\n\n\n0.2.1 Requisitos, Supuestos y Restricciones\nRequisitos:\n\nAcceso continuo y estable a las bases de datos PostgreSQL y MongoDB.\nServidor donde se desplegará la API del sistema desarrollado.\nCredenciales válidas para las APIs de OpenAI y el token de Telegram.\nComunicación fluida para la retroalimentación y validación de funcionalidades.\n\nSupuestos:\n\nLas bases de datos de la empresa contienen la información necesaria y están estructuradas de manera que permitan las consultas requeridas sin complicaciones.\nLa API de OpenAI y Telegram mantendrán su disponibilidad y rendimiento.\nEl servidor proveído por la empresa estará siempre disponible para el equipo de desarrollo sin restricciones.\n\nRestricciones:\n\nPosibles limitaciones en la tasa de llamadas a las APIs externas.\nLa complejidad de las consultas SQL puede requerir optimización.\nLa seguridad de la información debe ser una prioridad en todo momento."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html#terminología",
    "href": "2_salsas_chatbot/quarto/1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3 Terminología",
    "text": "0.3 Terminología\nAlgunas de las terminologías clave para este proyecto son:\n\nPython: Lenguaje de programación de alto nivel, fundamental para el desarrollo del backend del chatbot.\nFastAPI: Framework de desarrollo web en Python para construir APIs de forma rápida y eficiente.\nLangChain: Herramienta para construir aplicaciones que combinan modelos de lenguaje con fuentes de datos externas y lógica personalizada.\nInteligencia Artificial (IA): Campo de la informática que desarrolla sistemas capaces de realizar tareas que requieren inteligencia humana.\nModelos de Lenguaje Grande (LLM): Modelos de IA entrenados con vastos volúmenes de texto para comprender y generar lenguaje natural. En este proyecto, se utiliza GPT-4.1 de OpenAI.\nSistema de Recuperación Aumentada con Generación (RAG): Técnica que combina LLMs con bases de datos externas para recuperar información relevante y generar respuestas más precisas y contextualizadas.\nRepresentación Vectorial: Proceso de convertir datos textuales en representaciones numéricas (vectores) para facilitar su análisis y búsqueda.\nModelo de Embeddings: Algoritmo que transforma palabras o frases en vectores. Se utilizan OpenAI Embeddings para documentos internos.\nBase de Datos Vectorial (FAISS): Sistema de almacenamiento optimizado para buscar y recuperar información midiendo la similitud entre vectores. Se utiliza para documentos internos.\nBase de Datos SQL (Structured Query Language): Sistema de almacenamiento relacional que organiza los datos en tablas. En este proyecto, PostgreSQL es la fuente de datos transaccionales y financieros.\nAPI (Interfaz de Programación de Aplicaciones): Conjunto de reglas que permite que diferentes sistemas de software se comuniquen entre sí.\nAPI Key: Clave de autenticación utilizada para acceder a servicios protegidos por una API.\nEndpoint (API): Dirección específica dentro de una API donde se accede a una funcionalidad concreta.\nPayload (HTTP): Contenido de los datos enviados en una solicitud HTTP.\nChatbot: Programa que interactúa con los usuarios mediante lenguaje natural.\nTelegram Bot API: La interfaz de programación que permite a los desarrolladores crear bots que interactúan con los usuarios de Telegram.\nWebhook: Un mecanismo que permite a una aplicación recibir información en tiempo real de otra aplicación cuando ocurre un evento específico.\nMongoDB: Base de datos NoSQL utilizada para almacenar el historial de sesiones y los respaldos de mensajes.\nOpenAI Whisper: Modelo de IA para la transcripción de audio a texto.\nReportlab: Librería de Python para la generación de documentos PDF. `"
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html#beneficios",
    "href": "2_salsas_chatbot/quarto/1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4 Beneficios",
    "text": "0.4 Beneficios\n\nAcceso Instantáneo a la Información: El personal puede obtener datos clave de ventas y finanzas en segundos, directamente desde Telegram.\nAnálisis de Datos Simplificado: La capacidad de generar reportes en PDF con un análisis generado por el LLM democratiza el acceso a distintas perspectivas del negocio.\nReducción de Carga de Trabajo: Disminuye la necesidad de consultas manuales a las bases de datos y/o la preparación de reportes rutinarios.\nMejora en la Productividad: Permite a los equipos enfocarse en tareas de mayor valor al tener la información al alcance de la mano.\nInnovación Tecnológica: Salsas Castillo se posiciona a la vanguardia en el uso de IA para la gestión empresarial."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/1_comprension.html#costos",
    "href": "2_salsas_chatbot/quarto/1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5 Costos",
    "text": "0.5 Costos\n\nTiempo: El desarrollo y la implementación del chatbot requieren una inversión de tiempo significativa del equipo de desarrollo.\nFinancieros: Costos asociados a las suscripciones de APIs (OpenAI) y, potencialmente, a la infraestructura de servidores para el despliegue."
  },
  {
    "objectID": "1_ct_chatbot/quarto/preparacion.html",
    "href": "1_ct_chatbot/quarto/preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "1_ct_chatbot/quarto/preparacion.html#preparación-de-los-datos",
    "href": "1_ct_chatbot/quarto/preparacion.html#preparación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "1_ct_chatbot/quarto/preparacion.html#estructura-final-de-los-datos",
    "href": "1_ct_chatbot/quarto/preparacion.html#estructura-final-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Estructura final de los datos",
    "text": "2. Estructura final de los datos\nUna vez completado el proceso ETL, la información de productos y promociones queda estructurada de la siguiente manera, lista para ser consumida por el chatbot y el sistema de recomendación:\nEn el caso de los productos, la información final queda estructurada de la siguiente manera:\n {\n        \"nombre\": \"Nombre del Producto\",\n        \"clave\": \"Clave\",\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave concatenadas.\",\n        \"fichaTecnica\": {\n            \"Caracteristica1\": \"Valor1\",\n            \"Caracteristica2\": \"Valor2\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto.\",\n            \"LongSummary\": \"Resumen largo.\"\n        }\n    },\n    {...}\nReiteramos que el proceso de limpieza y enriquecimiento aplicado a las promociones fue similar al de los productos. A continuación, se presenta el resultado final de la estructura deseada para las promociones:\n{\n        \"nombre\": \"Nombre de la Promoción\",\n        \"producto\": \"Clave\", # Clave del producto en promoción\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave de la promoción.\",\n        \"fichaTecnica\": {\n            \"CaracteristicaA\": \"ValorA\",\n            \"CaracteristicaB\": \"ValorB\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto de la promoción.\",\n            \"LongSummary\": \"Resumen largo de la promoción.\"\n        }\n    },\n    {...}"
  },
  {
    "objectID": "1_ct_chatbot/quarto/home.html",
    "href": "1_ct_chatbot/quarto/home.html",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "1_ct_chatbot/quarto/home.html#introducción",
    "href": "1_ct_chatbot/quarto/home.html#introducción",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "1_ct_chatbot/quarto/despliegue.html",
    "href": "1_ct_chatbot/quarto/despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "1_ct_chatbot/quarto/despliegue.html#revisión-del-proceso",
    "href": "1_ct_chatbot/quarto/despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "1_ct_chatbot/quarto/despliegue.html#plan-de-implementación",
    "href": "1_ct_chatbot/quarto/despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2 Plan de implementación",
    "text": "2 Plan de implementación\nLa fase de implementación implica el despliegue de los componentes desarrollados y su integración en el entorno web de pruebas de CT Online. El objetivo es habilitar el widget de chatbot para un grupo controlado de usuarios.\n\n2.1. Arquitectura de Despliegue y Conexión\nLa fase de implementación en el entorno de pruebas requiere el despliegue de la API del chatbot (desarrollada en Python con FastAPI) y la integración del widget (desarrollado en JavaScript, HTML y CSS) en la página web de pruebas de CT Online.\nUn desafío técnico crucial identificado durante la planificación del despliegue fue la política de seguridad de “contenido mixto” (mixed content) impuesta por los navegadores modernos. Dado que la página de pruebas de la empresa se sirve a través de HTTPS para garantizar una conexión segura, el navegador bloquea las peticiones que el código JavaScript del widget intenta realizar a recursos o servicios que no son seguros, como la API FastAPI que opera con el protocolo HTTP. Realizar llamadas directas desde HTTPS a HTTP resulta en un error de “mixed content”, impidiendo la comunicación.\nTambién se evaluó la posibilidad de servir la API directamente a través de HTTPS. Sin embargo, esta alternativa implicaba mayores retos técnicos y operativos, como la gestión de certificados SSL válidos que autentiquen la comunicación segura entre el servidor y los navegadores. Aunque se intentó utilizar un certificado autofirmado, los navegadores modernos no lo reconocen como confiable, lo que resultaba en el bloqueo automático de las conexiones. Además del reto técnico, esta opción implicaba complicaciones administrativas relacionadas con la obtención, configuración y renovación de certificados válidos, lo que aumentaba la complejidad del despliegue inicial.\nPara superar este obstáculo y permitir la comunicación segura entre el frontend en HTTPS y la API, se ha adoptado la siguiente arquitectura de despliegue utilizando un backend proxy:\n\nLa aplicación principal de la empresa, que opera en un entorno seguro con HTTPS, actuará como intermediaria. Dado que esta aplicación utiliza PHP, el proxy se implementará en este lenguaje.\nEl widget, integrado en la página de pruebas (servida en HTTPS), no realizará llamadas directas a la API FastAPI. En su lugar, el JavaScript del widget será configurado para enviar todas sus peticiones a un nuevo endpoint específico en el backend PHP.\nEl backend PHP recibirá estas peticiones entrantes del frontend (en HTTPS).\nEl código PHP, realizará entonces la solicitud real a la API. La comunicación entre el backend PHP y la API no está sujeta a las restricciones de contenido mixto del navegador, o sea, sin importa si la API se sirve en HTTP o HTTPS, el PHP siempre se puede comunicar con el servicio sin problema.\nEl backend PHP recibirá la respuesta de la API y la reenviará de vuelta al frontend del widget (en HTTPS), en otras palabras, se apunta así mismo.\n\nEste enfoque de proxy en el backend PHP resuelve el problema del contenido mixto al asegurar que la comunicación entre el navegador (frontend) y la infraestructura del backend de la empresa siempre se realice a través de HTTPS. El desafío de integrar una fuente HTTP en un entorno HTTPS queda encapsulado en la comunicación de servidor a servidor.\n\n\n2.2 Gestión de Persistencia de Datos con MongoDB\nUna mejora significativa en la arquitectura del chatbot ha sido la migración de la gestión del historial de conversaciones desde archivos JSON locales a una base de datos NoSQL, específicamente MongoDB. Esta decisión se tomó para optimizar el almacenamiento, mejorar el rendimiento y facilitar el análisis de datos, especialmente considerando el volumen y la frecuencia de las interacciones de los usuarios.\nLa implementación en MongoDB se estructura en dos colecciones principales:\n\nsessions: Esta colección está diseñada para mantener los últimos n mensajes de cada sesión de usuario. Su objetivo es asegurar una recuperación de mensajes mínima y rápida, optimizando la experiencia del usuario final al evitar la carga de historiales extensos en cada interacción. Cada vez que se añade un nuevo mensaje, el más antiguo se desplaza si se supera el límite de mensajes configurado, manteniendo la colección ligera y eficiente para las operaciones del chatbot.\nmessage_backup: A diferencia de sessions, esta colección actúa como un histórico completo de todos los mensajes generados. Su propósito principal es el análisis de datos y la alimentación de sistemas de reportes automatizados. El esquema de esta colección está pensado para simular una tabla SQL, lo que facilita la búsqueda y recuperación de información para análisis posteriores. Cada documento en esta colección incluye tanto la consulta del usuario como la respuesta del chatbot, junto con metadatos relevantes como timestamps y detalles de tokens utilizados.\n\nEsta estrategia de persistencia de datos en MongoDB ha permitido superar las limitaciones de los archivos JSON, que no eran escalables para una gran cantidad de usuarios, ofreciendo un camino más robusto y óptimo para almacenar la información de las conversaciones.\n\n\n\nSchema MongoDB\n\n\n\n\n2.3 Desarrollo del widget frontend\nSe desarrolló un widget de chat personalizado e integrable mediante un simple script (sdk.js). Este widget se encarga de inyectar la interfaz de usuario (html) y cargar la lógica de la aplicación (app.js) y los estilos (styles.css) de forma dinámica en cualquier página web.\n\n\n2.4 Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nFrecuencia de uso del widget: Número de aperturas del chat y cantidad de interacciones por usuario.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.5 Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.6 Experiencia de desarrollo\nEl proyecto ha permitido consolidar la experiencia en el ciclo completo de desarrollo de una aplicación basada en modelos de lenguaje, desde la comprensión y preparación de datos complejos, pasando por el prototipado con herramientas como Langchain, hasta el desarrollo de una API robusta con FastAPI y la implementación de una interfaz de usuario dinámica y reusable (widget frontend desarrollado en JS, HTML y CSS). La resolución de desafíos específicos como el manejo de diferentes estructuras de datos para las vector stores y la integración segura de una API a un entorno web real (HTTPS/contenido mixto, CORS, permisos de red) han sido aprendizajes clave con complicaciones y problemas que se pudieron corregir y solucionar. Se han seguido buenas prácticas de desarrollo, enfocándose en la modularidad para facilitar futuras expansiones (ej: integración de LangGraph) y el mantenimiento del código.\n\n\n2.7 Despliegue del chatbot en el sistema de desarrollo\nEl chatbot fue desplegado exitosamente en el entorno de pruebas de CT Online, habilitado específicamente para fines de desarrollo e integración continua. Este entorno permite validar en condiciones casi reales el comportamiento tanto del frontend (widget) como de la API conversacional.\nEl proceso de despliegue consistió en los siguientes pasos:\n\nMontaje del entorno de la API: La API desarrollada con FastAPI se desplegó en un servidor, o ambiente virtual de linux, utilizando Gunicorn como servidor de aplicaciones y conectándose al backend de la página de CT Online.\nIntegración del widget en la página de pruebas: Se inyectó el script del widget directamente en la página, asegurando que se pudieran cargar dinámicamente los recursos necesarios (JS, HTML y CSS) desde un servidor de archivos estáticos. La integración se validó en distintos navegadores modernos para asegurar la compatibilidad y el correcto funcionamiento.\nGestión de versiones y control de cambios: Se utilizó Git para gestionar versiones del código tanto del sistema del Chatbot, la API y del widget. Esto permitió llevar un registro detallado de los cambios realizados y facilitó el proceso de despliegue incremental, en caso de futuras modificaciones o ajustes.\nVerificación funcional: Tras el despliegue inicial, se realizaron pruebas manuales y automatizadas para verificar el correcto funcionamiento del flujo de conversación, el tiempo de respuesta de la API y el comportamiento del widget en diferentes escenarios (errores, entradas no reconocidas, ausencia de resultados, etc.).\nConsideraciones de seguridad: Aunque se trata de un entorno de pruebas, se aseguraron medidas básicas como la validación de origen en CORS, limitación de rutas expuestas en la API, y uso de HTTPS para todas las comunicaciones entre cliente y servidor.\n\nLa imagen a continuación muestra el chatbot funcionando en su entorno de desarrollo, con el widget incrustado en la página de pruebas de CT Online:\n\n\n\nChatbot desplegado en el ambiente de desarrollo\n\n\nEste hito marca un avance significativo hacia la validación en entorno real del sistema conversacional, permitiendo recopilar feedback de usuarios internos antes de considerar un despliegue completo en producción.\n\n\n2.8 Despliegue del chatbot en el sistema de producción\nA partir del despliegue en el ambiente de pruebas, donde recopilamos retroalimentación e hicimos iteraciones sobre el modelo, finalmente pudimos desplegarlo al ambiente de producción. Este despliegue es escalonado, empezando por la sucursal de Hermosillo y con los vendedores de la empresa, recopilando nuevamente retroalimentación pero más pegada a casos de estudio concretos y reales.\n\n\n\nChatbot desplegado en el ambiente de producción"
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html",
    "href": "1_ct_chatbot/quarto/comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "CT Internacional es una empresa mexicana encargada en distribuir soluciones de Tecnologías de la Información (TI), preferida para hacer negocios de los distribuidores e integradores del mundo de la tecnología. Fundada en 1992 en la ciudad de Hermosillo, esta empresa surgió como una respuesta a la oportunidad de llevar soluciones de TI, principalmente en el noroeste del país (Saúl Rojo). La cual poco a poco se fue expandiendo hasta lograr el alcance que tiene hoy en día, convirtiéndose en una empresa mayorista de alto impacto en el canal de distribución. Actualmente es una de las mejores empresas mexicanas y tiene presencia con 52 sucursales en todos los estados del país. Además cuenta con un canal de distribución integrado por más de 31 mil clientes y aliados de negocio a quiénes proporciona un extenso catálogo de productos y servicios de más de 202 marcas agrupadas en 12 unidades de negocio.\nTeniendo más de 25 años en el mercado, el crecimiento constante del negocio y del mundo de la tecnología, además de la gran cantidad de productos que ofrecen para su distribución; la empresa tiene la oportunidad de aprovechar los avances tecnológicos que se han ido generado hoy en día a su favor. Esto con la intención de ofrecer un mejor servicio a sus clientes y darles un mejor acercamiento de sus productos para que sean capaces de ver un catálogo especial o personalizado en sus necesidades específicas.\nObjetivo del proyecto:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nImpacto:\nLa empresa se estaría adaptando a la nueva era tecnológica, consolidando su liderazgo en el mercado al ofrecer productos que antes podían pasar desapercibidos para el cliente. Con este sistema, los consumidores pueden tomar decisiones ajustadas a sus necesidades, generando valor tanto para ellos como para la empresa."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html#objetivos-de-la-línea-de-investigación",
    "href": "1_ct_chatbot/quarto/comprension.html#objetivos-de-la-línea-de-investigación",
    "title": "Comprensión del negocio",
    "section": "0.1 Objetivos de la línea de investigación",
    "text": "0.1 Objetivos de la línea de investigación\nEsta línea de investigación se enfoca en aprovechar la información disponible sobre los productos de la empresa, incluyendo características técnicas, precios y disponibilidad, con el objetivo de optimizar su accesibilidad y uso en procesos comerciales.\nEl primer paso es extraer, analizar y evaluar la calidad de los datos almacenados en la base de datos de la empresa para determinar si son suficientes para el desarrollo del sistema o si es necesario complementarlos con información adicional.\nUna vez validada la información, se procederá a su transformación mediante modelos de embeddings, convirtiéndola en representaciones vectoriales para su almacenamiento en una base de datos vectorial.\nEl objetivo final es desarrollar un sistema que permita a los usuarios consultar productos con base en especificaciones detalladas mediante un chatbot, el cual recuperará información relevante desde la base de datos vectorial, mejorando la precisión y relevancia de las recomendaciones.\nActualmente:\nLos productos se ofrecen a través de la plataforma, donde los clientes pueden realizar búsquedas según sus necesidades. Sin embargo, el sistema de búsqueda presenta limitaciones: si los clientes no utilizan ciertas palabras clave que están relacionadas al producto, encontrar lo que buscan puede volverse complicado. Además, la empresa cuenta con un sistema de asistencia por llamada o correo, donde un operador ayuda a los clientes a resolver dudas o encontrar productos específicos.\nObjetivo principal:\nDesarrollar un chatbot inteligente para la recomendación de productos, conectado a una base de datos vectorial para ofrecer respuestas precisas y relevantes a los usuarios, y conectado a SQL para consultar información dinámica. Con la intención de optimizar el proceso de búsqueda de productos que la empresa ofrece.\nObjetivos específicos:\n\nAnalizar y organizar la información disponible sobre un conjunto de los productos para evaluar su calidad y definir los atributos clave que se utilizarán en el sistema de recomendación. Además, identificar si es necesario crear nuevas características o ajustar las existentes para mejorar la precisión de las recomendaciones.\nDesarrollar un modelo de representación vectorial que convierta la información de los productos en representaciones vectoriales para su almacenamiento y recuperación eficiente.\nImplementar un sistema RAG que integre un modelo de lenguaje grande (LLM) con la base de datos vectorial para generar recomendaciones de productos precisas y relevantes.\nValidar el desempeño del sistema mediante pruebas de precisión, relevancia de las recomendaciones y eficiencia en la recuperación de información.\n\nCriterios de éxito:\n\nPrecisión en la recomendación de productos:\n\nAl menos el 90% de las 100 recomendaciones evaluadas deben coincidir con las necesidades descritas por el usuario, dando una respuesta directa, precisa y concisa en la información que proporcionada. La calidad de estas recomendaciones será validada por expertos con conocimiento en los productos.\n\nReproducibilidad y consistencia:\n\nConsultas similares deben producir respuestas coherentes en al menos el 90% de los 100 casos, evitando variaciones innecesaria, además de sugerencia de productos no disponibles o fuera de stock, ni productos que no existan. Este criterio también será evaluado por los mismos expertos quiénes determinarán la calidad de la respuesta.\n\nEscalabilidad:\n\nLa estructura del sistema mantiene su rendimiento al aumentar la cantidad de productos de la base de datos.\nLa calidad de las respuestas y el rendimiento del chatbot se mantiene constante al aumentar la cantidad de usuarios."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html#evaluación-de-la-situación-actual",
    "href": "1_ct_chatbot/quarto/comprension.html#evaluación-de-la-situación-actual",
    "title": "Comprensión del negocio",
    "section": "0.2 Evaluación de la situación actual",
    "text": "0.2 Evaluación de la situación actual\nLos recursos que se tienen actualmente para este proyecto son:\n\nDatos: Registro de todos los productos de la empresa disponibles para distribución. La información incluye nombre, marca, tipo, modelo, descripción, palabras clave, stock disponible, ficha técnica con especificaciones, características principales, fecha del registro y precio. Toda esta información se tiene en una base de datos SQL, la cual se actualiza periódicamente.\nHerramientas: Python, servicios de OpenAI y bases de datos SQL.\nEquipo humano: Especialistas en el sistema de distribución de la empresa, expertos en la estructura y manejo de la base de datos para facilitar la extracción de datos, y científicos de datos encargados del desarrollo y evaluación del sistema junto con los especialistas.\n\n\n0.2.1 Requisitos, supuestos y restricciones\nRequisitos:\n\nAcceso a los registros de los productos de la empresa y equipo de compúto con características específicas para el desarrollo del proyecto.\nComunicación constante con los expertos de la empresa para constante evaluación y retroalimentación.\nCredenciales para el uso y acceso de herramientas de la empresa.\n\nSupuestos:\n\nLos permisos y el acceso a la base de datos se matendrá a la disposición del equipo siempre que se necesite sin problemas.\nAPI KEY con créditos suficientes disponibles, para evitar problemas con las llamadas de los modelos de OpenAI.\n\nRestricciones:\n\nLimitaciones en el uso del enlace de la conexión de la base de datos ya que se puede saturar.\nConexiones del sistema con la red de la empresa presentan problemas de sseguridad y permisos que se deben verificar y aprobar por las personas encargadas de infraestructura."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html#terminologia",
    "href": "1_ct_chatbot/quarto/comprension.html#terminologia",
    "title": "Comprensión del negocio",
    "section": "0.3 Terminología",
    "text": "0.3 Terminología\nAlgunas de las terminologías clave para este proyecto son:\n\nPython: Lenguaje de programación de alto nivel, ampliamente utilizado en el desarrollo de software, análisis de datos, inteligencia artificial y automatización.\n\nFastAPI: Framework de desarrollo web en Python que permite construir APIs de forma rápida, sencilla y eficiente.\n\nLangChain: Herramienta para construir aplicaciones que combinan modelos de lenguaje con fuentes de datos externas y lógica personalizada.\n\nInteligencia artificial (IA): Campo de la informática que desarrolla sistemas capaces de realizar tareas que requieren inteligencia humana, como el aprendizaje, la toma de decisiones y el procesamiento del lenguaje natural.\n\nModelos de lenguaje grande (LLM): Modelos de inteligencia artificial entrenados con grandes volúmenes de texto para comprender y generar lenguaje natural.\n\nSistema de recuperación mejorada (RAG): Técnica que combina modelos de lenguaje con bases de datos externas para recuperar información relevante y generar respuestas más precisas.\n\nRepresentación vectorial: Proceso de convertir datos textuales en representaciones numéricas (vectores) para facilitar su análisis y búsqueda.\n\nModelo de embeddings: Algoritmo que transforma palabras o frases en vectores de manera que su similitud semántica pueda medirse matemáticamente.\n\nBase de datos vectorial: Sistema de almacenamiento optimizado para buscar y recuperar información midiendo la similitud entre vectores.\n\nBase de datos SQL (Structured Query Language): Sistema de almacenamiento relacional que organiza los datos en tablas con filas y columnas, usando SQL para consultarlos.\n\nAPI (Interfaz de Programación de Aplicaciones): Conjunto de reglas que permite que diferentes sistemas de software se comuniquen entre sí.\n\nAPI KEY: Clave de autenticación utilizada para acceder a servicios protegidos por una API.\n\nEndpoint (API): Dirección específica dentro de una API donde se accede a una funcionalidad concreta.\n\nPayload (HTTP): Contenido de los datos enviados en una solicitud HTTP, como un formulario o un JSON con información.\n\nChatbot: Programa que interactúa con los usuarios mediante lenguaje natural para responder preguntas o realizar tareas automatizadas.\n\nFrontend: Parte visual o interfaz con la que interactúa el usuario en una aplicación web.\n\nBackend: Parte lógica o del servidor donde se procesa la información y se ejecutan las funciones principales de una aplicación.\n\nHTTP (Hypertext Transfer Protocol): Protocolo para la transferencia de datos en la web.\n\nHTTPS (HTTP Secure): Versión segura del protocolo HTTP que cifra los datos para proteger la comunicación.\n\nContenido mixto (Mixed Content): Problema de seguridad que ocurre cuando una página HTTPS carga recursos desde una fuente HTTP, lo cual puede ser bloqueado por los navegadores.\n\nCertificado SSL/TLS: Archivo digital que autentica la identidad de un sitio web y cifra la información entre el servidor y el navegador.\n\nAutoridad Certificadora (CA): Entidad confiable que emite certificados digitales para garantizar la seguridad en la web.\n\nProxy: Servidor que actúa como intermediario entre un cliente y otro servidor, utilizado para redirigir solicitudes.\n\nPuerto (de red): Punto lógico de conexión en un servidor que permite recibir solicitudes de red.\n\nWidget: Componente visual incrustado en una página web, como un chatbot o formulario."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html#beneficios",
    "href": "1_ct_chatbot/quarto/comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4 Beneficios",
    "text": "0.4 Beneficios\n\nInnovación en la oferta de productos: Introducir un nuevo enfoque en la recomendación de productos, brindando una experiencia más personalizada y efectiva tanto para la empresa como para los clientes.\nOptimización de la estrategia del mercado: La implementación del sistema abre oportunidades para nuevas estrategias de negocio, mejorando la eficiencia y el alcance en la comercialización de productos."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension.html#costos",
    "href": "1_ct_chatbot/quarto/comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5 Costos",
    "text": "0.5 Costos\n\nTiempo: El proyecto tiene un plazo estimado de 3 meses para desarrollar una versión funcional y tangible que sirva como punto de partida para futuras mejoras e implementación en el flujo de trabajo.\nFinancieros: Se consideran costos asociados a suscripciones de herramientas de pago necesarias para el desarrollo e implementación del sistema."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/5_despliegue.html",
    "href": "0_cenace_helpdesk/quarto/5_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/5_despliegue.html#revisión-del-proceso",
    "href": "0_cenace_helpdesk/quarto/5_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/5_despliegue.html#plan-de-implementación",
    "href": "0_cenace_helpdesk/quarto/5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación\nEl plan de despliegue se centra en migrar la arquitectura de desarrollo a un entorno de producción escalable de tipo piloto, manteniendo la modularidad del sistema y optimizando el rendimiento.\n\n2.1. Arquitectura de despliegue y conexión\nLa arquitectura final para el despliegue estará compuesta por los siguientes componentes clave, implementados en un entorno de producción como Databricks, Azure o una plataforma similar:\n\nServidor de la API (backend): Implementación de la API desarrollada en FastAPI (main.py, chat.py) en un servidor escalable. Este servidor manejará las peticiones de los usuarios, coordinará las operaciones del RAG y se comunicará con la base de datos y el LLM.\nModelo de lenguaje (LLM): El modelo de lenguaje gemma3:4b se desplegará en un servidor con aceleración por GPU para asegurar un rendimiento óptimo en la generación de respuestas.\nBase de datos vectorial: La base de datos vectorial de FAISS, que almacena los embeddings de los documentos (vectorstore.py), se mantendrá, pero se integrará con un sistema de almacenamiento persistente y escalable en la nube para garantizar la disponibilidad y el rendimiento.\nBase de datos de historial y tickets (MongoDB): La base de datos de MongoDB, utilizada para almacenar el historial de conversaciones y la gestión de tickets, se migrará a un servicio de bases de datos gestionado en la nube para asegurar la persistencia y la seguridad de los datos.\nInterfaz del usuario (frontend): La presentación del sistema se monta de la mano con FastAPI aprovechando su clase interna Jinja2Templates.\n\n\n\n2.2. Desarrollo del frontend\nLa interfaz se desarrolló con ayuda de 3 tipos de archivos básicos para páginas web. Múltiples archivos de JavaScript, un archivo HTML y estilos modernos que mejoraran la experiencia del usuario gracias a un archivo CSS.\n\n2.2.1. Interfaz del usuario final\nA continuación, se presentan capturas de pantalla de las principales secciones de la interfaz de usuario final desplegada, mostrando las funcionalidades clave del sistema:\n\nInicio de sesión: Sección inicial donde el usuario puede acceder a su sesión. Por delimitaciones del proyecto, una sesión más segura no fue implementada. \nPestaña de Chat: Interfaz principal de conversación donde el usuario interactúa con el LLM para resolver incidentes. Muestra el historial de mensajes y las referencias utilizadas por el modelo. \nPestaña de Documentos: Permite a los usuarios cargar nuevos documentos PDF a la base de conocimientos y visualizar los documentos ya procesados. \nPestaña de Soluciones: Muestra las soluciones que han sido marcadas como “útiles” por los usuarios (a través del botón “like”). Desde aquí se pueden procesar estas soluciones para re-indexarlas en la base vectorial. \nPestaña de Tickets: Presenta la lista de tickets registrados, permitiendo visualizar su detalle (título, descripción y categoría) y llevar un ticket específico a una nueva conversación en el chat, iterando hasta llegar a la solución del ticket. \n\n\n\n\n2.3. Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.4. Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.5. Flujo de despliegue en el ambiente virtual\nEl despliegue del sistema en un ambiente virtual o de nube (como el entorno piloto propuesto) implica una transición desde la configuración de desarrollo local hacia una arquitectura gestionada, escalable y robusta. El flujo general se compone de las siguientes etapas conceptuales:\n\nContenerización de la Aplicación: El primer paso consiste en empaquetar la aplicación backend de FastAPI, junto con todas sus dependencias Python, en una imagen de contenedor (utilizando Docker o Podman). Esto garantiza un entorno de ejecución consistente y aislado, independientemente de la infraestructura subyacente. Si se opta por auto-alojar el LLM con Ollama en producción, este también podría ser contenerizado. Los detalles técnicos para construir estas imágenes se basan en la configuración del entorno descrita en el manual de instalación.\nConfiguración de Servicios Gestionados: A diferencia del entorno local, en un ambiente virtual o de nube se aprovecharán servicios gestionados para componentes clave. Esto incluye la configuración de una instancia de MongoDB Atlas (o similar) para la persistencia del historial y los tickets, y potencialmente un servicio de inferencia de modelos con aceleración GPU para hospedar gemma3:4b, asegurando rendimiento y escalabilidad. La conexión a estos servicios se configurará mediante variables de entorno, similar a lo descrito en el archivo .env del manual. La base vectorial de FAISS requerirá un sistema de almacenamiento persistente asociado al contenedor o servicio que la gestione.\nOrquestación de Contenedores: Para gestionar la aplicación contenerizada (y potencialmente el LLM), se utilizará una herramienta de orquestación (como Podman). La orquestación facilitará el despliegue de múltiples instancias de la API para escalabilidad horizontal, gestionará el balanceo de carga entre ellas y asegurará la alta disponibilidad mediante la recuperación automática en caso de fallos.\nImplementación de Monitoreo y Logging: Se configurarán herramientas de monitoreo específicas del entorno de nube para rastrear el rendimiento de la API, la latencia del LLM, el uso de recursos (CPU, GPU, memoria) y el estado de las bases de datos. Se establecerán sistemas de logging centralizados para capturar los registros de la aplicación (como los generados en nohup.out localmente) y facilitar la depuración y el análisis de errores en tiempo real, alineados con las métricas definidas en el plan de monitoreo.\nIntegración y Pruebas Finales: Una vez desplegada y configurada la API en el nuevo entorno, el paso final es la integración con el sistema de Help Desk del CENACE (si aplica para la fase piloto). Se realizarán pruebas funcionales y de carga en este entorno para validar la correcta operación antes de ponerlo a disposición de los usuarios piloto.\n\nPara obtener las instrucciones técnicas detalladas, comandos específicos y configuraciones de ejemplo para cada una de estas etapas, consulte la sección “Manual de instalación y despliegue” en la Documentación Técnica (6_documentacion.qmd)."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/3_preparacion.html",
    "href": "0_cenace_helpdesk/quarto/3_preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "La fase de preparación de datos es crucial para el funcionamiento del sistema RAG. Su objetivo es convertir la información no estructurada, proveniente de los documentos técnicos del CENACE, en un formato que permita una búsqueda eficiente y una recuperación semántica de alta calidad. Este proceso se divide en tres etapas principales: extracción, transformación y carga de los datos."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/3_preparacion.html#extracción-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/3_preparacion.html#extracción-de-los-datos",
    "title": "Preparación de los datos",
    "section": "1. Extracción de los datos",
    "text": "1. Extracción de los datos\nLa principal fuente de información son los documentos técnicos en formato PDF, así como la base de datos de incidentes resueltos. Para la ingesta de documentos, el sistema utiliza el módulo doccollection.py, específicamente la clase DisjointCollection.\n\nIngesta de archivos: Los documentos PDF se cargan y procesan de forma individual utilizando la librería PyPDF2. Se extrae el texto de cada página, junto con sus metadatos inherentes (título, autor, etc.).\nFragmentación de texto (chunking): El texto extraído se divide en fragmentos lógicos o “chunks” para preservar el contexto de la información. El TextSplitter del módulo doccollection está configurado con los siguientes parámetros para optimizar la cohesión del texto:\n\nTamaño del chunk: 1500 caracteres.\nSolapamiento (overlap): 200 caracteres. Este solapamiento asegura que la información contextual clave no se pierda en los límites de cada fragmento.\n\nAsignación de metadatos: A cada chunk se ke asignan metadatos esenciales, como el nombre del archivo de origen (filename), el número de página (page_number), y un identificador único de referencia (reference). Estos metadatos son vitales para referenciar las fuentes en las respuestas del LLM, evitando alucinaciones y permitiendo al usuario validar la información."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/3_preparacion.html#transformación-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/3_preparacion.html#transformación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Transformación de los datos",
    "text": "2. Transformación de los datos\nUna vez que los documentos se han dividido en chunks, se transforman en una representación numérica que la computadora puede entender y procesar eficientemente.\n\nVectorización: Cada chunk de texto es procesado por el modelo de embeddings bge-m3:latest, implementado en la clase OllamaEmbedder.\nRepresentación vectorial: El modelo convierte el texto en un vector numérico de alta dimensión. Estos vectores capturan el significado semántico del texto; los chunks con un significado similar se agrupan en un espacio vectorial. Este enfoque va más allá de la simple búsqueda por palabres clave, ya que permite al sistema encontrar información relevante incluso si la consulta utiliza un vocabulario o una sintaxis diferente."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/3_preparacion.html#carga-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/3_preparacion.html#carga-de-los-datos",
    "title": "Preparación de los datos",
    "section": "3. Carga de los datos",
    "text": "3. Carga de los datos\nLos vectores generados se almacenan en una base de datos vectorial optimizada para la búsqueda de similitud.\n\nBase de datos vectorial: Se utiliza FAISS (Facebook AI Similarity Search), una librería de código abierto para la búsqueda eficiente en grandes conjuntos de vectores. FAISS indexa los vectores de manera que las consultas de similitud se puedan ejecutar en milisegundos.\nPersistencia: La clase FAISSVectorStore es responsable de la carga y el almacenamiento de los vectores. El índice de FAISS se guarda en un archivo local (index.faiss), mientras que los metadatos de los chunks se almacenan en un diccionario (index.pkl).\nBúsqueda semántica: Una vez cargados los datos, la base de datos vectorial está lista para recibir consultas. Cuando un usuario envía una pregunta, esta se convierte en un vector, que luego se utiliza para encontrar los vectores más cercanos (los chunks más relevantes) en el índice de FAISS."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/3_preparacion.html#flujo-de-preparación",
    "href": "0_cenace_helpdesk/quarto/3_preparacion.html#flujo-de-preparación",
    "title": "Preparación de los datos",
    "section": "4. Flujo de preparación",
    "text": "4. Flujo de preparación\nEl proceso completo es orquestado por la clase RAG y es ejecutado como un flujo de trabajo de indexación:\n\nSolicitud de carga: El usuario sube un documento PDF a través del endpoint /documents.\nManejo de la ingesta: El rag.py recibe el archivo y delega su procesamiento al doccollection.\nGeneración de chunks y metadatos: doccollection lee el PDF, extrae el texto y lo divide en chunks. Asigna metadatos como el ID del documento, el nombre del archivo y el número de página a cada uno de ellos.\nVectorización y almacenamiento: Cada chunk y sus metadatos asociados se envían al vectorstore, que a su vez utiliza el embedder para obtener su representación vectorial. Finalmente, los vectores se agregan al índice de FAISS, asegurando que el conocimiento esté disponible para futuras consultas."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html#propuesta-de-solución",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html#objetivos",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html#terminología",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html#beneficios",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/1_comprension.html#costos",
    "href": "0_cenace_helpdesk/quarto/1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/0_home.html",
    "href": "0_cenace_helpdesk/quarto/0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/0_home.html#introducción",
    "href": "0_cenace_helpdesk/quarto/0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/2_comprension_datos.html",
    "href": "0_cenace_helpdesk/quarto/2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/2_comprension_datos.html#recolección-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/2_comprension_datos.html#descripción-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de los datos extraídos, se obtuvo la siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nLas variables se describen a continuación:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/2_comprension_datos.html#exploración-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nLa exploración comenzó con el análisis del número de categorías únicas en los tickets. Esto es crucial, ya que parte del proyecto es desarrollar un modelo capaz de clasificar o asignar una categoría a nuevos tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nAl observar la distribución, se decidió trabajar únicamente con categorías que tuvieran más de 20 incidentes, considerando la cantidad de datos necesaria para los procesos de entrenamiento, validación y prueba. Esta selección resultó en un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección se analiza la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Esto nos permite entender la cantidad de información disponible que puede ser útil para resolver problemas recurrentes y para el desarrollo del modelo de clasificación.\n\n\n\nDistribución de palabras en los títulos\n\n\nA partir de esta gráfica podemos notar que alrededor de 9 palabras promedio son las que se utilizan en los títulos; lo cual es normal dado que tendemos a englobar las problemáticas en pocas palabras. Sin embargo, vemos que hay problemáticas que pueden pasar el promedio, hasta llegar a las 30 palabras aproximadamente.\n\n\n\nDistribución de palabras en las descripciones\n\n\nPara las descripciones vemos que el promedio es mayor, aproximadamente hasta las 88 palabras, aunque en la mayoría de los casos son menos las que se utilizan. Esto también es normal ya que aquí es donde las personas desarrollan los detalles del incidente el cual están enfrentando. También vemos que hay incidentes que pueden tomar tantas palabras hasta llegar a las 400, 500, y hasta las 1000, aunque este último sea poco común.\n\n\n\nDistribución de palabras en las soluciones\n\n\nEn este caso, nosotros esperábamos que en esta sección hubiéramos encontrado una mayor cantidad de palabras, porque en este caso encontramos que en promedio se utilizan 23 palabras, donde frecuentemente son menos. También encontramos que no todos los incidentes contienen una descripción detallada de la solución o de los pasos que se siguieron para resolver la situación; vimos que solo el 41% de los incidentes contienen una explicación de la solución.\n\n\n0.3.3. Análisis de los bigramas más comunes\nSe realizó un análisis de los pares de palabras más frecuentes en los títulos y descripciones para identificar patrones en la forma en que se plantean las problemáticas.\n\n\n\nBigramas de los títulos\n\n\nEn los títulos lo primero que llama la atención es la codificación de ciertas palabras, ya que por detalles confidenciales, se codificaron nombres propios, sistemas, subestaciones, etc. Luego, podemos ver que se mencionan muchas veces los despliegues de distintos sistemas de software y actualizaciones.\n\n\n\nBigramas de las descripciones\n\n\nEn las descripciones vemos que en la gran mayoría de los casos son saludos hacia la persona que se están dirigiendo. Dejando de lado estos saludos, vemos que se mencionan detalles muy técnicos y específicos que solo expertos en el tema podrán entender."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "0_cenace_helpdesk/quarto/2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nA partir de los resultados previos, se determinó que la columna con menos información es la de soluciones. Solo el 41% de los registros contienen una explicación, con un promedio de 23 palabras, aunque en muchos casos la cantidad es menor.\nAl discutir esta situación con los expertos, se descubrió que el desarrollo y la documentación de soluciones no es una práctica común en el departamento, lo que resulta en una pérdida de conocimiento. Esto motivó la búsqueda de una propuesta para mejorar la persistencia de las soluciones."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/4_modelado.html",
    "href": "0_cenace_helpdesk/quarto/4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/4_modelado.html#modelado",
    "href": "0_cenace_helpdesk/quarto/4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/4_modelado.html#evaluación",
    "href": "0_cenace_helpdesk/quarto/4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación\nLa evaluación de un sistema de este tipo es una tarea no trivial. En lugar de métricas tradicionales, se adoptó un enfoque basado en la calidad de la recuperación y la generación, utilizando el marco propuesto por RAGAS. Este enfoque se centra en tres dimensiones clave:\n\nFidelidad: Las respuestas se basan únicamente en el contexto recuperado (los PDF y base de conocimientos). Que no presente alucinaciones en un 95% de los casos. ¿La respuesta se basa fielmente en el contexto recuperado?\nRelevancia de la respuesta: Las respuestas generadas respondan a la consulta del usuario. ¿La respuesta aborda directamente la consulta del usuario?\nRelevancia del contexto: La información recuperada hayan sido relevantes y útiles. ¿La información recuperada es pertinente para la pregunta?\n\nEstas métricas, alineadas con el juicio humano, permiten un ciclo de evaluación robusto y ágil."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/6_documentacion.html",
    "href": "0_cenace_helpdesk/quarto/6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "0_cenace_helpdesk/quarto/6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngit clone https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/6_documentacion.html#documentación-técnica-del-código",
    "href": "0_cenace_helpdesk/quarto/6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\nLa solución se basa en una arquitectura de Recuperación Aumentada con Generación (RAG). La estructura modular del código, organizada en paquetes de Python, permite una clara separación de responsabilidades.\n\n2.1. Estructura de carpetas y módulos\n\nAPI/\n\nchat.py: Contiene los endpoints de FastAPI para interactuar con el chatbot.\nmain.py: Archivo principal que define la aplicación FastAPI y monta los endpoints.\n\nollama/\n\nassistant.py: Clase que encapsula la lógica para generar embeddings utilizando el modelo bge-m3:latest de Ollama.\n\ndoccollection.py: Módulo que maneja la carga y el procesamiento de documentos (PDF’s) para generar fragmentos de texto.\nrag.py: Clase principal del sistema RAG que integra el assistant, el doccollection y el vectorstore.\nvectorstore.py: Módulo que implementa la base de datos vectorial con FAISS.\nsettings/\n\nclients.py: Archivo de configuración que establece la conexión con la base de datos y el cliente de Ollama.\nconfig.py: Define las rutas de directorios para los vectores y los documentos procesados.\n\ntools/\n\nassistant.py: Clase base abstracta para el asistente LLM.\ndoccollection.py: Clase base abstracta para la colección de documentos.\nembedder.py: Clase base abstracta para el generador de embeddings.\nvectorstore.py: Clase base abstracta para almacén de vectores.\n\ntypes.py: Módulo que define modelos de datos con Pydantic para tipado de datos como Text, TextMetadata, Question, etc.\n\n\n\n2.2. Modelos LLM utilizados\nEl flujo de información en el sistema RAG sigue dos rutas principales:\n\nIndexación de documentos:\n\n\nLos archivos PDF son cargados y procesados por el módulo doccollection.py.\ndoccollection divide cada documento en fragmentos.\nCada fragmento es enviado al embedder.py para generar su representación vectorial.\nLos vectores resultantes se almacenan en la base de datos vectorial de FAISS, implementada en vectorstore.py, junto con sus metadatos.\n\n\nProceso de consulta (QA):\n\n\nUna consulta de usuario llega el endpoint de chat.py.\nLa consulta es vectorizada por el embedder.\nEl vectorstore realiza una búsqueda de similitud semántica para recuperar los fragmentos de documento más relevantes.\nEstos fragmentos se envían al assistant.py, que los utiliza como contexto.\nEl assistant utiliza el LLM (gemma3:4b) para generar una respuesta coherente y contextualizada.\nLa respuesta es devuelta al usuario a través del chat.py y el main.py.\n\n\n\n2.3. Puntos de entrada y funciones clave\n\nGestión de conversaciones: El módulo assistant.py gestiona el historial de conversación en MongoDB, permitiendo que el chatbot mantenga un contexto limitado con el usuario.\nGestión de tickets: Las funciones add_ticket y update_ticket_metadata en rag.py y sus respectivos endpoints en chat.py demuestran la capacidad del sistema para interactuar y actualizar una base de datos de tickets.\nBucle de retroalimentación: La funcionalidad has_liked_solution_in_conversation permite identificar y potencialmente re-indexar soluciones validadas por los usuarios, mejorando continuamente la base de conocimientos."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "0_cenace_helpdesk/quarto/6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\nLa base de conocimientos del chatbot se construye a partir de un proceso que comprende la extracción, segmentación y vectorización del contenido textual proveniente de documentos en formato PDF.\nLos vectores resultantes son posteriormente indexados y almacenados en una base de datos vectorial, la cual constituye el núcleo de la recuperación de información relevante durante las interacciones con el chatbot.\n\n\n3.2. Flujo de la interacción\nEl usuario debe acceder a la pestaña Documentos, donde podrá seleccionar los archivos que desea incorporar a la base de conocimientos del sistema. Una vez elegidos, los documentos se suben a la carpeta correspondiente dentro del entorno donde se encuentra desplegado el sistema (backend). Posteriormente, estos archivos son procesados siguiendo el flujo descrito en el apartado anterior, dando como resultado la creación de la base vectorial o base de conocimientos del sistema.\n\n\n\nGeneración de la base de datos vectorial\n\n\n\n\n3.3. Recomendaciones para futura mejora\n\nLectura de documentos escaneados\n\nActualmente, el sistema no puede extraer información de documentos escaneados. Sería recomendable integrar un módulo de Reconocimiento Óptico de Caracteres (OCR) para ampliar la capacidad de análisis, o de igual manera, Modelo Multimodales que pudieran extraer la información y almacenarla en documentos PDF que sean posteriormente vectorizados.\n\nSistema de seguridad para el inicio de sesión\n\nEl mecanismo de inicio de sesión actual es básico, pues solo requiere ingresar el nombre del usuario.\nAunque esta simplicidad se ajusta al alcance inicial del proyecto, se sugiere incorporar un sistema de autenticación más robusto, que garantice la seguridad de acceso y manejo de información.\n\nSistema de corrección de ortografía\n\nDurante el desarrollo de los modelos de clasificación, se identificó que la falta de ortografía en los tickets afectaba la calidad del análisis.\nSe propuso el desarrollo de un sistema tipo journalist capaz de identificar las 5 W’s y reconstruir el contexto completo del texto, corrigiendo iterativamente los errores ortográficos al momento de cargar los datos."
  },
  {
    "objectID": "0_cenace_helpdesk/quarto/6_documentacion.html#arquitectura-del-sistema",
    "href": "0_cenace_helpdesk/quarto/6_documentacion.html#arquitectura-del-sistema",
    "title": "Documentación",
    "section": "4. Arquitectura del sistema",
    "text": "4. Arquitectura del sistema\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial.\n\n\n\nArquitectura del sistema\n\n\n\n4.1. Componentes clave de conversación y chat\n\nPOST /chat/stream: Endpoint principal para la interacción conversacional. Recibe una consulta y un conversation_id, y devuelve una respuesta generada por el LLM en tiempo real a través de un stream.\nGET /chat/history/{user_id}/{conversation_id}: Recupera el historial de mensajes de una conversación específica.\nPOST /conversations: Crea una nueva conversación, generando un conversation_id único.\nGET /conversations/{user_id}: Lista todas las conversaciones de un usuario, incluyendo sus títulos y la fecha de la última actualización.\nDELETE /conversations: Elimina una conversación específica y su historial de la base de datos.\nPATCH /message-metadata: Permite actualizar los metadatos de un mensaje, utilizado para la funcionalidad de “gustar” una solución.\n\n\n\n4.2. Componentes clave de documentos y soluciones\n\nPOST /documents: Permite cargar nuevos archivos (en formato PDF) a la base de datos vectorial para expandir la base de conocimientos.\nGET /documents: Lista todos los documentos que han sido procesados y están disponibles para la consulta.\nDELETE /documents: Elimina un documento específico de la base de datos vectorial, eliminando también su referencia y los fragmentos asociados.\nPOST /solutions: Procesa y re-indexa soluciones “gustadas” por los usuarios, agregándolas como nuevos documentos a la base de datos vectorial para mejorar la precisión del sistema.\nDELETE /solutions: Elimina una solución específica de la base de datos vectorial.\n\n\n\n4.3. Componentes clave de tickets\n\nGET /tickets: Recupera una lista de todos los tickets almacenados en la base de datos de MongoDB.\nPOST /tickets: Permite añadir un nuevo ticket a la base de datos, con campos como título, descripción y categoría.\nPATCH /tickets/{ticket_reference}: Actualiza los metadatos de un ticket existente, como su estado de solución (is_solved)."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension_datos.html",
    "href": "1_ct_chatbot/quarto/comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n['nombre', 'clave', 'categoria', 'categoria', 'marca', 'tipo',\n       'modelo', 'descripcion', 'descripcion_corta', 'palabrasClave']\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension_datos.html#recolección-de-los-datos",
    "href": "1_ct_chatbot/quarto/comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n['nombre', 'clave', 'categoria', 'categoria', 'marca', 'tipo',\n       'modelo', 'descripcion', 'descripcion_corta', 'palabrasClave']\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension_datos.html#descripción-de-los-datos",
    "href": "1_ct_chatbot/quarto/comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2 Descripción de los datos",
    "text": "0.2 Descripción de los datos\nAl combinar la información extraída de SQL con las fichas técnicas en formato XML, obtenemos las siguientes variables:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\nNombre\nNombre del producto tal como aparece en la página web o catálogo.\nTexto\n\n\nClave\nCódigo único que distingue al producto de otros en el sistema.\nTexto\n\n\nCategoría\nClasificación o tipo de producto al que pertenece.\nTexto\n\n\nMarca\nNombre de la empresa que fabrica o distribuye el producto.\nTexto\n\n\nTipo\nEspecificación del tipo de producto (por ejemplo, cable, bateria, etc.).\nTexto\n\n\nModelo\nIdentificación del modelo específico del producto.\nTexto\n\n\nDetalles\nDescripción completa y detallada del producto.\nTexto\n\n\nFicha técnica\nInformación técnica detallada sobre el producto.\nTexto\n\n\nResumen\nResumen general del producto y sus características principales.\nTexto"
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension_datos.html#exploración-de-los-datos",
    "href": "1_ct_chatbot/quarto/comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de productos\nPara comenzar, examinaremos la distribución de categorías dentro del catálogo, identificando cuáles son las más representativas entre un total de 247 categorías distintas.\nEl siguiente análisis nos permitirá visualizar las 10 categorías más frecuentes, lo que nos dará una mejor comprensión de la composición del inventario.\n\n\n\nTop 10 categorías\n\n\nVemos que los tóners destacan como la categoría predominante, seguidos por las aplicaciones de seguridad, aunque en este último caso, la diferencia con el resto de las categorías es menos marcada en comparación con la primera.\nDel mismo modo, exploraremos la distribución de marcas en los productos y visualizaremos las 10 más comunes dentro de un total de 195 marcas registradas.\n\n\n\nTop 10 marcas\n\n\nEn este análisis, observamos que la marca BROBOTIX sobresale como la más frecuente en el catálogo. Le sigue MANHATTAN, con una diferencia más reducida respecto a las siguientes marcas, en un patrón similar al que se observó en las categorías.\n\n\n0.3.2 Distribución de las palabras asociadas a los productos\nEn esta sección, analizaremos la cantidad de palabras utilizadas en diferentes descripciones de los productos. Esto nos permitirá entender cómo se estructuran los nombres, descripciones y palabras clave dentro del catálogo.\nPara ello, compararemos la distribución de palabras en los siguientes atributos:\n\nNombre del producto\nDescripción completa\nDescripción corta\nPalabras clave asociadas Este análisis nos ayudará a identificar patrones en la longitud de las descripciones y su posible impacto en la categorización y búsqueda de los productos.\n\n\n\n\nComparación de distribuciones\n\n\nAl analizar las distribuciones, observamos que muchas de las instancias de la descripción corta comienzan con 0, pero luego la distribución se aproxima a una distribución quasi-normal, con un promedio de 5 palabras por instancia. En el caso de las descripciones, aunque no todas las instancias comienzan con 0, la distribución de palabras muestra un promedio de 1 palabra. Finalmente, las palabras en los nombres siguen una distribución aparentemente normal, con un promedio de 8 palabras por instancia."
  },
  {
    "objectID": "1_ct_chatbot/quarto/comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "1_ct_chatbot/quarto/comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nObservando el comportamiento de las distribuciones de la gráfica pasada, observamos que debe haber presencia de varios datos nulos, además de una distribución poco común para la variable de descripción ya que el promedio indica que es 1, algo que no se esperaría en una variable de este estilo.\nSi observamos la información de los datos:\nRangeIndex: 6526 entries, 0 to 6525\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   idProductos        6526 non-null   int64 \n 1   nombre             6526 non-null   object\n 2   clave              6526 non-null   object\n 3   categoria          6526 non-null   object\n 4   marca              6526 non-null   object\n 5   tipo               6526 non-null   object\n 6   modelo             6526 non-null   object\n 7   descripcion        6526 non-null   object\n 8   descripcion_corta  763 non-null    object\n 9   palabrasClave      6268 non-null   object\n 10  detalles_precio    6526 non-null   object\ndtypes: int64(1), object(10)\nmemory usage: 561.0+ KB\nAquí vemos que la razón por la cual la distribución de descripción corta empezaba en 0, era porque alrededor del 89% son datos nulos. Sin embargo vemos que la descripción no tiene datos nulos, pero aún así sigue siendo curioso que la cantidad de palabras en promedio sea 1. Para esto analizaremos esta columna, contando sus valores únicos.\ndescripcion\n0                                                                                                                   4983\n                                                                                                                    1460\nTipo: Limpiador& Función: Para computadoras& Características: Limpieza profunda y protección antiestática              1\nColor: Negro& Compatible: L200                                                                                         1\nTipo: Vertical sencillo&#38; Compatible: Para rack de 42U&#38; Ducto: 4x4 pulgadas&#38; Color: Negro texturizado       1\nName: count, dtype: int64\nSi observamos en los datos, la gran mayoría de los datos tienen escrito el valor 0 (en tipo string). Y el segundo valor más frecuente son un espacio en blanco."
  },
  {
    "objectID": "1_ct_chatbot/quarto/documentacion.html",
    "href": "1_ct_chatbot/quarto/documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\nPodman: Gestión y ejecución de los contenedores de la aplicación (API, frontend y otros microservicios) que consumen dichas bases de datos.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nIP=\nPORT=\nUSER=\nPSSWD=\nDB=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nSUCURSALES_URL=\"\"  # Url de la sección de sucursales\nRELOAD_VECTORS_POST= \"https://localhost:8000/internal/reload_vectorstores\"\nURL=''           # Url del servicio de fichas tecnicas\nTOKEN_API=\nTOKEN_CT=\nCONTENT_TYPE=\nCOOKIE=\nDOMINIO=\nBOUNDARY=\n\n# Conexión a MongoDB\nMONGO_URI=\"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB=\"\"\nMONGO_COLLECTION_SESSIONS=\"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP=\"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS=\"tbl_productos\"\nMONGO_COLLECTION_SALES=\"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS=\"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones que están dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nPara cada promoción o promociones nuevas que fueron cargadas después del inicio del mes, este método busca promociones faltantes y los agrega a la base de datos.\npython3 -c \"from ct.ETL.pipeline import update_sales; update_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (recomendación opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo update_products.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/update_products.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/update_products.sh\n\n# Probar manualmente \n~/proyectoCT/update_products.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/update_products.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/update_products.sh\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/update_products.log\nPara actualizar las ofertas, que cada mes se cargan, solo es necesario agregar la siguiente tarea:\nchmod +x ~/proyectoCT/reload_sales.sh\n\n~/proyectoCT/reload_sales.sh\n\ncrontab -e\n\n# Cada 1° y 2° de cada mes se ejecuta la tarea a las 9 en punto (Hermosillo)\n0 9 1,2 * * ~/proyectoCT/reload_sales.sh\n\ncrontab -l\n\ncat ~/proyectoCT/logs/reload_sales.log\nPara agregar ofertas que se vayan agregando a lo largo de la semana:\nchmod +x ~/proyectoCT/update_sales.sh\n\n~/proyectoCT/update_sales.sh\n\n# Agregar la tarea\ncrontab -e\n\n# A partir del 2 de cada mes hasta que acabe el mes, agrega promociones que se hayan subido después de la fecha inicial, de 10 a 7 pm (Hermosillo)\n0 10-19 2-31 * 1-6 ~/proyectoCT/update_sales.sh \n\ncrontab -l\n\ncat ~/proyectoCT/logs/update_sales.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nInsertar/editar tareas\nI\n\n\nGuardar las tareas\n:wq\n\n\nNo hacer ningún cambio\n:q!\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarias para que no hayan problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "1_ct_chatbot/quarto/documentacion.html#manual-de-instalación-y-despliegue.",
    "href": "1_ct_chatbot/quarto/documentacion.html#manual-de-instalación-y-despliegue.",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\nPodman: Gestión y ejecución de los contenedores de la aplicación (API, frontend y otros microservicios) que consumen dichas bases de datos.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nIP=\nPORT=\nUSER=\nPSSWD=\nDB=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nSUCURSALES_URL=\"\"  # Url de la sección de sucursales\nRELOAD_VECTORS_POST= \"https://localhost:8000/internal/reload_vectorstores\"\nURL=''           # Url del servicio de fichas tecnicas\nTOKEN_API=\nTOKEN_CT=\nCONTENT_TYPE=\nCOOKIE=\nDOMINIO=\nBOUNDARY=\n\n# Conexión a MongoDB\nMONGO_URI=\"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB=\"\"\nMONGO_COLLECTION_SESSIONS=\"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP=\"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS=\"tbl_productos\"\nMONGO_COLLECTION_SALES=\"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS=\"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones que están dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nPara cada promoción o promociones nuevas que fueron cargadas después del inicio del mes, este método busca promociones faltantes y los agrega a la base de datos.\npython3 -c \"from ct.ETL.pipeline import update_sales; update_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (recomendación opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo update_products.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/update_products.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/update_products.sh\n\n# Probar manualmente \n~/proyectoCT/update_products.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/update_products.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/update_products.sh\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/update_products.log\nPara actualizar las ofertas, que cada mes se cargan, solo es necesario agregar la siguiente tarea:\nchmod +x ~/proyectoCT/reload_sales.sh\n\n~/proyectoCT/reload_sales.sh\n\ncrontab -e\n\n# Cada 1° y 2° de cada mes se ejecuta la tarea a las 9 en punto (Hermosillo)\n0 9 1,2 * * ~/proyectoCT/reload_sales.sh\n\ncrontab -l\n\ncat ~/proyectoCT/logs/reload_sales.log\nPara agregar ofertas que se vayan agregando a lo largo de la semana:\nchmod +x ~/proyectoCT/update_sales.sh\n\n~/proyectoCT/update_sales.sh\n\n# Agregar la tarea\ncrontab -e\n\n# A partir del 2 de cada mes hasta que acabe el mes, agrega promociones que se hayan subido después de la fecha inicial, de 10 a 7 pm (Hermosillo)\n0 10-19 2-31 * 1-6 ~/proyectoCT/update_sales.sh \n\ncrontab -l\n\ncat ~/proyectoCT/logs/update_sales.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nInsertar/editar tareas\nI\n\n\nGuardar las tareas\n:wq\n\n\nNo hacer ningún cambio\n:q!\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarias para que no hayan problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "1_ct_chatbot/quarto/documentacion.html#documentación-técnica-del-código",
    "href": "1_ct_chatbot/quarto/documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2 Documentación técnica del código",
    "text": "2 Documentación técnica del código\n\n2.1 Estructura de carpetas y módulos\nEl proyecto sigue una estructura modular para facilitar la gestión y el mantenimiento. A continuación, se detalla el propósito de los módulos principales y algunas de sus funciones clave:\nct/langchain/tool_agent.py\nEste archivo contiene la lógica principal del agente conversacional, incluyendo la interacción con herramientas externas, gestión del historial de conversación, uso de MongoDB, y conexión con el modelo que se esté utilizando en el momento a través de LangChain y OpenAI.\n\nClases y funciones clave:\n\nclass ToolAgent:\n\n__init__:\n\nPropósito: Inicializa el agente, configurando el modelo LLM, conectando a MongoDB para la persistencia de sesiones e historial, definiendo el prompt principal del sistema y registrando las herramientas disponibles.\nComportamiento:\n\nEstablece self.model a “gpt-4.1” (O cualquier modelo disponible).\nInicializa las conexiones a las colecciones de MongoDB (sessions, message_backup).\nDefine self.prompt como un ChatPromptTemplate que guía el comportamiento del agente, incluyendo instrucciones de formato de respuesta y el manejo del historial (chat_history).\nDefine self.tools como una lista de objetos Tool y StructuredTool, que el agente puede invocar. Estas herramientas incluyen search_information_tool, inventory_tool y sales_rules_tool, cada una con su descripción y esquema de argumentos (args_schema) cuando aplica.\nself.executor se inicializa a None y se construye bajo demanda.\n\n\nclear_session_history(self, session_id: str) -&gt; bool:\n\nPropósito: Limpia el historial de mensajes (last_messages) para una sesión de usuario específica en la base de datos de MongoDB. Limpia el historial de mensajes para una sesión en particular.\nParámetros:\n\nsession_id (str): El identificador único de la sesión cuyo historial se desea borrar.\n\nRetorna: bool: True si la operación fue exitosa, False en caso de error.\nComportamiento: Actualiza el documento de la sesión en MongoDB, estableciendo last_messages como una lista vacía. Maneja excepciones de PyMongo y otras.\n\nensure_session(self, session: str) -&gt; dict:\n\nPropósito: Garantiza que exista una entrada para la sesion_id en la colección sessions de MongoDB. Si no existe, la crea; si existe, actualiza la marca de tiempo de la última actividad.\nParámetros:\n\nsession_id (str): El identificador de la sesión.\n\nRetorna: dict: El documento de la sesión actualizado o recién creado.\nComportamiento: Utiliza update_one con $setOnInsert y $set para manejar la lógica de upsert y actualización de actividad.\n\nbuild_executor(self):\n\nPropósito: Construye el AgentExecutor de LangChain, que es el componente principal que orquesta la interacción entre el LLM, las herramientas y el prompt.\nParámetros: Ninguno (usa atributos de la clase).\nComportamiento:\n\nCrea un ChatOpenAI LLM con el modelo y la configuración de streaming.\nCrea un agente de funciones de OpenAI (create_openai_functions_agent) vinculando el LLM, las herramientas y el prompt.\nInicializa self.executor como una instancia de AgentExecutor, configurándolo para ser verbose=False y con un max_iterations para controlar la profundidad de la ejecución del agente.\n\n\nrun:\n\nasync def run(\n session_id: str, \n question: str, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta una consulta del usuario a través del agente, gestiona el historial de chat, recopila métricas y transmite la respuesta en tiempo real.\nParámetros:\n\nsession_id (str): ID de la sesión del usuario.\nquestion (str): La pregunta del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario, usado en el prompt del LLM.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta a medida que se generan.\nComportamiento:\n\nRecupera el historial completo de la sesión (get_session_history).\nTrunca el historial (trim_messages) para ajustarse a la ventana de contexto del LLM, priorizando los mensajes más recientes.\nInicializa un TokenCostProcess y CostCalcAsyncHandler para el seguimiento de tokens y costos.\nSi el executor no está construido, llama a build_executor().\nDefine los inputs para el executor, incluyendo la query, chat_history, listaPrecio y session_id.\nUtiliza self.executor.astream() para obtener la respuesta en streaming.\nAcumula los fragmentos de la respuesta completa.\nEn el bloque finally, calcula la duración y los metadatos de la interacción.\nPersiste los mensajes del usuario y del asistente en las colecciones sessions y message_backup de MongoDB.\n\nget_session_history(self, session_id: str) -&gt; list[BaseMessage]:\n\nPropósito: Recupera el historial de mensajes de una sesión específica desde MongoDB y lo convierte a objetos BaseMessage de LangChain.\nParámetros:\n\nsession_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: list[baseMessage]: Una lista de objetos HumanMessage y AIMessage que representan el historial de conversación.\nComportamiento: Consulta la colección sessions en MongoDB para el session_id dado y mapea los mensajes almacenados a los tipos de mensaje de LangChain.\n\nadd_message\n\ndef add_message(\n session_id: str, \n message_type: str, \n content: str, \n metadata: dict = None): \n\nPropósito: Añade un nuevo mensaje (de usuario o asistente) al historial de last_messages de una sesión en MongoDB, manteniendo un tamaño fijo para optimizar el rendimiento.\nParámetros:\n\nsession_id (str): ID de la sesión.\nmessage_type (str): Tipo de mensaje, puede ser “human” o “assistant”.\ncontent (str): Contenido textual del mensaje.\n\nComportamiento:\n\nCrea un diccionario short_msg con el tipo, contenido y timestamp.\nUtiliza $push con $each, $sort y $slice para añadir el nuevo mensaje y truncar la lista last_messages a los últimos 24 mensajes (configurable).\n\nadd_message_backup\n\ndef add_message_backup(\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un respaldo completo de cada interacción (pregunta del usuario y respuesta completa del asistente) junto con métricas detalladas en la colección message_backup de MongoDB para análisis posterior.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta original del usuario.\nfull_answer (str): La respuesta completa generada por el asistente.\nmetadata (dict): Diccionario con metadatos adicionales (tokens, costo, duración, modelo utilizado).\n\nComportamiento: Inserta un nuevo documento en message_backup con toda la información relevante para análisis posterior.\nadd_irrelevant_message\n\ndef add_irrelevant_message(\n self,\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un mensaje etiquetado como “irrelevante” en la colección message_backup. Esto es útil para el monitoreo y posible re-entrenamiento del clasificador.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta del usuario clasificada como irrelevante.\nfull_answer (str): La respuesta generada por el moderador para consultas irrelevantes.\n\nComportamiento: Inserta un nuevo documento en message_backup con el campo label establecido en False.\nmake_metadata\n\ndef make_metadata(\n self,\n token_cost_process: TokenCostProcess,\n duration: float = None) -&gt; dict : \n\nPropósito: Genera un diccionario con metadatos de la interacción, incluyendo información sobre el costo, los tokens utilizados y el tiempo de procesamiento.\nParámetros:\n\ntoken_cost_process (TokenCostProcess): Objeto que contiene información de los tokens.\nduration (float): Duración de la ejecución en segundos.\n\nRetorna: dict: Diccionario con metadatos.\n\n\n\nct/tools/search_information.py\nEste módulo define la herramienta search_information_tool, que permite al chatbot realizar búsquedas semánticas en las bases de datos vectoriales de productos y promociones para encontrar elementos relevantes.\n\nClases y funciones clave:\n\nvectorstore:\n\nPropósito: Una instancia global de LangchainVectorStore que carga el vector store combinado de productos y promociones desde SALES_PRODUCTS_VECTOR_PATH.\n\nretriever_productos:\n\nPropósito: Un retriever configurado para buscar similitudes en el vector store, filtrando específicamente por documentos de la colección “productos”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"productos\"}.\n\nretriever_promociones:\n\nPropósito: Un retriever configurado de manera similar, pero filtrando por documentos de la colección “promociones”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"promociones\"}.\n\nparse_page_content (content):\n\nPropósito: Una función auxiliar interna que parsea el page_content de un documento de LangChain (que es una cadena de texto concatenada) de nuevo a un diccionario de clave-valor.\nParámetros:\n\ncontent (str): La cadena de texto del page_content del documento.\n\nRetorna: dict: Un diccionario con las características del producto/promoción.\nComportamiento: Utiliza expresiones regulares para dividir la cadena por . y luego por : para extraer las claves y valores.\n\nsearch_information_tool(query) -&gt; dict:\n\nPropósito: Busca productos y promociones relevantes en las bases de datos vectoriales utilizando la búsqueda semántica.\nParámetros:\n\nquery (str): La consulta de búsqueda del usuario.\n\nRetorna: dict: Un diccionario que contiene dos listas: \"Promociones\" y \"Productos\", donde cada lista contiene diccionarios de los resultados encontrados.\nComportamiento:\n\nInvoca retriever_promociones.invoke(query) y retriever_productos.invoke(query) para obtener los documentos más relevantes de cada colección.\nUtiliza parse_page_content() para transformar el page_content de cada documento recuperado en un formato de diccionario estructurado.\n\n\n\n\nct/tools/inventory.py\nEste módulo define la herramienta inventory_tool, que permite al chatbot consultar la disponibilidad, precio y moneda de un producto específico en la base de datos MySQL.\n\nClases y funciones clave:\n\nclass InventoryInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave única del producto a consultar.\nlistaPrecio (int): El ID de la lista de precios a considerar para la consulta.\n\n\ninventory_tool(clave: str, listaPrecio: int) -&gt; str:\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\n\nRetorna: str: Una cadena de texto formateada con la información del producto (claave, precio original, moneda, existencias, si está en promoción) o un mensaje de promoción no encontrada.\nComportamiento:\n\nConstruye una consulta SQL que une las tablas productos, existencias, precio y promociones.\nSe conecta a MySQL, ejecuta la consulta con los parámetros proporcionados.\nFormatea el resultado para indicar la moneda (MXN/USD) y el estado de promoción (si el producto en cuestión está o no en promoción).\nIncluye manejo de errores para problemas de conexión a la base de datos o errores inesperados.\n\n\n\n\nct/tools/sales_rules_tool.py\nEste módulo define la herramienta sales_rules_tool, que permite al chatbot aplicar reglas de promoción y calcular el precio final de un producto, considerando la lista de precios y la sucursal del usuario.\n\nClases y funciones clave:\n\nSUCURSALES:\n\nPropósito: Un diccionario global cargado desde un archivo JSON (ID_SUCURSAL) que mapea nemónicos de sucursal a sus IDs.\n\nclass SalesInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta sales_rule_tool utilizando Pydantic.\nAtributos:\n\nclave (str): La clave única del producto en promoción.\nlistaPrecio (int): El ID de la lista de precios a considerar.\nsession_id (str): El ID de la sesión del usuario, utilizado para inferir la sucursal.\n\n\nobtener_id_sucursal(session_id: str) -&gt; str:\n\nPropósito: Extrae el ID de la sucursal a partir del session_id del usuario, utilizando patrones predefinidos (e.g., “XXCTIN” o nemónicos como “HMO”).\nParámetros:\n\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: El ID de la sucursal como una cadena.\nComportamiento: Utiliza expresiones regulares para extraer el nemónico o ID de la sucursal del session_id y lo busca en el diccionario SUCURSALES. Lanza ValueError si no puede extraer o encontrar la sucursal.\n\nquery_sales():\n\nPropósito: Retorna la consulta SQL para obtener los detalles de la promoción más relevante para un producto, lista de precios y sucursal específicos.\nParámetros: Ninguno.\nRetorna: str: La cadena de la consulta SQL.\nComportamiento: La consulta filtra por promociones activas, producto, lista de precios y sucursal, ordenando por fecha de inicio para obtener la promoción más reciente.\n\nsales_rules_tool(clave: str, listaPrecio: int, session_id: str) -&gt; str:\n\nPropósito: Aplica las reglas de promoción para un producto dado, calculando el precio final y generando un mensaje descriptivo para el usuario.\nParámetros:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: Una cadena de texto formateada que describe la promoción aplicada (precio, final, descuento, condiciones, vigencia) o un mensaje si el producto no está en promoción.\nComportamiento:\n\nObtiene el id_sucursal usando obtener_id_sucursal().\nSe conecta a MySQL y ejecuta query_sales() para obtener los detalles de la promoción.\nEvalúa diferentes tipos de promociones (precio de oferta, descuento porcentual, “en compra de X recibe Y”).\nCalcula el precio_final y construye un mensaje descriptivo.\nManeja casos donde la promoción no está vigente o el producto no se encuentra en promoción.\nIncluye manejo de errores para problemas de base de datos o errores inesperados.\n\n\n\n\nct/tools/status.py\nEste módulo define la herramienta status_tool, que permite al chatbot obtener el estado actual de un pedido a través de su número de factura.\n\nClases y funciones clave:\n\nclass StatusInput:\n\nPropósito: Modelo de datos Pydantic para validar y describir el argumento de entrada factura requerido por la herramienta.\nComportamiento: Asegura que el número de factura sea una cadena de texto.\n\nstatus_tool(factura: str) -&gt; str:\n\nPropósito: Consulta una base de datos de MongoDB para encontrar el estado de un pedido específico usando su número de factura.\nParámetros: factura (str): El número de factura del pedido.\nRetorna: str: Una descripción textual del estado del pedido (ej. “Pedido entregado al domicilio”, “Pedido en generación”).\nComportamiento:\n\nConecta a la base de datos de MongoDB. Realiza una consulta para encontrar el documentos del pedido por su folio (factura).\nRealiza una consulta para encontrar el documento del pedido por su folio (factura).\nSi el pedido es encontrado, navega al último estado registrado en el historial de estados (estatus).\nUtiliza una declaración match para mapear los estados de la base de datos (ej. ‘Pendiente’, ‘Enviado’, ‘Entregado’) a descripciones amigables para el usuario.\nManeja casos especiales como el estado de ‘Transito’ para formatear la fecha y hora de manera legible.\nDevuelve un mensaje apropiado si el pedido no es encontrado.\n\n\n\n\nct/moderation/query_moderator.py\nEste módulo se encarga de clasificar las consultas del usuario (relevante, irrelevante, inapropiado) y de gestionar el comportamiento inapropiado, incluyendo la aplicación de sanciones temporales.\n\nClases y funciones claves:\n\nclass QueryModerator:\n\n__init__(model: str = \"gemma3:4b\", assistant : ToolAgent = None):\n\nPropósito: Inicializa el moderador de consultas, configurando el modelo LLM para clasificación y una referencia al ToolAgent para interactuar con la base de datos de sesiones.\nParámetros:\n\nmodel (str): Nombre del modelo de Ollama a utilizar para la clasificación de consultas (por defecto “gemma3:4b”).\nassistant (ToolAgent): Instancia del ToolAgent para acceder a la gestión de sesiones en MongoDB.\n\n\nclassify_query(query: str) -&gt; str\n\nPropósito: Clasifica la consulta del usuario como ‘relevante’, ‘irrelevante’ o ‘inapropiado’ utilizando un modelo de lenguaje.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\n\nRetorna: Un str con una de las clasificaciones relevante, irrelevante, o inapropiado.\nComportamiento:\n\nUtiliza ollama.generate() con un system_prompt predefinido (_classification_prompt) para guiar la clasificación del modelo.\nConfigura opciones del modelo como temperature = 0 para un comportamiento determinista.\n\n\n_classification_prompt(self) -&gt; str:\n\nPropósito: Retorna el system prompt utilizado por el modelo de clasificación para categorizar las consultas del usuario.\nParámetros: Ninguno.\nRetorna: str: La cadena de texto del system prompt.\nComportamiento: Define las reglas y ejemplos para que el LLM clasifique las consultas en las tres categorías.\n\npolite_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida y amigable cuando la consulta del usuario es clasificada como ‘irrelevante’.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con la respuesta cortés.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y confiabilidad en la respuesta.\n\nban_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida que advierte al usuario sobre el uso de lenguaje inapropiado y las posibles consecuencias.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con el mensaje de advertencia.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y control de tono.\n\nevaluate_inappropriate_behavior(self, session: dict, query: str):\n\nPropósito: Evalúa el comportamiento inapropiado del usuario, incrementa el contador de intentos y determina la duración de una posible sanción (baneo progresivo).\nParámetros:\n\nsession (dict): El documento de la sesión del usuario, que contiene el historial de intentos inapropiados y el estado de baneo.\nquery (str): La consulta inapropiada actual del usuario.\nRetorna: tuple[str, int, Optional[datetime]]: Una tupla que contiene:\n\nmsg (str): El mensaje de sanción a mostrar al usuario.\ntries (int): El número actualizado de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la cual el usuario estará baneado (o None si es solo una advertencia).\n\nComportamiento:\n\nImplementa una lógica de escalamiuento progresivo de sanciones (advertencia, 1 min, 3 min, 10 min, 1 hora, 1 día, 7 días) basada en el número de intentos.\nReinicia el contador de intentos si ha pasado suficiente tiempo desde el último incidente.\n\n\n\ncheck_if_banned(self, session: dict) -&gt; Optional[str]:\n\nPropósito: Verifica si el usuario asociado a una sesión está actualmente baneado. Si el baneo ha expirado, limpia el estado de baneo en la base de datos. Verifica si el usuario está actualmente baneado.\nParámetros:\n\nsession (dict): El documento de la sesión del usuario.\n\nRetorna: Optional[str]: Un mensaje de baneo si el usuario está actualmente restringido, o None si no está baneado o si el baneo ha expirado.\nComportamiento: Compara la fecha y hora actual con la fecha banned_until de la sesión. Si el baneo ha expirado, actualiza la sesión en MongoDB para eliminar el campo banned_until.\n\nupdate_inappropriate_session(self, session, tries, banned_until):\n\nPropósito: Actualiza los campos relacionados con el comportamiento inapropiado (inappropiate_tries, last_inappropiate, banned_until) en el documento de la sesión del usuario en MongoDB.\nParámetros:\n\nsession_id (str): ID de la sesión.\ntries (int): El número de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la que el usuario está baneado (o None).\n\nComportamiento: Realiza una operación de update_one en la colección sessions para establecer los campos especificados.\n\n\n\n\nct/langchain/moderated_tool_agent.py\nEste módulo orquesta el flujo completo de una consulta de usuario, incluyendo la moderación de contenido y la delegación de la consulta al agente principal de herramientas (ToolAgent) si es relevante.\n\nClases y funciones claves:\n\nclass ModeratedToolAgent:\n\n__init__(self):\n\nPropósito: Inicializa el ModeratedToolAgent, creando instancias de ToolAgent y QueryModerator y vinculándolos para coordinar el flujo de la conversación.\nComportamiento:\n\nCrea self.tool_agent para manejar la lógica principal del chatbot y la interración con herramientas.\nCrea self.moderator para la clasificación y gestión de comportamiento, pasándole self.tool_agent para que el moderador pueda actualizar el estado de la sesión.\n\n\nrun\n\nasync def run(\n query: str, \n session_id: str = None, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta el flujo completo de una consulta del usuario, desde la verificación de baneo y la clasificación, hasta la generación de la respuesta (relevante, irrelevante, inapropiado) y su streaming.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\nsession_id (str): ID de la sesión del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta final del chatbot.\nComportamiento:\n\nAsegura la existencia de la sesión (tool_agent.ensure_session).\nVerifica si el usuario está baneado (moderator.check_if_banned). Si lo está, cede el mensaje de baneo y termina.\nClasifica la query (moderator.classify_query).\nBasado en la label de clasificación:\n\nSi es relevante, delega la ejecución al tool_agent.run() para obtener una respuesta detallada con herramientas.\nSi es irrelevante, genera una respuesta cortés (moderator.polite_answer()) y la registra.\nSi es inapropiado, evalúa el comportamiento (moderator.evaluate_inappropriate_behavior()), actualiza la sesión (moderator.update_inappropriate_session()) y cede el mensaje de sanción.\nSi la clasificación no es reconocida, cede un mensaje de error genérico.\n\n\n\n\n\nct/chat.py\nEste módulo define los endpoints de la API FastAPI para la interacción del chat y la gestión del historial de sesiones, sirviendo como la interfaz principal entre el frontend y la lógica del chatbot.\n\nFunciones clave:\n\nassistant = ModeratedToolAgent():\n\nPropósito: Inicializa una instancia global de ModeratedToolAgent que será utilizada por todos los endpoints del chat.\nComportamiento: Se crea una única instancia para mantener el estado y las conexiones a la base de datos.\n\nget_chat_history(user_id: str) -&gt; list[dict[str, str]]:\n\nPropósito: Devuelve el historial de chat de un usuario específico en un formato JSON amigable para el frontend. Recupera el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: List[Dict[str, str]]: Una lista de diccionarios, donde cada diccionario representa un mensaje con las claves role (user o bot) y content (el texto del mensaje). Retorna una lista vacía si no hay historial.\nComportamiento: Llama a assistant.tool_agent.get_session_history() para obtener el historial de LangChain y lo transforma al formato JSON deseado.\n\nasync_chat_generator(request: QueryRequest) -&gt; AsyncGenerator[str, None]:\n\nPropósito: Un generador asíncrono que envuelve la función run de la clase ModeratedToolAgent para permitir el streaming de respuestas a los clientes de la API.\nParámetros:\n\nrequest (QueryRequest): Un objeto Pydantic que contiene la consulta del usuario (user_query), el ID del usuario (user_id), y la lista de precios (listaPrecio).\n\nRetorna: AsyncGenerator[str, None]: Cede los fragmentos de respuesta directamente desde el assistant.run().\n\nasync_chat_endpoint(request: QueryRequest) -&gt; StreamingResponse:\n\nPropósito: El endpoint HTTP POST principal para recibir nuevas consultas de chat de los usuarios y devolver respuestas en streaming (Server-Sent Events).\nParámetros\n\nrequest (QueryRequest): Objeto de solicitud Pydantic con la consulta del usuario, ID de usuario y lista de precios.\n\nRetorna: StreamingResponse: Una respuesta HTTP que permite al cliente recibir los fragmentos de la respuesta en tiempo real a medida que se generan.\nComportamiento: Envuelve el async_chat_generator en una StreamingResponse con media_type=\"text/event-stream\".\n\ndelete_chat_history_endpoint(user_id: str) -&gt; str:\n\nPropósito: Endpoint HTTP DELETE para eliminar el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se va a eliminar.\n\nRetorna: str: Una cadena “success” si la eliminación fue exitosa.\nComportamiento: Llama a assistant.tool_agent.clear_session_history() para borrar el historial. Maneja excepciones y levanta una HTTPException en caso de error interno.\n\n\n\nct/main.py\nEs el archivo principal de la aplicación FastAPI. Configura la aplicación, habilita CORS y registra los endpoints definidos en ct.chat.\n\nFunciones clave:\n\napp = FastAPI: Inicializa la aplicación FastAPI.\napp.add_middleware(CORSMiddleware, ...): Configura el middleware de CORS para permitir solicitudes desde cualquier origen, métodos y cabeceras, lo cual es crucial para la integración del widget en diferentes dominios.\n@app.get(\"/history/{user_id}\"): Decorador que mapea la ruta GET /history/{user_id} a la función handle_history.\nhandle_history(user_id: str): Llama a get_chat_history de ct.chat para obtener y retornar el historial.\n@app.post(\"/chat\"): Decorador que mapea la ruta POST /chat a la función handle_chat.\nhandle_chat(request: QueryRequest): Llama a async_chat_endpoint de ct.chat para manejar la solicitud de chat y el streaming de la respuesta.\n@app.delete(\"/history/{user_id}\"): Decorador que mapea la ruta DELETE /history/{user_id} a la función handle_delete_history.\nhandle_delete_history(user_id: str): Llama a delete_chat_history_endpoint de ct.chat para borrar el historial de un usuario.\nif __name__ == \"__main__\":: Bloque de ejecución principal para correr la aplicación con Uvicorn en desarrollo. En producción con Gunicorn.\n\n\nct/ETL/extraction.py:\nMódulo de la capa de Extracción. Responsable de conectarse a la base de datos MySQL para extraer información de productos y promociones, y de interactuar con el servicio externo para obtener fichas técnicas.\n\nClases y funciones clave:\n\nclass Extraction:\n\n__init__(): Inicializa la clase con los parámetros de conexión a MySQL y configura un cloudscraper para la extracción de fichas técnicas.\n\nParámetros: Ninguno explícito, lee de ct.clients.\nComportamiento: Establece scraper con headers personalizados para los tokens de la API y cookies.\n\nids_query() -&gt; str: Retorna la consulta SQL para obtener IDs de productos válidos (con existencias y precios).\nget_valid_ids() -&gt; list: Ejecuta la consulta ids_query y retorna una lista de IDs de productos válidos.\n\nRetorna: list: Lista de IDs de productos.\nComportamiento: Se conecta a MySQL, ejecuta la consulta y maneja errores de conexión.\n\nproduct_query(id) -&gt; str: Retorna la consulta SQL para obtener detalles de un producto específico por ID. Incluye detalles de precios por lista, categoría, marca, etc.\nget_products() -&gt; pd.DataFrame: Extrae la información de todos los productos válidos desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de los productos.\nComportamientos: Itera sobre los IDs válidos, ejecuta product_query para cada uno y consolida los resultados en un DataFrame.\n\ncurrent_sales_query() -&gt; str: Retorna la consulta SQL para obtener las promociones vigentes.\nget_current_sales() -&gt; pd.DataFrame: Extrae las promociones vigentes desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de las promociones.\n\nget_specifications_cloudscraper:\n\nget_specifications_cloudscraper(\n    claves: List[str],\n    max_retries: int = 3,\n    sleep_seconds: float = 0.15\n) -&gt; Dict[str, dict]\nIntenta obtener las fichas técnicas para una lista de claves de productos desde un servicio externo, utilizando cloudscraper para manejar posibles protecciones como Cloudflare.\n\nParámetros:\n\nclaves (List[str]): Lista de claves de productos.\nmax_retries (int): Número máximo de reintentos por cada clave.\nsleep_seconds (float): Tiempo inicial de espera entre reintentos (con backoff exponencial).\n\nRetorna: Dict[str, dict]: Un diccionario donde la clave es la claveProducto y el valor es la ficha técnica en formato JSON.\nComportamiento: Realiza solicitudes POST al url del servicio de fichas técnicas. Implementa lógica de reintentos con backoff controlado y maneja diversos errores HTTP (ej., 403 Forbidden) y errores de JSON/red.\n\n\n\nct/ETL/transform.py\nMódulo de la capa de Transformación. Se encarga de limpiar, unificar y normalizar los datos extraídos, y de persistir las fichas técnicas en MongoDB.\n\nClases y funciones claves:\n\nclass Transform:\n\n__init__(): Inicializa la clase Transform, creando una instancia de Extraction y configurando la conexión a la colección de especificaciones de producto.\n\nParámetros:\n\nspecifications (dict): El diccionario que contiene los datos de la ficha técnica de un producto.\n\nRetorna: Un diccionario estructurado con fichaTecnica (pares nombre-valor de características) y resumen (descripciones cortas y largas).\nComportamiento: Navega a través de la estructura anidada de ProductFeature y SummaryDescription para extraer la información.\n\ntransform_specifications(specs: dict) -&gt; dict: Transforma múltiples especificaciones brutas (obtenidas del servicio externo) en un formato limpio.\n\nParámetros:\n\nspecs (dict): Diccionario de fichas técnicas brutas, donde la clave es la claveProducto.\n\nRetorna: Diccionario con fichas técnicas transformadas y limpias, listas para ser usadas o guardadas.\n\ntransform_products() -&gt; pd.DataFrame: Transforma los datos brutos de productos obtenidos de MySQL en un DataFrame limpio y estandarizado.\n\nRetorna: Un DataFrame con columnas relevantes, detalles concatenados y detalles_precio parseado de JSON.\n\nclean_products() -&gt; dict: Limpia los datos de productos y los enriquece con las fichas técnicas. Primero busca en MongoDB y si no encuentra, las extrae y las guarda.\n\nRetorna: Un diccionario de productos limpios y enriquecidos, listos para la carga final.\nComportamiento: Identifica las claves de productos para las que faltan fichas técnicas en MongoDB, las extrae usando self.data.get_specifications, las transforma y las guarda en la colección specifications. Finalmente, combina las fichas técnicas existentes y nuevas con los datos de productos.\n\ntransform_sales() -&gt; pd.DataFrame: Transforma los datos brutos de promociones (ventas) en un DataFrame limpio.\n\nRetorna: Un DataFrame con información de promociones, fechas formateadas y descuentos con símbolo de porcentaje.\n\nclean_sales() -&gt; dict: Limpia los datos de promociones y los enriquece con fichas técnicas de manera similar a clean_products().\n\nRetorna: Un diccionario de promociones limpias y enriquecidas con fichas técnicas.\n\n\n\n\nct/ETL/load.py\nMódulo de la capa de Carga. Maneja la inserción de los datos transformados en las colecciones de MongoDB y la construcción de la base de datos vectorial.\n\nClases y funciones clave:\n\nclass Load:\n\n__init__(): Inicializa la clase Load, creando una instancia de Transform y configurando las conexiones a las colecciones de MongoDB (products, sales, specifications) y el embedder de OpenAI.\nbuild_content(product: dict, product_features: list) -&gt; str: Contruye el contenido textual de un documento a partir de un diccionario de producto y una lista de características.\n\nParámetros:\n\nproduct (dict): Un diccionario con los datos de un producto.\nproduct_features (list): Lista de claves de características a incluir en el contenido.\n\nRetorna: Una cadena de texto concatenada que resume el producto, adecuada para la vectorización.\n\nmongo_products(): Carga las promociones limpias (procesadas por Transform) en la colección products de MongoDB, realizando upserts.\nmongo_sales(): Carga las promociones limpias (procesadas por Transform) en la colección sales de MongoDB, realizando upserts.\nload_products() -&gt; List[Document]: Carga los productos desde la colección products de MongoDB y los convierte en objetos langchain.schema.Document.\n\nRetorna: Una instancia del índice FAISS.\n\nproducts_vs(): Crea o actualiza el vector store para productos.\n\nComportamiento: Carga los productos desde MongoDB, los vectoriza en lotes (batch_size = 250) para optimizar el uso de memoria, y guarda el índice FAISS localmente en PRODUCTS_VECTOR_PATH.\n\nsales_products_vs(): Crea o actualiza el vector store para ventas/ofertas.\n\nComportamiento: Carga las ofertas desde MongoDB. Luego, carga el vector store de productos existente desde PRODUCTS_VECTOR_PATH y añade las ofertas a este índice (también en lotes), guardando el índice combinado en SALES_PRODUCTS_VECTOR_PATH.\n\n\n\n\nct/ETL/pipeline.py\nEste módulo actúa como el orquestador principal del proceso ETL (Extracción, Transformación, Carga). Centraliza la ejecución de las etapas de obtención, limpieza, enriquecimiento y carga de datos, asegurando que los vector stores de productos y promociones estén siempre actualizados.\n\nClases y funciones clave:\n\nrun_etl_pipeline():\n\nPropósito: Función principal que coordina la ejecución secuencial de todas las fases del pipeline ETL.\nComportamiento:\n\nInicializa una instancia de la clase Load.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.load_sales() para extraer, transformar y preparar los datos de promociones.\nInvoca load.sales_products_vs() para crear o actualizar el vector store de promociones, integrándolos con el vector store de productos existente,\n\nUso: Diseñada para ser el punto de entrada para la actualización programada o manual de la base de conocimientos del chatbot.\n\n\n\n\n\n2.2 Modelos LLM utilizados\nEl sistema utiliza una combinación estratégica de modelos de lenguaje para optimizar la funcionalidad y los costos:\n\nClasificación de consultas y respuestas iniciales (Ollama - gemma3:4b):\n\nFunción: Este modelo open-source, cargado localmente a través de Ollama, es el primer punto de contacto. Su función principal es clasificar las consultas de los usuarios como relevantes (productos que se ofrecen en la empresa), irrelevantes (cualquier producto o tema fuera del ámbito de negocio), o inapropiado (lenguaje ofensivo).\nVentaja: Permite una gestión eficiente de consultas no relacionadas con el negocio sin incurrir en costos de API’s comerciales, y es ideal para respuestas de “baneo” o corteses.\n\nGeneración de respuestas relevantes (OpenAI - gpt-41):\n\nFunción: Este modelo avanzado de OpenAI es el encargado de generar las respuestas detalladas y contextualizadas para las consultas clasificadas como relevantes. Trabaja en conjunto con la cadena RAG para integrar la información recuperada de la base de datos vectorial.\nVentaja: Ofrece alta calidad y precisión en las respuestas, especialmente en el manejo de precios, promociones complejas y detalles de productos, lo que fue validad en pruebas comparativas.\n\n\n\n\n2.3 Puntos de entrada y funciones clave\nEstos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot. Es crucial que aquí se documenten las funciones y clases directamente expuestas o que inician un flujo principal.\n\nModeratedToolAgent.run:\n\nModeratedToolAgent.run(\n  query: str, \n  session_id: str = None, \n  listaPrecio: str = None\n  ) -&gt; AsyncGenerator[str, None]:\n\nEs la función principal que orquesta el glujocompleto de una consulta de usuario.\nParámetros:\n\nquery (str): La pregunta o entrada del usuario.\nsession_id (str): Un identificador único para la sesión del usuario, utilizado para mantener el historial correspondiente.\nlistaPrecio (str): El nivel o clave de precio específico del cliente, que se pasa al LLM para asegurar la precisión de los precios en las consultas dinámicas (ej. precios y promociones).\n\nComportamiento:\n\nVerifica si el usuario está actualmente baneado (usando QueryModerator.check_if_banned). Si es así, retorna un mensaje de baneo.\nClasifica la query (usando QueryModerator.classify_query) en una de las categorías: relevante, irrelevante, o inapropiado.\nBasado en la clasificación:\n\n\nSi es relevante, delega la ejecución al ToolAgent.run() para obtener una respuesta detallada con el uso de herramientas y la base de datos vectorial.\nSi es irrelevante, retorna una respuesta predefinida y cortés (QueryModerator.polite_answer()) y registra la interacción.\nSi es inapropiado, evalúa el comportamiento (QueryModerator.evaluate_inappropriate_behavior()), actualiza el estado de baneo en la sesión del usuario (QueryModerator.update_inappropriate_session()) y retorna el mensaje de sanción apropiado.\n\n\nCede fragmentos de la respuesta en tiempo real (streaming).\nRegistra la interacción completa, incluyendo la pregunta, respuesta y metadatos relevantes para análisis futuro.\n\nToolAgent.run:\n\nToolAgent().run(\n session_id: str, \n question: str, \n listaPrecio: str\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Responde a consultas relevantes de productos, utilizando el agente de LangChain con acceso a herramientas y memoria de conversación. Es la función que el ModeratedToolAgent invoca cuando una consulta es clasificada como relevante.\nParámetros:\n\nsession_id (str): ID de la sesión para recuperar/actualizar el historial.\nquestion (str): La pregunta original del usuario.\nlistaPrecio (str): El nivel de precios del cliente.\n\nComportamiento:\n\nAsegura la existencia de la sesión en MongoDB (ensure_session).\nRecupera el historial de la sesión (get_session_history) y lo trunca para ajustarse a la ventana de contexto del LLM (trim_messages).\nInicializa el AgentExecutor si no ha sido construido.\nEjecuta la query a través del AgentExecutor de LangChain, que orquesta el uso del LLM y las herramientas (search_information_tool, inventory_tool, sales_rules_tool) según la necesidad de la consulta.\nTransmite la respuesta en fragmentos (astream()).\nRegistra la interacción completa (pregunta, respuesta, métricas como tokens y costo) en las colecciones sessions y message_backup de MongoDB.\n\nload.py::products_vs() y load.py::sales_products_vs():\n\nPropósito: Funciones clave para la creación y actualización incremental de las bases de datos vectoriales (FAISS) de productos y ofertas, respectivamente. Orquestan el proceso de carga de documentos desde MongoDB y su vectorización.\nComportamiento:\n\nproducts_vs(): Carga todos los productos de la colección products de MongoDB, los convierte a langchain.schema.Document y los vectoriza en lotes (batch_size=250) utilizando OpenAIEmbeddings. Finalmente, guarda el índice FAISS resultante en PRODUCTS_VECTOR_PATH.\nsales_products_vs(): Carga las ofertas de la colección sales de MongoDB. Luego, carga el índice de productos existente (desde PRODUCTS_VECTOR_PATH) y añade las ofertas a este mismo índice, también en lotes (batch_size=200). Finalmente, guarda el índice combinado (productos + ofertas) en SALES_PRODUCTS_VECTOR_PATH. Esta estrategia asegura que las ofertas se integren sin necesidad de re-vectorizar todo el catálogo de productos, optimizando tiempo y recursos."
  },
  {
    "objectID": "1_ct_chatbot/quarto/documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "1_ct_chatbot/quarto/documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3 Guía de entrenamiento y mejora",
    "text": "3 Guía de entrenamiento y mejora\nFlujo de Datos (ETL)\nLa información que alimenta la base de conocimientos del chatbot sigue un proceso ETL (Extracción, Transformación, Carga) estructurado que garantiza que el modelo tenga acceso a datos actualizados, limpios y ricos en contexto semántico.\nPara una descripción detallada de cada etapa del proceso ETL (Extracción de MySQL y servicio externo de fichas técnicas, Transformación para limpieza, unificación y estructuración, y Carga en MongoDB y la base de datos vectorial FAISS), por favor, consulta el documento “Preparación de los datos”.\n\n3.1 Generación de la base de datos vectorial\nLa base de datos vectorial FAISS es el corazón del sistema RAG, almacenando las representaciones vectoriales de la información de productos y promociones para búsquedas de similitud eficientes. Su generación y actualización son parte integral del proceso ETL, el cual asegura que el modelo tenga acceso a una base de conocimiento robusta y actualizada.\nLos detalles sobre el proceso de creación del índice vectorial, el uso de embeddings, la estrategia de procesamiento en lotes y la inclusión incremental de ofertas se encuentran descritos exhaustivamente en el documento “Preparación de los datos”.\n\n\n3.2 Recomendaciones para futura mejora\n\nEmbeddings locales (Ollama):\n\nPara reducir los costos asociados con los embeddings de OpenAI y disminuir la dependencia de servicios externos, se recomienda realizar pruebas con los modelos de embeddings disponibles a través de Ollama (u otras librerías de embeddings locales).\n\nIndexado incremental:\n\nActualmente, la actualización de la base vectorial puede implicar procesar grandes lotes. Para un mantenimiento más eficiente, especialmente si solo cambian algunos productos o sus precios/promociones, se podría implementar una función de actualización a nivel de producto.\n\nMonitoreo avanzado de rendimiento:\n\nAunque ya se registran métricas de costo y duración, se puede profundizar en el monitoreo.\n\nOptimización del manejo de promociones complejas:\n\nLas promociones tipo “en compra X lleva Y” aún presenta desafíos. Podría ser necesario enriquecer el contexto del embedding para estos productos específicos, reglas más explícitas dentro del LLM o creación de plantillas de prompts más específicos para estas promociones."
  },
  {
    "objectID": "1_ct_chatbot/quarto/documentacion.html#diagrama-de-arquitectura",
    "href": "1_ct_chatbot/quarto/documentacion.html#diagrama-de-arquitectura",
    "title": "Documentación",
    "section": "4 Diagrama de arquitectura",
    "text": "4 Diagrama de arquitectura\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial. Se ha actualizado para reflejar la implementación de MongoDB y los diferentes flujos.\n\n4.1 Componentes clave\n\nInterfaz de usuario (widget del chatbot): El componente frontal incrustado en la página web de CT Internacional, permitiendo la interacción directa del usuario.\nServicio intermediario PHP: Actúa como un puente seguro entre el frontend (HTTPS) y la API del chatbot (HTTPS pero con certificado autofirmado, o sea, no seguro) para resolver problemas de contenido mixto. Si el backend ya está en en HTTPS con un certificado SSL seguro, este componente puede ser obviado.\nAPI del chatbot (FastAPI): El servicio backend principal que procesa las consultas de los usuarios, orquesta la recuperación de información y se comunica con los modelos de lenguaje.\nBase de datos de productos y promociones (MySQL): Almacena la información transaccional y de precios de los productos y promociones de CT Internacional. Es la fuente original de los datos.\nServicio de fichas técnicas: Fuente de datos para la información detallada y semi-estructurada (XML) de los productos.\nMódulo ETL: Proceso automatizado que:\n\nExtrae datos de MySQL y el servicio de fichas técnicas.\nLimpia, unifica, y transforma los datos, persistiendo las fichas técnicas en MongoDB.\nCarga los datos limpios en colecciones de MongoDB (products, sales, specifications).\n\nBase de datos NoSQL (MongoDB): Almacena las fichas técnicas (specifications), así como el historial de las interacciones de los usuarios (sessions, message_backup).\n\nsessions: Mantiene los últimos n mensajes de cada usuario para recuperación rápida y una experiencia de usuario fluida.\nmessage_backup: Almacena un historial completo de todas las consultas y respuestas con métricas detalladas para fines de análisis y reportes.\n\nModelo de embeddings: Componente encargado de transformar tanto las consultas de los usuarios como la información de los productos en representaciones vectoriales numéricas (usando OpenAI Embeddings).\nBase de datos vectorial (FAISS): Almacena las representaciones vectoriales de la información de productos y ofertas, permitiendo búsquedas de similitud eficientes. Se actualiza con datos del ETL.\nClasificador de consultas (Ollama - gemma3:4b): Componente central que, para consultas relevantes, coordina la búsqueda de información en la base de datos vectorial y contextualiza esta información con la consulta del usuario.\nLLM: El motor principal del chatbot para consultas relevantes, responsable de generar respuestas coherentes y detalladas en base a la consulta contextualizada por el RAG.\nSistema de reportes automatizados: Procesa los datos del message_backup en MongoDB para generar insights sobre el uso del chatbot, intereses de los clientes, costos y rendimiento.\n\n\n\n4.2 Flujo de interacción principal\n\nEl usuario interactúa con el widget del chatbot en la página web.\nLa consulta se envía a la API del chatbot (potencialmente vía el servicio intermediario PHP)\nLa API crea o continúa una sesión de conversación, y la consulta es pasada al asistente conversacional.\nSi la consulta es relevante:\n\n\nsearch_information_tool: busca productos relevantes.\ninventory_tool: obtiene precios, existencias y moneda por clave de producto.\nsales_rules_tool: calcula el precio final considerando promociones o reglas de negocio.\nLa información recuperada se contextualiza y se envía al LLM para generar la respuesta al usuario.\n\n\nSi la consulta es irrelevante o inapropiada, el clasificador genera una respuesta adecuada (cortés o de advertencia) directamente al usuario.\nTodas las interacciones (consultas y respuestas) se registran en las colecciones sessions y message_backup en MongoDB.\nEl sistema de reportes automatizados accede a message_backup para generar análisis de datos.\n\n\n\n4.3 Flujo de datos\n\nEl módulo ETL extrae datos de MySQL y el servicio de fichas técnicas.\nLos datos se transforman y las fichas técnicas se cargan en MongoDB (colección specifications).\nLos datos transformados de products y sales (incluyendo las fichas técnicas obtenidas de MongoDB) son utilizados por el módulo ETL para construir y actualizar la base de datos vectorial.\n\n\n\n\nArquitectura del sistema"
  },
  {
    "objectID": "1_ct_chatbot/quarto/modelado.html",
    "href": "1_ct_chatbot/quarto/modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "1_ct_chatbot/quarto/modelado.html#modelado",
    "href": "1_ct_chatbot/quarto/modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "1_ct_chatbot/quarto/modelado.html#evaluación",
    "href": "1_ct_chatbot/quarto/modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2 Evaluación",
    "text": "2 Evaluación\nCon base en las respuestas generadas durante la etapa de modelado, se llevó a cabo una evaluación cualitativa para analizar la coherencia, relevancia y precisión de las recomendaciones de cada modelo. Este análisis nos permitió identificar oportunidades de mejora en el sistema, así como validar si el comportamiento del modelo es adecuado para continuar con su implementación o si requiere ajustes adicionales.\nA continuación, se presentan las respuestas generadas por el sistema para una serie de consultas simuladas por un usuario. Estas imágenes muestran el resultado del mejor modelo seleccionado (GPT 4o) ante cada solicitud:\n\nConsulta: “¡Hola! Me interesan computadoras de oficina”\n\nConsulta: “También me gustaría ver monitores de 27 pulgadas arriba de 75Hz”\n\nConsulta: “Y un no break gamer”\n\nConsulta: “Y una extensión doméstica”\n\n\nLos resultados obtenidos reflejan un desempeño sólido por parte del sistema. En todos los casos evaluados, las respuestas del chatbot fueron coherentes, alineadas con la base de datos y cumplieron con los criterios definidos:\n\nLas ofertas y promociones fueron correctamente identificadas y presentadas.\nLos precios y descripciones de los productos coincidieron con los datos reales.\nNo se observaron errores de alucinación ni pérdidas de coherencia en la conversación.\n\nEsto sugiere que el modelo es capaz de generar respuestas confiables y útiles para los usuarios, por lo que se considera viable continuar con las siguientes etapas del proyecto o bien escalar el sistema hacia una versión de prueba."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/0_home.html",
    "href": "2_salsas_chatbot/quarto/0_home.html",
    "title": "GlorIA: Análisis empresarial desde un enfoque con IA",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información crítica de ventas, finanzas, facturas y más. A través de este asistente conversacional, se busca empoderar al personal de toma de decisiones, proporcionando respuestas rápidas y precisas a sus consultas, y facilitando la toma de decisiones basadas en datos actualizados que cambian en tiempo real.\nEl chatbot se basa en un modelo agéntico MCP (Protocolo de Contexto del Modelo), que combina modelos de lenguaje avanzados con acceso directo a bases de datos relacionales (PostgreSQL) y vectoriales (FAISS). Además de la capacidad de generar reportes en PDF y generación de tablas para una visualización más amigable. La interacción principal se realiza a través de la plataforma de mensajería Telegram.\nEl proyecto se divide en las siguientes fases clave:\n\nComprensión del Negocio: Definición de los objetivos estratégicos y el contexto operativo de Salsas Castillo, enfocados en mejorar la eficiencia en el acceso a la información, el correcto manejo y manipulación de los datos para una generación eficiente de respuestas, y la comunicación interna.\nPreparación: Diseño e implementación de la arquitectura del chatbot, incluyendo la integración de bases de datos relacionales y vectoriales, el desarrollo de herramientas personalizadas, y su flujo de trabajo.\nModelado y Evaluación: La configuración de modelos de lenguaje y la validación de su desempeño a través de consultas controladas.\nDespliegue: Integración del chatbot en el entorno de producción de Telegram y la infraestructura de Salsas Castillo, asegurando su operatividad y accesibilidad.\n\nA través de estas fases, buscamos proporcionar una solución innovadora que transforme la manera en que Salsas Castillo accede y utiliza su información de negocio, impulsando la eficiencia y la agilidad en sus operaciones diarias."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/0_home.html#introducción",
    "href": "2_salsas_chatbot/quarto/0_home.html#introducción",
    "title": "GlorIA: Análisis empresarial desde un enfoque con IA",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot inteligente para Salsas Castillo, diseñado para optimizar el acceso a información crítica de ventas, finanzas, facturas y más. A través de este asistente conversacional, se busca empoderar al personal de toma de decisiones, proporcionando respuestas rápidas y precisas a sus consultas, y facilitando la toma de decisiones basadas en datos actualizados que cambian en tiempo real.\nEl chatbot se basa en un modelo agéntico MCP (Protocolo de Contexto del Modelo), que combina modelos de lenguaje avanzados con acceso directo a bases de datos relacionales (PostgreSQL) y vectoriales (FAISS). Además de la capacidad de generar reportes en PDF y generación de tablas para una visualización más amigable. La interacción principal se realiza a través de la plataforma de mensajería Telegram.\nEl proyecto se divide en las siguientes fases clave:\n\nComprensión del Negocio: Definición de los objetivos estratégicos y el contexto operativo de Salsas Castillo, enfocados en mejorar la eficiencia en el acceso a la información, el correcto manejo y manipulación de los datos para una generación eficiente de respuestas, y la comunicación interna.\nPreparación: Diseño e implementación de la arquitectura del chatbot, incluyendo la integración de bases de datos relacionales y vectoriales, el desarrollo de herramientas personalizadas, y su flujo de trabajo.\nModelado y Evaluación: La configuración de modelos de lenguaje y la validación de su desempeño a través de consultas controladas.\nDespliegue: Integración del chatbot en el entorno de producción de Telegram y la infraestructura de Salsas Castillo, asegurando su operatividad y accesibilidad.\n\nA través de estas fases, buscamos proporcionar una solución innovadora que transforme la manera en que Salsas Castillo accede y utiliza su información de negocio, impulsando la eficiencia y la agilidad en sus operaciones diarias."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/2_preparacion.html",
    "href": "2_salsas_chatbot/quarto/2_preparacion.html",
    "title": "Preparación, modelado y evaluación de los datos",
    "section": "",
    "text": "A diferencia de un proceso ETL tradicional con extracción y transformación masiva de datos a un destino intermedio, el proyecto con Salsas Castillo se enfoca en la conexión de datos en tiempo real a través de herramientas SQL, y la preparación de un vector store para documentos internos.\n\n\nLas fuentes de información principales para el chatbot son:\n\nBase de Datos PostgreSQL: Contiene la información transaccional de ventas, facturas, datos financieros consolidados, entra otras tablas. Estos datos se acceden directamente mediante consultas SQL generadas por el LLM.\nDocumentos Internos (Vector Store): Archivos como manuales, reglamentos o cualquier otra información textual estática que se haya procesado para búsquedas semánticas.\n\n\n\n\n\n\nLos datos de ventas y finanzas se acceden directamente desde PostgreSQL en tiempo real. La preparación aquí, se centra en cómo el sistema interactúa con la base de datos:\n\nConexión: Se utiliza psycopg2 para establecer la conexión con la base de datos empresarial. Las credenciales se gestionan de forma segura mediante variables de entorno.\nGeneración de Consultas: El LLM es capaz de generar consultas SQL dinámicamente, basándose en la pregunta del usuario y el esquema de la base de datos. Se han definido reglas específicas en el prompt del sistema (settings/prompts.py) para asegurar la sintaxis correcta.\nManejo de Columnas: El LLM está instruido para inferir el significado de las columnas y, si es necesario, consultar el esquema de la base de datos (sql_db_schema tool) antes de generar una consulta. Esto minimiza errores por nombres de columnas desconocidos.\nMapeo de Productos: Se incluye una lista de nombres de productos (dicts.py) para ayudar al LLM a hacer matches precisos con las presentaciones de productos en la base de datos, mejorando la relevancia de las respuestas.\n\n\n\n\n\nPara la información estática contenida en documentos internos (que no están en las bases de datos SQL), se construye un vector store para permitir búsquedas semánticas:\n\nDocumentos: Los documentos textuales se cargan y se convierten en objetos langchain.schema.Document.\nEmbeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas.\nÍndice FAISS: Se crea un índice FAISS (LangchainVectorStore en langchain/vectorstore.py) a partir de estos embeddings. Este índice se guarda localmente para su uso eficiente.\nActualización: Aunque no se describe un ETL formal, la actualización de este vector store implicaría volver a procesar los documentos modificados o nuevos para regenerar el índice."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/2_preparacion.html#preparación-de-los-datos",
    "href": "2_salsas_chatbot/quarto/2_preparacion.html#preparación-de-los-datos",
    "title": "Preparación, modelado y evaluación de los datos",
    "section": "",
    "text": "A diferencia de un proceso ETL tradicional con extracción y transformación masiva de datos a un destino intermedio, el proyecto con Salsas Castillo se enfoca en la conexión de datos en tiempo real a través de herramientas SQL, y la preparación de un vector store para documentos internos.\n\n\nLas fuentes de información principales para el chatbot son:\n\nBase de Datos PostgreSQL: Contiene la información transaccional de ventas, facturas, datos financieros consolidados, entra otras tablas. Estos datos se acceden directamente mediante consultas SQL generadas por el LLM.\nDocumentos Internos (Vector Store): Archivos como manuales, reglamentos o cualquier otra información textual estática que se haya procesado para búsquedas semánticas.\n\n\n\n\n\n\nLos datos de ventas y finanzas se acceden directamente desde PostgreSQL en tiempo real. La preparación aquí, se centra en cómo el sistema interactúa con la base de datos:\n\nConexión: Se utiliza psycopg2 para establecer la conexión con la base de datos empresarial. Las credenciales se gestionan de forma segura mediante variables de entorno.\nGeneración de Consultas: El LLM es capaz de generar consultas SQL dinámicamente, basándose en la pregunta del usuario y el esquema de la base de datos. Se han definido reglas específicas en el prompt del sistema (settings/prompts.py) para asegurar la sintaxis correcta.\nManejo de Columnas: El LLM está instruido para inferir el significado de las columnas y, si es necesario, consultar el esquema de la base de datos (sql_db_schema tool) antes de generar una consulta. Esto minimiza errores por nombres de columnas desconocidos.\nMapeo de Productos: Se incluye una lista de nombres de productos (dicts.py) para ayudar al LLM a hacer matches precisos con las presentaciones de productos en la base de datos, mejorando la relevancia de las respuestas.\n\n\n\n\n\nPara la información estática contenida en documentos internos (que no están en las bases de datos SQL), se construye un vector store para permitir búsquedas semánticas:\n\nDocumentos: Los documentos textuales se cargan y se convierten en objetos langchain.schema.Document.\nEmbeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas.\nÍndice FAISS: Se crea un índice FAISS (LangchainVectorStore en langchain/vectorstore.py) a partir de estos embeddings. Este índice se guarda localmente para su uso eficiente.\nActualización: Aunque no se describe un ETL formal, la actualización de este vector store implicaría volver a procesar los documentos modificados o nuevos para regenerar el índice."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/4_despliegue.html",
    "href": "2_salsas_chatbot/quarto/4_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del chatbot para Salsas Castillo ha seguido un enfoque iterativo, centrado en la integración de modelos de lenguaje con las bases de datos existentes y la plataforma de Telegram. Los principales retos se concentraron en asegurar la interacción fluida y precisa del LLM con las bases de datos SQL para consultas dinámicas, así como la correcta gestión del historial de conversación y la generación de reportes en PDF.\nEl objetivo principal de optimizar el acceso a la información de productos y finanzas se ha mantenido constante a lo largo del proyecto, adaptándose a las particularidades de Salsas Castillo.\n\n\nConsiderando los avances en el desarrollo y las pruebas internas, se ha decidido priorizar el despliegue en un entorno de pruebas real para validar el comportamiento del chatbot en condiciones operativas y recopilar feedback directo de los usuarios.\n\n\n\nLa decisión es proceder con la implementación del chatbot en un entorno de pruebas de Telegram. Esto permitirá:\n\nValidar la integración completa con la API de Telegram.\nProbar la conectividad y el rendimiento con las bases de datos PostgreSQL y MongoDB en un entorno real.\nRecopilar feedback de usuarios internos para identificar mejoras en la experiencia de usuario y la precisión de las respuestas.\nAsegurar la robustez y estabilidad del sistema antes de una posible implementación a mayor escala."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/4_despliegue.html#revisión-del-proceso",
    "href": "2_salsas_chatbot/quarto/4_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del chatbot para Salsas Castillo ha seguido un enfoque iterativo, centrado en la integración de modelos de lenguaje con las bases de datos existentes y la plataforma de Telegram. Los principales retos se concentraron en asegurar la interacción fluida y precisa del LLM con las bases de datos SQL para consultas dinámicas, así como la correcta gestión del historial de conversación y la generación de reportes en PDF.\nEl objetivo principal de optimizar el acceso a la información de productos y finanzas se ha mantenido constante a lo largo del proyecto, adaptándose a las particularidades de Salsas Castillo.\n\n\nConsiderando los avances en el desarrollo y las pruebas internas, se ha decidido priorizar el despliegue en un entorno de pruebas real para validar el comportamiento del chatbot en condiciones operativas y recopilar feedback directo de los usuarios.\n\n\n\nLa decisión es proceder con la implementación del chatbot en un entorno de pruebas de Telegram. Esto permitirá:\n\nValidar la integración completa con la API de Telegram.\nProbar la conectividad y el rendimiento con las bases de datos PostgreSQL y MongoDB en un entorno real.\nRecopilar feedback de usuarios internos para identificar mejoras en la experiencia de usuario y la precisión de las respuestas.\nAsegurar la robustez y estabilidad del sistema antes de una posible implementación a mayor escala."
  },
  {
    "objectID": "2_salsas_chatbot/quarto/4_despliegue.html#plan-de-implementación",
    "href": "2_salsas_chatbot/quarto/4_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de Implementación",
    "text": "2. Plan de Implementación\nLa fase de implementación implica el despliegue del backend del chatbot y su integración con la plataforma de Telegram.\n\n2.1. Arquitectura de Despliegue y Conexión\nEl chatbot de Salsas Castillo se despliega como un servicio de backend basado en FastAPI, que interactúa con la API de Telegram mediante webhooks. La persistencia de datos se maneja con MongoDB y las consultas a datos transaccionales se realizan en PostgreSQL.\n\nBackend del Chatbot (FastAPI): La aplicación principal se despliega en un servidor (o ambiente virtual Linux) utilizando Gunicorn para producción o Uvicorn para desarrollo.\nConexión con Telegram: La comunicación se establece a través de webhooks. Telegram envía las actualizaciones de mensajes al endpoint /webhook de la API de FastAPI. El chatbot, a su vez, utiliza la API de Telegram para enviar respuestas y documentos (PDFs).\nBases de Datos:\n\nPostgreSQL: Se establece una conexión directa desde el backend del chatbot para las consultas SQL.\nMongoDB: Se utiliza para almacenar el historial de sesiones (sessions) y un respaldo completo de mensajes (message_backup).\n\n\n\n\n2.2. Gestión de Persistencia de Datos con MongoDB\nLa gestión del historial de conversaciones es crucial para un chatbot. En Salsas Castillo, se utiliza MongoDB con dos colecciones principales:\n\nsessions: Almacena los últimos mensajes de cada sesión de usuario para mantener el contexto de la conversación. Se configura para mantener un tamaño fijo (ej., los últimos 24 mensajes) para optimizar el rendimiento.\nmessage_backup: Actúa como un histórico completo de todas las interacciones (preguntas del usuario, respuestas del chatbot, metadatos). Es fundamental para el análisis de datos, auditorías y futuras mejoras del modelo.\n\n\n\n2.3. Plan de Monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema:\n\nTiempo de Respuesta: Latencia de las respuestas del chatbot, incluyendo el tiempo de ejecución de las consultas SQL y las llamadas a la API de OpenAI.\nTasa de Éxito/Error: Monitoreo de las peticiones a la API del chatbot y a las bases de datos.\nCalidad de las Respuestas: Evaluación manual de la precisión, coherencia y relevancia de las respuestas, especialmente en escenarios complejos o con datos numéricos.\nUso del Chatbot: Frecuencia de interacciones por usuario, tipos de consultas más comunes.\nErrores en Logs: Revisión de los logs del servidor para identificar excepciones o problemas en el backend.\n\n\n\n2.4. Plan de Mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema:\n\nActualización de Dependencias: Revisión y actualización regular de las librerías de Python (FastAPI, LangChain, PyMongo, etc.).\nRevisión de Logs: Monitoreo activo de los logs del servidor y de las bases de datos para identificar y solucionar problemas.\nAuditoría de Datos y Respuestas: Evaluación periódica de la calidad de los datos en PostgreSQL y MongoDB, y verificación de la precisión de las respuestas del chatbot a lo largo del tiempo.\nOptimización de Consultas: Refinamiento continuo de las consultas SQL generadas por el LLM para mejorar el rendimiento.\nActualización del Vector Store: Si se añaden nuevos documentos internos, se programará la actualización del vector store FAISS.\n\n\n\n2.5. Experiencia de Desarrollo\nEl proyecto ha permitido consolidar la experiencia en el desarrollo de un chatbot completo, desde la integración con plataformas de mensajería (Telegram) hasta la orquestación de LLMs con bases de datos relacionales y vectoriales. Los aprendizajes clave incluyen:\n\nManejo de la interacción entre LLMs y bases de datos SQL para consultas dinámicas.\nImplementación de la persistencia de sesiones y el historial de mensajes en MongoDB.\nDesarrollo de herramientas personalizadas (ej., generación de PDFs) y su integración en el flujo del agente.\nGestión de la transcripción de audio para una experiencia de usuario más inclusiva.\nAdherencia a buenas prácticas de desarrollo modular y escalable.\n\n\n\n2.6. Despliegue del Chatbot en el Sistema de Pruebas\nEl chatbot será desplegado en un entorno de pruebas de Telegram, accesible para un grupo controlado de usuarios internos de Salsas Castillo. Este despliegue permitirá validar el sistema en condiciones casi reales.\nEl proceso de despliegue consistirá en:\n\nMontaje del Entorno de la API: Despliegue de la API de FastAPI en un servidor dedicado, asegurando la conectividad con PostgreSQL y MongoDB.\nConfiguración del Webhook de Telegram: Establecer el webhook para que Telegram envíe las actualizaciones de mensajes al endpoint de la API.\nVerificación Funcional: Realizar pruebas exhaustivas para verificar el flujo de conversación, la precisión de las respuestas, la generación de PDFs y el manejo de la transcripción de audio.\nConsideraciones de Seguridad: Asegurar la autenticación de usuarios (mediante whitelist de Telegram IDs), el manejo seguro de credenciales y la protección de datos.\n\nEste hito marca un avance significativo hacia la validación en entorno real del sistema conversacional, permitiendo recopilar feedback de usuarios internos antes de considerar un despliegue completo en producción."
  }
]