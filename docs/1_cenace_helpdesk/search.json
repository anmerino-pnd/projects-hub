[
  {
    "objectID": "6_documentacion.html",
    "href": "6_documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngh repo fork https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "href": "6_documentacion.html#manual-de-instalación-y-despliegue",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source), así como conectividad a una instancia de MongoDB para el registro del historial de conversaciones y tickets.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nCUDA: Tarjeta de video y con drivers actualizados en el ambiente.\nPython: Versión 3.12.9.\nPip: Última versión.\nUV: Última versión (gestor de paquetes y entornos).\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial de conversaciones y tickets.\nPodman: Herramienta de virtualización y contenedores sin daemon; utilizado para correr MongoDB.\n\n\n\n\n\nFastAPI y Uvicorn: Utilizados para construir y servir la API web que expone los endpointsmdel chatbot y la gestión de documentos. Permiten crear una interfaz robusta y asíncrona.\nOllama (Python Client): Librería cliente para interactuar con el servicio Ollama, que hospeda y ejecuta los modelos de lenguaje open-source (gemma:4b) y de embeddings (bge-m3:latest) localmente.\nPymongo: El controlador oficial de Python para MongoDB. Es esencial para interactuar con la base de datos donde se almacena el historial de conversaciones, la información de los tickets y el registro de archivos procesados.\nFAISS (faiss-cpu): Biblioteca desarrollada por Meta AI para la búsqueda eficiente de similitud y agrupamiento de vectores densos. Es el núcleo de la base de datos vectorial del sistema.\nUV: Gestor de paquetes y entornos virtuales, asegura la reproducibilidad del entorno.\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\nClonar el repositorio:\n\ngh repo fork https://github.com/anmerino-pnd/proyectoCenace\ncd cenacellm\n\nConfigurar el entorno:\n\npip install uv\nuv venv\nsource .venv\\Scripts\\activate\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\nConfigurar Ollama:\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b y bge-m3:latest estén disponible.\n\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nollama pull bge-m3:latest\n\nConfigurar variables de entorno:\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n\n# Servidor donde está corriendo Ollama\nOLLAMA_BASE_URL=\"http://localhost:11434\"\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://localhost:27017\" \nDB_NAME = \"CENACE_LLM\"\n\nPreparar el backend:\n\nCon la ayuda de este comando arranca el contenedor de Mongo el cual es utilizado para guardar la información de las sesiones, conversaciones, documentos, etc.\npodman run -d --name mongo \\\n  -p 27017:27017 \\\n  docker.io/library/mongo:7.0\nEste comando inicia la API, especificando el número del puerto\nnohup uvicorn cenacellm.API.main:app --reload &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\nVerificar logs:\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out"
  },
  {
    "objectID": "6_documentacion.html#documentación-técnica-del-código",
    "href": "6_documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2. Documentación técnica del código",
    "text": "2. Documentación técnica del código\nLa solución se basa en una arquitectura de Recuperación Aumentada con Generación (RAG). La estructura modular del código, organizada en paquetes de Python, permite una clara separación de responsabilidades.\n\n2.1. Estructura de carpetas y módulos\nEl proyecto sigue una estructura modular para facilitar la gestión y el mantenimiento. A continuación, se detalla el propósito de los módulos y clases principales, además de sus funciones clave.\n\n2.1.1. Documentos\n\ncenacellm/doccollection.py\n\nClases y funciones clave\n\nclass DisjointCollection(DocCollection):\n\n__init__:\n\nPropósito: Inicializar la configuración predeterminada para la segmentación de textos.\nComportamiento: Establece el tamaño del fragmento (chunk_size) en 1500 caracteres y el solapamiento máximo (max_overlap) en 200 caracteres para asegurar continuidad entre segmentos.\n\nget_chunks() -&gt; list:\n\nPropósito: Dividir uno o varios objetos de texto en fragmentos más pequeños basados en la configuración semántica definida.\nParámetros:\n\ntexts (Union[Text, List[Text]]): Un objeto Text individual o una lista de objetos Text que contienen el contenido a fragmentar.\n\nRetorna: Una lista de objetos Text donde cada elemento es un fragmento del contenido original, conservando los metadatos.\nComportamiento: Utiliza TextSplitter para segmentar el contenido. Si la entrada es un solo texto, lo convierte en lista. Itera sobre los textos, genera los chunks y crea nuevos objetos Text heredando los metadatos del padre.\n\nload_pdf() -&gt; List[Text]:\n\nPropósito: Leer un archivo PDF, extraer su contenido textual página por página y generar metadatos detallados.\nParámetros:\n\npdf_path (str): La ruta del archivo PDF a procesar.\ncollection (str): El nombre opcional de la colección a la que pertenecerá el documento.\n\nRetorna: Una lista de objetos Text, donde cada objeto representa el contenido de una página del PDF.\nComportamiento: Utiliza PdfReader para leer el archivo. Genera un ID de referencia único (uuid4). Extrae metadatos nativos del PDF y crea un diccionario de metadatos enriquecido para cada página (incluyendo número de página, total de páginas, nombre de archivo y referencia). Finalmente, instancia objetos Text con el contenido extraído y estos metadatos.\n\n\n\n\n\n\n\n2.1.2. Embedder\n\ncenacellm/ollama/embedder.py\n\nClases y funciones clave\n\nclass OllamaEmbedder(Embedder):\n\n__init__:\n\nPropósito: Configurar el modelo de embeddings que se utilizará para la vectorización.\nComportamiento: Define el modelo bge-m3:latest como el motor predeterminado para generar los vectores.\n\nvectorize() -&gt; NDarray:\n\nPropósito: Convertir una cadena de texto en su representación vectorial numérica.\nParámetros:\n\ns (str): La cadena de texto (prompt) que se desea vectorizar.\n\nRetorna: Un arreglo de NumPy (np.array) con tipo de dato float32 que representa el embedding del texto.\nComportamiento: Realiza una llamada a la API de Ollama utilizando el modelo configurado y retorna el vector resultante extraído de la respuesta.\n\nvectorize_batch() -&gt; list[np.ndarray]:\n\nPropósito: Generar embeddings para una lista de textos múltiples en secuencia.\nParámetros:\n\ntexts (list[str]): Lista de cadenas de texto a vectorizar.\n\nRetorna: Una lista de arreglos de NumPy, correspondientes a los vectores de cada texto de entrada.\nComportamiento: Itera sobre la lista de textos proporcionada y llama al método vectorize para cada elemento individualmente, acumulando los resultados.\n\ndim() -&gt; int:\n\nPropósito: Obtener la dimensión del espacio vectorial generado por el modelo\nRetorna: Un entero que representa la longitud del vector (número de dimensiones).\nComportamiento: Vectoriza una palabra de prueba (“Hola”) y calcula la longitud del arreglo resultante para determinar la dimensionalidad del modelo.\n\n\n\n\n\n\n\n2.1.3. Vector Store\n\ncenacellm/vectorstore.py\n\nClases y funciones clave\n\nclass FAISSVectorStore(VectorStore):\n\n__init__:\n\nPropósito: Inicializar el índice de búsqueda vectorial (FAISS) y cargar datos persistentes si existen.\nComportamiento: Verifica si existe un índice previo en disco. Si existe, lo carga (intentando usar GPU si es posible) junto con el diccionario de textos (pickle). Si no, crea un índice IndexFlatL2 nuevo.\n\nget_similar() -&gt; list:\n\nPropósito: Realizar una búsqueda de similitud semántica en el índice vectorial.\nParámetros:\n\nv (np.ndarray): El vector de consulta (query embedding).\nk (int): Número de vecinos más cercanos a recuperar (por defecto 10).\nfilter_metadata (Dict[str, str]): Filtros opcionales para restringir la búsqueda (ej. por colección).\n\nRetorna: Una lista de tuplas (vector, texto) con los resultados más relevantes.\nComportamiento: Ejecuta index.search y filtra los resultados basándose en los metadatos proporcionados y la validez de los índices recuperados.\n\nadd_text() -&gt; None:\n\nPropósito: Añadir un único vector y su texto asociado al índice.\nParámetros:\n\nv (np.ndarray): El vector a añadir.\nt (Text): El objeto de texto asociado.\n\nComportamiento: Añade el vector al índice FAISS y almacena el par (vector, texto) en el diccionario en memoria.\n\nadd_texts() -&gt; None:\n\nPropósito: Añadir un lote de vectores y textos al índice de manera eficiente.\nParámetros:\n\nvectors (list[np.ndarrar]): Lista de vectores.\nchunks (list[Text]): Lista de objetos de texto correspondientes.\n\nComportamiento: Apila los vectores en una matriz numpy y los añade al índice en una sola operación, actualizando luego el diccionario secuencialmente.\n\nsave_index() -&gt; None:\n\nPropósito: Persistir el estado actual del índice y los textos en disco.\nComportamiento: Escribe el índice FAISS en un archivo .faiss y serializa el diccionario de textos en un archivo .pkl.\n\ndistance() -&gt; None:\n\nPropósito: Calcular la distancia euclidiana entre dos vectores.\nParámetros:\n\nv1 (np.ndarrar): Primer vector.\nv2 (np.ndarrar): Segundo vector.\n\nRetorna: Un valor flotante representando la distancia (norma L2).\n\ndelete() -&gt; None:\n\nPropósito: Eliminar un elemento específico del índice por su ID interno.\nParámetros:\n\nidx (int): Índice numérico del elemento a eliminar.\n\nComportamiento: Elimina la entrada del diccionario y remueve el ID del índice FAISS.\n\nupdate_metadata() -&gt; None:\n\nPropósito: Actualizar los metadatos de un texto ya indexado.\nParámetros:\n\nidx (int): Índice del elemento.\nnew_metadata (Dict[str, str]): Diccionario con los valores a actualizar.\n\nComportamiento: Crea una copia del objeto Text y TextMetadata con los nuevos valores y actualiza la referencia en el diccionario.\n\ndelete_by_reference() -&gt; None:\n\nPropósito: Eliminar todos los vectores asociados a un documento específico.\nParámetros:\n\nreference_id (str): UUID del documento a eliminar.\n\nComportamiento: Itera sobre el diccionario para encontrar todos los índices que coincidan con la referencia y los elimina tanto del diccionario como del índice FAISS.\n\n\n\n\n\n\n\n2.1.4. Agente\n\ncenacellm/ollama/assistant.py\n\nClases y funciones clave\n\nclass OllamaAssistant(Assistant):\n\n__init__:\n\nPropósito: Configurar la conexión a MongoDB y definir el modelo LLM (gemma3:4b).\nComportamiento: Establece la conexión con la base de datos, selecciona las colecciones y crea índices para consultas eficientes.\n\nload_history() -&gt; list:\n\nPropósito: Recuperar el historial de chat de una conversación.\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\n\nRetorna: Lista de mensajes (diccionarios).\nComportamiento: Consulta MongoDB filtrando por usuario y conversación.\n\nsave_history() -&gt; None:\n\nPropósito: Guardar o actualizar el historial de una conversación.\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\nhistory (list): Lista actualizada de mensajes.\nconversation_title (Optional[str]): Título opcional para la conversación.\n\nComportamiento: Realiza una operación update_one con upsert=True en MongoDB, actualizando mensajes, fecha de modificación y título si se provee.\n\nsave_backup() -&gt; None:\n\nPropósito: Guardar un respaldo incremental del historial.\nParámetros:\n\nuser_id (str): ID del usuario.\nhistory_chunk (list): Fragmento de mensajes a respaldar.\n\nComportamiento: Hace un push de los nuevos mensajes a una colección de respaldo separada.\n\nclear_conversation_history() -&gt; None:\n\nPropósito: Limpiar los mensajes de una conversación sin borrar el registro de la misma.\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\n\nComportamiento: Establece el campo messages como una lista vacía en MongoDB.\n\ndelete_conversation() -&gt; None:\n\nPropósito: Eliminar completamente una conversación.\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\n\nComportamiento: Elimina el documento completo de la colección de conversaciones.\n\nmake_metadata() -&gt; CallMetadata:\n\nPropósito: Estructurar los metadatos de una llamada al LLM.\nParámetros:\n\nresponse (GenerateResponse): Objeto de respuesta de Ollama.\nduration (float): Tiempo de ejecución.\nreferences: Chunks utilizados como contexto.\n\nRetorna: Objeto CallMetadata estandarizado.\n\nanswer() -&gt; Tuple[Generator[str, None, None], str, Dict[str, Any]]:\n\nPropósito: Generar una respuesta del asistente utilizando contexto y streaming.\nParámetros:\n\nquestion (Question): Pregunta del usuario.\nchunks (Chunks): Contexto recuperado.\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\n\nRetorna: Un generador de tokens, el ID del mensaje del bot y los metadatos finales.\nComportamiento: Construye el prompt con historial y contexto, llama a api.generate en modo stream, acumula tokens, y al finalizar guarda el turno en el historial y calcula metadatos.\n\nupdate_message_metadata() -&gt; bool:\n\nPropósito: Actualizar metadatos de un mensaje específico (ej. “like”).\nParámetros:\n\nuser_id (str): ID del usuario.\nmessage_id (str): ID del mensaje completo.\nnew_metadata (Dict[str, Any]): Metadatos nuevos a actualizar.\n\nComportamiento: Busca el mensaje en el historial del usuario y actualiza sus campos de metadatos en MongoDB.\n\nget_liked_solutions() -&gt; List[Dict[str, Any]]:\n\nPropósito: Obtener todas las respuestas marcadas como útiles (“liked”) por el usuario.\nParámetros:\n\nuser_id (str): ID del usuario.\n\nRetorna: Lista de pares pregunta-respuesta útiles.\nComportamiento: Itera sobre todas las conversaciones del usuario buscando mensajes con metadata.disable = True.\n\nget_user_conversations() -&gt; List[Dict[str, Any]]:\n\nPropósito: Listar las conversaciones activas del usuario.\nParámetros:\n\nuser_id (str): ID del usuario.\n\nRetorna: Lista con ID, título y fecha de actualización.\nComportamiento: Consulta MongoDB proyectando solo los campos necesarios y ordenando por fecha. Genera un título por defecto si no existe.\n\nhas_liked_solution_in_conversation() -&gt; bool:\n\nPropósito: Verificar si una conversación contiene soluciones validadas.\nParámetros:\n\nconversation_id (str): ID de la conversación.\n\nRetorna: True si existe al menos un mensaje “liked”.\n\n\n\n\n\n\n\n2.1.5. RAG\n\ncenacellm/rag.py\n\nClases y funciones clave\n\nclass RAG:\n\n__init__:\n\nPropósito: Orquestar todos los componentes del sistema (Assistant, Embedder, VectorStore, DB).\nComportamiento: Inicializa las instancias, conecta a MongoDB, carga cachés de archivos procesados y configura índices únicos en la base de datos.\n\n_load_processed_files() -&gt; Dict[str, Any]:\n\nPropósito: Cargar en memoria el registro de archivos ya procesados para evitar re-procesamiento.\nRetorna: Diccionario mapeando claves de archivo a sus metadatos.\n\n_save_processed_files() -&gt; None:\n\nPropósito: Persistir el estado de los archivos procesados en MongoDB.\n\n_delete_processed_file() -&gt; None:\n\nPropósito: Eliminar registros de archivos procesados de la base de datos y memoria.\nParámetros:\n\nfile_key (List[str]): ID del documento procesado en MongoDB.\n\nComportamiento: Cargar IDs de soluciones ya indexadas para evitar duplicados.\n\n_load_processed_solutions_ids() -&gt; set:\n\nPropósito: Cargar IDs de soluciones ya indexadas para evitar duplicados.\n\n_add_processed_solution_id() -&gt; None:\n\nPropósito: Registrar una nueva solución como procesada en MongoDB.\nParámetros:\n\nuser_id (str): ID del usuario.\nmessage_id (str): ID del documento del mensaje en MongoDB.\n\n\nload_documents() -&gt; list:\n\nPropósito: Procesar una carpeta de PDFs e indexarlos.\nParámetros:\n\nfolder_path (str): Ruta de los archivos.\ncollection_name (str): Nombre de la colección lógica.\nforce_reload (bool): Forzar re-procesamiento si ya existen.\n\nRetorna: Estadísticas [docs_totales, nuevos_docs, chunks_totales]\nComportamiento: Lee PDFs, verifica cambios (tamaño/fecha), fragmenta, vectoriza por lotes, guarda en vectorstore y actualiza el registro de archivos procesados.\n\nquery() -&gt; Tuple[Generator[str, None, None], List, str, Dict[str, Any]]:\n\nPropósito: Ejecutar la lógica de recuperación y llamada al asistente (núcleo del RAG).\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\nquestion (str): Consulta del usuario.\nk (int): Número de chunks a recuperar.\nfilter_metadata (Optional[Dict[str, Any]]): Filtros de búsqueda.\n\nRetorna: Generador de tokens, chunks usados, ID del mensaje y metadatos.\nComportamiento: Vectoriza la pregunta, busca en el vectorstore (balanceando documentos y soluciones si no hay filtro), y delega la generación al assistant.\n\nanswer() -&gt; Generator[Union[str, Dict[str, Any]], None, None]:\n\nPropósito: Wrapper sobre query para exponer una interfaz de streaming unificada.\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\nquestion (str): Consulta del usuario.\nk (int): Número de chunks a recuperar.\nfilter_metadata (Optional[Dict[str, Any]]): Filtros de búsqueda.\n\nRetorna: Generador que emite tokens de texto y finalmente un JSON con metadatos.\n\ndelete_conversation() -&gt; None:\n\nPropósito: Eliminar una conversación y desvincular tickets asociados\nParámetros:\n\nuser_id (str): ID del usuario.\nconversation_id (str): ID de la conversación.\n\nComportamiento: Llama a assistant.delete_conversation y actualiza tickets en MongoDB poniendo su solucion_id en None.\n\nadd_liked_solutions_to_vectorstore() -&gt; int:\n\nPropósito: Convertir interacciones exitosas en nuevo conocimiento vectorial.\nParámetros:\n\nuser_id (str): ID del usuario.\n\nRetorna: Cantidad de soluciones añadidas.\nRetorna: Obtiene soluciones “liked”, verifica si ya existen, extrae metadatos, crea nuevos objetos Text y los indexa en el vectorstore.\n\ndelete_from_vectorstore() -&gt; None:\n\nPropósito: Eliminar documentos o soluciones del índice vectorial.\nParámetros:\n\nreference_id (str): ID del vector almacenado en la base de datos vectorial.\n\nComportamiento: Toma un índice del vector objetivo, lo busca en el archivo indexado y lo elimina.\n\nget_tickets() -&gt; List:\n\nPropósito: Obtener todos los tickets con su estado de resolución.\nComportamiento: Consulta MongoDB y calcula dinámicamente el campo is_solved.\n\nadd_ticket() -&gt; Dict:\n\nPropósito: Obtener todos los tickets con su estado de resolución.\nComportamiento: Consulta MongoDB y calcula dinámicamente el campo is_solved.\n\nupdate_ticket_metadata() -&gt; bool:\n\nPropósito: Modificar campos de un ticket existente.\nParámetros:\n\nticket_reference (str): ID del ticket almacenado en MongoDB.\nnew_metadata (Dict[str, Any]): Nuevos metadatos a actualizar.\n\nComportamiento: Actualiza los metadatos de un ticket específico en la base de datos basado en su ‘reference’.\n\nget_ticket_by_conversation() -&gt; Optional[Dict]:\n\nPropósito: Encontrar el ticket asociado a una conversación activa.\n\n\n\n\n\n\n\n2.1.6. API\n\ncenacellm/API/chat.py\nEsta capa actúa como lógica de negocio intermedia entre FastAPI y el sistema RAG.\n\nClases y funciones clave\nclass QueryRequest: Modelo de datos para la solicitud de chat (user_id, query, conversation_id, etc.).\nclass UpdateMetadataRequest: Modelo para actualizar metadatos (ej. likes).\nclass CreateConversationRequest: Modelo para iniciar conversaciones (title opcional).\nclass AddTicketRequest: Modelo para creación de tickets.\nasync_chat_stream() -&gt; StreamingResponse:\n\nPropósito: Iniciar el flujo de respuesta del chat.\nComportamiento: Llama a rag.answer y devuelve un StreamingResponse para envío en tiempo real.\n\nload_documents(), get_preprocessed_files(): Wrappers para exponer funciones del RAG.\nupload_documents() -&gt; Dict:\n\nPropósito: Guardar archivos físicos en el servidor.\nComportamiento: Recibe UploadFile, escribe en disco y retorna estado.\n\ndelete_document() -&gt; Dict:\n\nPropósito: Orquestar la eliminación de documentos.\nComportamiento: Elimina del vectorstore (vía RAG) y borra el archivo físico del disco.\n\nprocess_liked_solutions_to_vectorstore(): Dispara la re-indexación de soluciones útiles.\ndelete_solution_by_reference(): Elimina soluciones aprendidas del vectorstore.\n\n\n\ncenacellm/API/main.py\nAquí se encuentra el main de todo el sistema y las llamadas FastAPI…\n\nClases y funciones clave\n\nPOST/chat: Inicia la generación de respuesta en streaming.\nGET /history/{user_id}/{conversation_id}: Devuelve el historial de mensajes.\nDELETE /history/{user_id}/{conversation_id}: Borra el historial de una conversación.\nPOST /upload_documents: Sube archivos PDF al servidor.\nPOST /load_documents: Dispara el procesamiento e indexación de los PDFs subidos.\nPOST /delete_document: Elimina documentos procesados.\nPATCH /history/{user_id}/messages/{message_id}: Actualiza metadatos (usado para dar like).\nPOST /process_liked_solutions/{user_id}: Convierte likes en conocimiento vectorial.\nGET /conversations/{user_id}: Lista conversaciones del usuario.\nPOST /new_conversation: Crea una nueva sesión de chat.\nPOST /delete_conversation: Elimina una sesión.\nGET /tickets y POST /tickets: Gestión de tickets de soporte.\nGET /: Sirve la interfaz de usuario (UI) renderizando index.html.\n\n\n\n\n\n\n2.2. Modelos LLM utilizados\nEl flujo de información en el sistema RAG sigue dos rutas principales:\n\nIndexación de documentos:\n\n\nLos archivos PDF son cargados y procesados por el módulo doccollection.py.\ndoccollection divide cada documento en fragmentos.\nCada fragmento es enviado al que se encuentra en embedder.py y el modelo bge-m3 genera su representación vectorial.\nLos vectores resultantes se almacenan en la base de datos vectorial de FAISS, implementada en vectorstore.py, junto con sus metadatos.\n\n\nProceso de consulta (QA):\n\n\nUna consulta de usuario llega el endpoint de chat.py.\nLa consulta es vectorizada por el embedder.\nEl vectorstore realiza una búsqueda de similitud semántica para recuperar los fragmentos de documento más relevantes.\nEstos fragmentos se envían al assistant.py, que los utiliza como contexto.\nEl assistant utiliza el LLM (gemma3:4b) para generar una respuesta coherente y contextualizada.\nLa respuesta es devuelta al usuario a través del chat.py y el main.py.\n\n\n\n2.3. Puntos de entrada y funciones clave\n\nGestión de conversaciones: El módulo assistant.py gestiona el historial de conversación en MongoDB, permitiendo que el chatbot mantenga un contexto limitado con el usuario.\nGestión de tickets: Las funciones add_ticket y update_ticket_metadata en rag.py y sus respectivos endpoints en chat.py demuestran la capacidad del sistema para interactuar y actualizar una base de datos de tickets.\nBucle de retroalimentación: La funcionalidad has_liked_solution_in_conversation permite identificar y potencialmente re-indexar soluciones validadas por los usuarios, mejorando continuamente la base de conocimientos."
  },
  {
    "objectID": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "6_documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3. Guía de entrenamiento y mejora",
    "text": "3. Guía de entrenamiento y mejora\n\n3.1. Generación de la base de datos vectorial\nLa base de conocimientos del chatbot se construye a partir de un proceso que comprende la extracción, segmentación y vectorización del contenido textual proveniente de documentos en formato PDF.\nLos vectores resultantes son posteriormente indexados y almacenados en una base de datos vectorial, la cual constituye el núcleo de la recuperación de información relevante durante las interacciones con el chatbot.\n\n\n3.2. Flujo de la interacción\nEl usuario debe acceder a la pestaña Documentos, donde podrá seleccionar los archivos que desea incorporar a la base de conocimientos del sistema. Una vez elegidos, los documentos se suben a la carpeta correspondiente dentro del entorno donde se encuentra desplegado el sistema (backend). Posteriormente, estos archivos son procesados siguiendo el flujo descrito en el apartado anterior, dando como resultado la creación de la base vectorial o base de conocimientos del sistema.\n\n\n\nGeneración de la base de datos vectorial\n\n\n\n\n3.3. Recomendaciones para futura mejora\n\nLectura de documentos escaneados\n\nActualmente, el sistema no puede extraer información de documentos escaneados. Sería recomendable integrar un módulo de Reconocimiento Óptico de Caracteres (OCR) para ampliar la capacidad de análisis, o de igual manera, Modelo Multimodales que pudieran extraer la información y almacenarla en documentos PDF que sean posteriormente vectorizados.\n\nSistema de seguridad para el inicio de sesión\n\nEl mecanismo de inicio de sesión actual es básico, pues solo requiere ingresar el nombre del usuario.\nAunque esta simplicidad se ajusta al alcance inicial del proyecto, se sugiere incorporar un sistema de autenticación más robusto, que garantice la seguridad de acceso y manejo de información.\n\nSistema de corrección de ortografía\n\nDurante el desarrollo de los modelos de clasificación, se identificó que la falta de ortografía en los tickets afectaba la calidad del análisis.\nSe propuso el desarrollo de un sistema tipo journalist capaz de identificar las 5 W’s y reconstruir el contexto completo del texto, corrigiendo iterativamente los errores ortográficos al momento de cargar los datos."
  },
  {
    "objectID": "6_documentacion.html#arquitectura-del-sistema",
    "href": "6_documentacion.html#arquitectura-del-sistema",
    "title": "Documentación",
    "section": "4. Arquitectura del sistema",
    "text": "4. Arquitectura del sistema\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial.\n\n\n\nArquitectura del sistema\n\n\n\n4.1. Componentes clave de conversación y chat\n\nPOST /chat/stream: Endpoint principal para la interacción conversacional. Recibe una consulta y un conversation_id, y devuelve una respuesta generada por el LLM en tiempo real a través de un stream.\nGET /chat/history/{user_id}/{conversation_id}: Recupera el historial de mensajes de una conversación específica.\nPOST /conversations: Crea una nueva conversación, generando un conversation_id único.\nGET /conversations/{user_id}: Lista todas las conversaciones de un usuario, incluyendo sus títulos y la fecha de la última actualización.\nDELETE /conversations: Elimina una conversación específica y su historial de la base de datos.\nPATCH /message-metadata: Permite actualizar los metadatos de un mensaje, utilizado para la funcionalidad de “gustar” una solución.\n\n\n\n4.2. Componentes clave de documentos y soluciones\n\nPOST /documents: Permite cargar nuevos archivos (en formato PDF) a la base de datos vectorial para expandir la base de conocimientos.\nGET /documents: Lista todos los documentos que han sido procesados y están disponibles para la consulta.\nDELETE /documents: Elimina un documento específico de la base de datos vectorial, eliminando también su referencia y los fragmentos asociados.\nPOST /solutions: Procesa y re-indexa soluciones “gustadas” por los usuarios, agregándolas como nuevos documentos a la base de datos vectorial para mejorar la precisión del sistema.\nDELETE /solutions: Elimina una solución específica de la base de datos vectorial.\n\n\n\n4.3. Componentes clave de tickets\n\nGET /tickets: Recupera una lista de todos los tickets almacenados en la base de datos de MongoDB.\nPOST /tickets: Permite añadir un nuevo ticket a la base de datos, con campos como título, descripción y categoría.\nPATCH /tickets/{ticket_reference}: Actualiza los metadatos de un ticket existente, como su estado de solución (is_solved)."
  },
  {
    "objectID": "4_modelado.html",
    "href": "4_modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#modelado",
    "href": "4_modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en el uso de modelos de lenguaje grandes (open-source) alojados localmente, los modelos considerados para esta fase son los siguientes:\n\nModelos open-source integrados mediante Ollama: Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contempla el modelo gemma3:4b, que ofrece un buen rendimiento en tareas conversacionales, manteniendo la privacidad de la información sensible del CENACE. Este modelo es de código abierto y no requiere una API externa, lo que se alinea con la necesidad de mantener el control sobre los datos.\nModelos de embeddings: Se utiliza un modelo de embeddings como bge-m3:latest (también disponible en Ollama) para convertir los fragmentos de la documentación en vectores numéricos, lo que permite una búsqueda semántica eficiente en la base de datos vectorial.\n\n\n\n\nComo se mencionó anteriormente, la arquitectura fundamental del sistema es de tipo RAG. Cuando un usuario envía una consulta, el sistema realiza los siguientes pasos:\n\nLa consulta se transforma en un vector.\nSe realiza una búsqueda de similitud en la base de datos vectorial (FAISS) para encontrar los fragmentos de documentos más relevantes.\nEstos fragmentos, junto con la consulta del usuario, se envían al LLM (gemma3:4b) alojado en Ollama.\nEl LLM utiliza el contexto proporcionado para generar una respuesta coherente y precisa que es devuelta al usuario.\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquestion: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nuser_id: Identificador de sesión que permite obtener el contexto del usuario.\nconversation_id: Identificador de la conversación del usuario.\nk: Cantidad de documentos recuperados.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información utilizada por el sistema está referenciada de los documentos cargados que se le proveen al chatbot.\nLas respuestas siguen un orden y van acorde al incidente que se está atendiendo.\nEl modelo es capaz de analizar y brindar soluciones solamente a partir de la información de la base de datos vectorial sin presentar alucinaciones en el desarrollo de la respuesta, en un 95% de los casos.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "4_modelado.html#evaluación",
    "href": "4_modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2. Evaluación",
    "text": "2. Evaluación\nLa fase de evaluación es crucial para validar el desempeño del sistema y asegurar que cumple con los objetivos del proyecto. La evaluación se realiza a través de pruebas manuales y automatizadas.\n\n2.1 Criterios de evaluación\nLa evaluación de un sistema de este tipo es una tarea no trivial. En lugar de métricas tradicionales, se adoptó un enfoque basado en la calidad de la recuperación y la generación, utilizando el marco propuesto por RAGAS. Este enfoque se centra en tres dimensiones clave:\n\nFidelidad: Las respuestas se basan únicamente en el contexto recuperado (los PDF y base de conocimientos). Que no presente alucinaciones en un 95% de los casos. ¿La respuesta se basa fielmente en el contexto recuperado?\nRelevancia de la respuesta: Las respuestas generadas respondan a la consulta del usuario. ¿La respuesta aborda directamente la consulta del usuario?\nRelevancia del contexto: La información recuperada hayan sido relevantes y útiles. ¿La información recuperada es pertinente para la pregunta?\n\nEstas métricas, alineadas con el juicio humano, permiten un ciclo de evaluación robusto y ágil."
  },
  {
    "objectID": "2_comprension_datos.html",
    "href": "2_comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#recolección-de-los-datos",
    "href": "2_comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información son, los documentos de información (manuales, guías, contratos, etc.) y la base de datos que contiene todos los incidentes resueltos por parte de la gerencia Noroeste del CENACE. Además, se nos compartieron 4 archivos pdf para crear una base de datos vectorial con ellos. Los incidentes resueltos fueron extraídos de SQL y trabajados en formato csv.\n\n\n\nInicialmente, se realizó un Análisis Exploratorio de Datos (EDA) sobre un conjunto de 175 incidentes registrados entre enero de 2023 y mayo de 2024. Este conjunto incluía 14 variables, como el título, la descripción, la categoría y la solución, entre otras. El título y la descripción se utilizarán posteriormente para la clasificación de los incidentes.\nAunque este EDA proporcionó hallazgos relevantes, también puso de manifiesto la necesidad de trabajar con un conjunto de datos más amplio y abordar las inconsistencias presentes en los registros.\n\n\n\nPara incrementar el volumen de datos disponibles, fue necesario realizar un proceso de codificación y anonimización para proteger información sensible (nombres de personal, números telefónicos, correos electrónicos, normas, nombres de subestaciones, etc.).\nUtilizando herramientas como expresiones regulares y SpaCy, logramos recuperar un total de 2,817 registros, lo que representa 15 veces más registros que los obtenidos inicialmente."
  },
  {
    "objectID": "2_comprension_datos.html#descripción-de-los-datos",
    "href": "2_comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2. Descripción de los datos",
    "text": "0.2. Descripción de los datos\nA partir de los datos extraídos, se obtuvo la siguiente información:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2817 entries, 0 to 2816\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   titulo       2817 non-null   object\n 1   descripcion  2817 non-null   object\n 2   solucion     1159 non-null   object\n 3   categories   2817 non-null   object\n 4   fecha        2817 non-null   object\ndtypes: object(5)\nmemory usage: 110.2+ KB\nLas variables se describen a continuación:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\ntitulo\nTítulo del incidente en cuestión.\nTexto\n\n\ndescripcion\nDesarrollo de la problemática y explicación del incidente.\nTexto\n\n\nsolucion\nExplicación de cómo se llegó a la solución.\nTexto\n\n\ncategories\nCategoría a la que pertenece la problemática.\nTexto\n\n\nfecha\nFecha.\nTexto"
  },
  {
    "objectID": "2_comprension_datos.html#exploración-de-los-datos",
    "href": "2_comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de categorías\nLa exploración comenzó con el análisis del número de categorías únicas en los tickets. Esto es crucial, ya que parte del proyecto es desarrollar un modelo capaz de clasificar o asignar una categoría a nuevos tickets a partir de su contenido.\ncategories\nADTR SP7 &gt; SCADA                                     1410\nINTRANET Y SOPORTE DE APLICACIONES                    379\nCOMPUTO Y PERIFERICOS                                 257\nSEGURIDAD INFORMATICA                                 166\nCORREO ELECTRONICO                                     97\nADTR SP7 &gt; SIREL                                       89\nADTR SP7                                               82\nTELEFONIA Y HERRAMIENTAS COLABORATIVAS                 55\nINFRAESTRUCTURA BASICA Y DE SERVICIOS PROPIOS          51\nADTR &gt; Consulta                                        38\nADTR SP7 &gt; Historico                                   37\nADTR SP7 &gt; SIGUARD                                     32\nINTERNET                                               32\nADTR SP7 &gt; Hospedaje                                   23\nADOMEM                                                 16\nADTR                                                   11\nDESARROLLO DE APLICACIONES                             10\nOPERACION DE RED DE DATOS                               9\nMESA DE SERVICIO                                        8\nBASE DE DATOS                                           7\nADTR &gt; Hospedaje de Aplicativos de Potencia (EMS)       6\nMONITOREO DE ACTIVOS DE TIC                             2\nName: count, dtype: int64\nAl observar la distribución, se decidió trabajar únicamente con categorías que tuvieran más de 20 incidentes, considerando la cantidad de datos necesaria para los procesos de entrenamiento, validación y prueba. Esta selección resultó en un total de 14 categorías.\n\n\n0.3.2. Distribución de las palabras\nEn esta sección se analiza la cantidad de palabras utilizadas en los títulos, descripciones y soluciones. Esto nos permite entender la cantidad de información disponible que puede ser útil para resolver problemas recurrentes y para el desarrollo del modelo de clasificación.\n\n\n\nDistribución de palabras en los títulos\n\n\nA partir de esta gráfica podemos notar que alrededor de 9 palabras promedio son las que se utilizan en los títulos; lo cual es normal dado que tendemos a englobar las problemáticas en pocas palabras. Sin embargo, vemos que hay problemáticas que pueden pasar el promedio, hasta llegar a las 30 palabras aproximadamente.\n\n\n\nDistribución de palabras en las descripciones\n\n\nPara las descripciones vemos que el promedio es mayor, aproximadamente hasta las 88 palabras, aunque en la mayoría de los casos son menos las que se utilizan. Esto también es normal ya que aquí es donde las personas desarrollan los detalles del incidente el cual están enfrentando. También vemos que hay incidentes que pueden tomar tantas palabras hasta llegar a las 400, 500, y hasta las 1000, aunque este último sea poco común.\n\n\n\nDistribución de palabras en las soluciones\n\n\nEn este caso, nosotros esperábamos que en esta sección hubiéramos encontrado una mayor cantidad de palabras, porque en este caso encontramos que en promedio se utilizan 23 palabras, donde frecuentemente son menos. También encontramos que no todos los incidentes contienen una descripción detallada de la solución o de los pasos que se siguieron para resolver la situación; vimos que solo el 41% de los incidentes contienen una explicación de la solución.\n\n\n0.3.3. Análisis de los bigramas más comunes\nSe realizó un análisis de los pares de palabras más frecuentes en los títulos y descripciones para identificar patrones en la forma en que se plantean las problemáticas.\n\n\n\nBigramas de los títulos\n\n\nEn los títulos lo primero que llama la atención es la codificación de ciertas palabras, ya que por detalles confidenciales, se codificaron nombres propios, sistemas, subestaciones, etc. Luego, podemos ver que se mencionan muchas veces los despliegues de distintos sistemas de software y actualizaciones.\n\n\n\nBigramas de las descripciones\n\n\nEn las descripciones vemos que en la gran mayoría de los casos son saludos hacia la persona que se están dirigiendo. Dejando de lado estos saludos, vemos que se mencionan detalles muy técnicos y específicos que solo expertos en el tema podrán entender."
  },
  {
    "objectID": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "2_comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nA partir de los resultados previos, se determinó que la columna con menos información es la de soluciones. Solo el 41% de los registros contienen una explicación, con un promedio de 23 palabras, aunque en muchos casos la cantidad es menor.\nAl discutir esta situación con los expertos, se descubrió que el desarrollo y la documentación de soluciones no es una práctica común en el departamento, lo que resulta en una pérdida de conocimiento. Esto motivó la búsqueda de una propuesta para mejorar la persistencia de las soluciones."
  },
  {
    "objectID": "0_home.html",
    "href": "0_home.html",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "0_home.html#introducción",
    "href": "0_home.html#introducción",
    "title": "Desarrollo de un help desk basado en un modelo de lenguaje grande",
    "section": "",
    "text": "Este proyecto se centra en la elaboración de un sistema inteligente incorporado al sistema de seguimiento de incidentes de la mesa de ayuda (Help Desk) del Centro Nacional de Control de Energía (CENACE). Utilizando manuales, guías de procedimientos y la base de conocimientos de la organización, el sistema incorpora modelos grandes de lenguaje (LLMs) y técnicas de procesamiento de lenguaje natural (PLN) para la clasificación de incidentes, recuperación de información relevante y generación de soluciones sugeridas. Este sistema no solo proveerá apoyo inmediato a los ingenieros del CENACE, sino que también nutrirá la base de conocimientos implementada con las soluciones generadas.\nEl proyecto está estructurado en las siguientes fases, siguiendo el ciclo CRISP-DM:\n\nComprensión del Negocio: Definición del problema, el contexto de la mesa de ayuda del CENACE, y los objetivos específicos del proyecto.\nComprensión de los Datos: Recolección y análisis preliminar de la documentación técnica disponible y de los tickets de incidentes históricos.\nPreparación de los Datos: Limpieza, transformación y estructuración de los documentos técnicos y datos de tickets para su posterior uso en el sistema RAG y la base de datos vectorial.\nDesarrollo del Sistema de Help Desk: Implementación del chatbot y la arquitectura de RAG que utilizará el LLM para interpretar las consultas y generar respuestas basadas en la base de conocimientos.\nEvaluación: Validación de la precisión y relevancia de las respuestas generadas por el sistema, asegurando su utilidad para los ingenieros en su trabajo diario.\nImplementación: Despliegue del sistema de Help Desk en un entorno de pruebas del CENACE y posterior integración en el flujo de trabajo de los ingenieros.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con la información técnica del CENACE, optimizando el flujo de trabajo de los ingenieros y facilitando una toma de decisiones más rápida y precisa, particularmente en la zona noroeste del país (Sonora y Sinaloa), donde el proyecto se ha enfocado inicialmente."
  },
  {
    "objectID": "1_comprension.html",
    "href": "1_comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "El Centro Nacional de Control de Energía (CENACE) es el organismo encargado de la planeación y el control operativo del Sistema Eléctrico Nacional (SEN). El CENACE cuenta con una mesa de ayuda (Help Desk), la cual provee soporte y le da seguimiento a incidentes reportados en todo el país. En este trabajo nos restringimos a los incidentes reportados para la zona noroeste, la cual se conforma de los estados de Sonora y Sinaloa. Los incidentes se reportan en formato de tickets y son gestionados por los ingenieros de la organización.\nTeniendo en cuenta la naturaleza crítica del sector energético, la constante evolución tecnológica y la gran cantidad de documentación técnica existente, surge la necesidad de optimizar el acceso a la información. La búsqueda manual de esta información para resolver incidentes puede ser un proceso lento, impactando la eficiencia operativa. Además de aprovechar la base de conocimientos que ya se tiene para proponer soluciones a problemas previamente vistos."
  },
  {
    "objectID": "1_comprension.html#propuesta-de-solución",
    "href": "1_comprension.html#propuesta-de-solución",
    "title": "Comprensión del negocio",
    "section": "0.1. Propuesta de solución",
    "text": "0.1. Propuesta de solución\nProponemos desarrollar un sistema de Help Desk inteligente basado en Inteligencia Artificial Generativa que utilice la metodología de Recuperación Aumentada por Generación (RAG) y modelos de lenguaje grandes (LLM). Este sistema actuará como una herramienta de apoyo para los ingenieros, proporcionando respuestas inmediatas a sus consultas técnicas. El sistema permitirá una clasificación y recuperación de información más eficiente, y generará respuestas contextualizadas a partir de la base de conocimientos interna del CENACE."
  },
  {
    "objectID": "1_comprension.html#objetivos",
    "href": "1_comprension.html#objetivos",
    "title": "Comprensión del negocio",
    "section": "0.2. Objetivos",
    "text": "0.2. Objetivos\nEl objetivo principal es elaborar un sistema de Help Desk que utilice la base de conocimientos del CENACE y modelos de lenguaje grande para que los ingenieros tengan apoyo inmediato y puedan tomar decisiones rápidas al alcance de la mano. Con esto, se espera ahorrar tiempo en la clasificación, recuperación y generación de la información técnica, y a su vez, nutrir la base de conocimientos con las soluciones generadas."
  },
  {
    "objectID": "1_comprension.html#terminología",
    "href": "1_comprension.html#terminología",
    "title": "Comprensión del negocio",
    "section": "0.3. Terminología",
    "text": "0.3. Terminología\n\nRAG (Retrieval-Augmented Generation): Un enfoque de IA que combina la recuperación de información con la generación de lenguaje, para crear respuestas más precisas y contextualizadas.\nLLM (Large Language Model): Modelo de lenguaje grande capaz de comprender y generar texto similar al humano, como el modelo gemma3:4b que se utilizará en el proyecto.\nOllama: Un framework que permite ejecutar modelos de lenguaje grandes de código abierto de forma local.\nVector Embeddings: Representaciones numéricas de texto que capturan su significado semántico, facilitando la búsqueda de información similar.\nBase de datos vectorial: Una base de datos optimizada para almacenar y buscar vector embeddings.\nFastAPI: Un framework web de Python de alto rendimiento para construir APIs.\nMongoDB: Una base de datos NoSQL que se utilizará para almacenar el historial de conversaciones, tickets y la documentación original."
  },
  {
    "objectID": "1_comprension.html#beneficios",
    "href": "1_comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4. Beneficios",
    "text": "0.4. Beneficios\n\nInnovación en el soporte técnico: Introducir un nuevo enfoque para acceder a la información técnica, brindando una experiencia más personalizada y eficiente para los ingenieros.\nOptimización del flujo de trabajo: El sistema permitirá a los ingenieros ahorrar tiempo en la búsqueda de información, lo que se traducirá en una mayor eficiencia operativa y una toma de decisiones más rápida.\nPreservación del conocimiento: El sistema ayuda a estructurar y hacer accesible la vasta base de conocimientos del CENACE, garantizando que el conocimiento institucional no se pierda.\nClasificación y sugerencia automática: El sistema puede ayudar a clasificar los tickets de incidentes y sugerir soluciones, lo que agiliza el proceso de resolución."
  },
  {
    "objectID": "1_comprension.html#costos",
    "href": "1_comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5. Costos",
    "text": "0.5. Costos\n\nTiempo: El proyecto tiene un plazo estimado para desarrollar una versión funcional que pueda ser evaluada y mejorada.\nFinancieros: Aunque se utilizan modelos y herramientas de código abierto, se consideran costos asociados al hardware necesario para ejecutar los modelos de manera local (servidores, etc.)."
  },
  {
    "objectID": "3_preparacion.html",
    "href": "3_preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "La fase de preparación de datos es crucial para el funcionamiento del sistema RAG. Su objetivo es convertir la información no estructurada, proveniente de los documentos técnicos del CENACE, en un formato que permita una búsqueda eficiente y una recuperación semántica de alta calidad. Este proceso se divide en tres etapas principales: extracción, transformación y carga de los datos."
  },
  {
    "objectID": "3_preparacion.html#extracción-de-los-datos",
    "href": "3_preparacion.html#extracción-de-los-datos",
    "title": "Preparación de los datos",
    "section": "1. Extracción de los datos",
    "text": "1. Extracción de los datos\nLa principal fuente de información son los documentos técnicos en formato PDF, así como la base de datos de incidentes resueltos. Para la ingesta de documentos, el sistema utiliza el módulo doccollection.py, específicamente la clase DisjointCollection.\n\nIngesta de archivos: Los documentos PDF se cargan y procesan de forma individual utilizando la librería PyPDF2. Se extrae el texto de cada página, junto con sus metadatos inherentes (título, autor, etc.).\nFragmentación de texto (chunking): El texto extraído se divide en fragmentos lógicos o “chunks” para preservar el contexto de la información. El TextSplitter del módulo doccollection está configurado con los siguientes parámetros para optimizar la cohesión del texto:\n\nTamaño del chunk: 1500 caracteres.\nSolapamiento (overlap): 200 caracteres. Este solapamiento asegura que la información contextual clave no se pierda en los límites de cada fragmento.\n\nAsignación de metadatos: A cada chunk se ke asignan metadatos esenciales, como el nombre del archivo de origen (filename), el número de página (page_number), y un identificador único de referencia (reference). Estos metadatos son vitales para referenciar las fuentes en las respuestas del LLM, evitando alucinaciones y permitiendo al usuario validar la información."
  },
  {
    "objectID": "3_preparacion.html#transformación-de-los-datos",
    "href": "3_preparacion.html#transformación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Transformación de los datos",
    "text": "2. Transformación de los datos\nUna vez que los documentos se han dividido en chunks, se transforman en una representación numérica que la computadora puede entender y procesar eficientemente.\n\nVectorización: Cada chunk de texto es procesado por el modelo de embeddings bge-m3:latest, implementado en la clase OllamaEmbedder.\nRepresentación vectorial: El modelo convierte el texto en un vector numérico de alta dimensión. Estos vectores capturan el significado semántico del texto; los chunks con un significado similar se agrupan en un espacio vectorial. Este enfoque va más allá de la simple búsqueda por palabres clave, ya que permite al sistema encontrar información relevante incluso si la consulta utiliza un vocabulario o una sintaxis diferente."
  },
  {
    "objectID": "3_preparacion.html#carga-de-los-datos",
    "href": "3_preparacion.html#carga-de-los-datos",
    "title": "Preparación de los datos",
    "section": "3. Carga de los datos",
    "text": "3. Carga de los datos\nLos vectores generados se almacenan en una base de datos vectorial optimizada para la búsqueda de similitud.\n\nBase de datos vectorial: Se utiliza FAISS (Facebook AI Similarity Search), una librería de código abierto para la búsqueda eficiente en grandes conjuntos de vectores. FAISS indexa los vectores de manera que las consultas de similitud se puedan ejecutar en milisegundos.\nPersistencia: La clase FAISSVectorStore es responsable de la carga y el almacenamiento de los vectores. El índice de FAISS se guarda en un archivo local (index.faiss), mientras que los metadatos de los chunks se almacenan en un diccionario (index.pkl).\nBúsqueda semántica: Una vez cargados los datos, la base de datos vectorial está lista para recibir consultas. Cuando un usuario envía una pregunta, esta se convierte en un vector, que luego se utiliza para encontrar los vectores más cercanos (los chunks más relevantes) en el índice de FAISS."
  },
  {
    "objectID": "3_preparacion.html#flujo-de-preparación",
    "href": "3_preparacion.html#flujo-de-preparación",
    "title": "Preparación de los datos",
    "section": "4. Flujo de preparación",
    "text": "4. Flujo de preparación\nEl proceso completo es orquestado por la clase RAG y es ejecutado como un flujo de trabajo de indexación:\n\nSolicitud de carga: El usuario sube un documento PDF a través del endpoint /documents.\nManejo de la ingesta: El rag.py recibe el archivo y delega su procesamiento al doccollection.\nGeneración de chunks y metadatos: doccollection lee el PDF, extrae el texto y lo divide en chunks. Asigna metadatos como el ID del documento, el nombre del archivo y el número de página a cada uno de ellos.\nVectorización y almacenamiento: Cada chunk y sus metadatos asociados se envían al vectorstore, que a su vez utiliza el embedder para obtener su representación vectorial. Finalmente, los vectores se agregan al índice de FAISS, asegurando que el conocimiento esté disponible para futuras consultas."
  },
  {
    "objectID": "5_despliegue.html",
    "href": "5_despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "5_despliegue.html#revisión-del-proceso",
    "href": "5_despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la propuesta de una solución a la fuga de conocimientos que presentaba la empresa. Una alternativa que mitigara este problema y además, fuera punto de partida para futuras implementaciones, permitiendo la persistencia del conocimiento y el constante crecimiento de la base de conocimientos del chatbot.\n\n\nConsiderando los avances y aprendizajes obtenidos, se evaluó la opción de pasar a una fase de implementación en un entorno virtual donde se puedan hacer pruebas y se garantice la escalabilidad. Esto permitirá validar el desempeño del sistema con un volumen de datos y usuarios más grande, preparando el terreno para un despliegue completo en producción de tipo piloto."
  },
  {
    "objectID": "5_despliegue.html#plan-de-implementación",
    "href": "5_despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2. Plan de implementación",
    "text": "2. Plan de implementación\nEl plan de despliegue se centra en migrar la arquitectura de desarrollo a un entorno de producción escalable de tipo piloto, manteniendo la modularidad del sistema y optimizando el rendimiento.\n\n2.1. Arquitectura de despliegue y conexión\nLa arquitectura final para el despliegue estará compuesta por los siguientes componentes clave, implementados en un entorno de producción como Databricks, Azure o una plataforma similar:\n\nServidor de la API (backend): Implementación de la API desarrollada en FastAPI (main.py, chat.py) en un servidor escalable. Este servidor manejará las peticiones de los usuarios, coordinará las operaciones del RAG y se comunicará con la base de datos y el LLM.\nModelo de lenguaje (LLM): El modelo de lenguaje gemma3:4b se desplegará en un servidor con aceleración por GPU para asegurar un rendimiento óptimo en la generación de respuestas.\nBase de datos vectorial: La base de datos vectorial de FAISS, que almacena los embeddings de los documentos (vectorstore.py), se mantendrá, pero se integrará con un sistema de almacenamiento persistente y escalable en la nube para garantizar la disponibilidad y el rendimiento.\nBase de datos de historial y tickets (MongoDB): La base de datos de MongoDB, utilizada para almacenar el historial de conversaciones y la gestión de tickets, se migrará a un servicio de bases de datos gestionado en la nube para asegurar la persistencia y la seguridad de los datos.\nInterfaz del usuario (frontend): La presentación del sistema se monta de la mano con FastAPI aprovechando su clase interna Jinja2Templates.\n\n\n\n2.2. Desarrollo del frontend\nLa interfaz se desarrolló con ayuda de 3 tipos de archivos básicos para páginas web. Múltiples archivos de JavaScript, un archivo HTML y estilos modernos que mejoraran la experiencia del usuario gracias a un archivo CSS.\n\n2.2.1. Interfaz del usuario final\nA continuación, se presentan capturas de pantalla de las principales secciones de la interfaz de usuario final desplegada, mostrando las funcionalidades clave del sistema:\n\nInicio de sesión: Sección inicial donde el usuario puede acceder a su sesión. Por delimitaciones del proyecto, una sesión más segura no fue implementada. \nPestaña de Chat: Interfaz principal de conversación donde el usuario interactúa con el LLM para resolver incidentes. Muestra el historial de mensajes y las referencias utilizadas por el modelo. \nPestaña de Documentos: Permite a los usuarios cargar nuevos documentos PDF a la base de conocimientos y visualizar los documentos ya procesados. \nPestaña de Soluciones: Muestra las soluciones que han sido marcadas como “útiles” por los usuarios (a través del botón “like”). Desde aquí se pueden procesar estas soluciones para re-indexarlas en la base vectorial. \nPestaña de Tickets: Presenta la lista de tickets registrados, permitiendo visualizar su detalle (título, descripción y categoría) y llevar un ticket específico a una nueva conversación en el chat, iterando hasta llegar a la solución del ticket. \n\n\n\n\n2.3. Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.4. Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.5. Flujo de despliegue en el ambiente virtual\nEl despliegue del sistema en un ambiente virtual o de nube (como el entorno piloto propuesto) implica una transición desde la configuración de desarrollo local hacia una arquitectura gestionada, escalable y robusta. El flujo general se compone de las siguientes etapas conceptuales:\n\nContenerización de la Aplicación: El primer paso consiste en empaquetar la aplicación backend de FastAPI, junto con todas sus dependencias Python, en una imagen de contenedor (utilizando Docker o Podman). Esto garantiza un entorno de ejecución consistente y aislado, independientemente de la infraestructura subyacente. Si se opta por auto-alojar el LLM con Ollama en producción, este también podría ser contenerizado. Los detalles técnicos para construir estas imágenes se basan en la configuración del entorno descrita en el manual de instalación.\nConfiguración de Servicios Gestionados: A diferencia del entorno local, en un ambiente virtual o de nube se aprovecharán servicios gestionados para componentes clave. Esto incluye la configuración de una instancia de MongoDB Atlas (o similar) para la persistencia del historial y los tickets, y potencialmente un servicio de inferencia de modelos con aceleración GPU para hospedar gemma3:4b, asegurando rendimiento y escalabilidad. La conexión a estos servicios se configurará mediante variables de entorno, similar a lo descrito en el archivo .env del manual. La base vectorial de FAISS requerirá un sistema de almacenamiento persistente asociado al contenedor o servicio que la gestione.\nOrquestación de Contenedores: Para gestionar la aplicación contenerizada (y potencialmente el LLM), se utilizará una herramienta de orquestación (como Podman). La orquestación facilitará el despliegue de múltiples instancias de la API para escalabilidad horizontal, gestionará el balanceo de carga entre ellas y asegurará la alta disponibilidad mediante la recuperación automática en caso de fallos.\nImplementación de Monitoreo y Logging: Se configurarán herramientas de monitoreo específicas del entorno de nube para rastrear el rendimiento de la API, la latencia del LLM, el uso de recursos (CPU, GPU, memoria) y el estado de las bases de datos. Se establecerán sistemas de logging centralizados para capturar los registros de la aplicación (como los generados en nohup.out localmente) y facilitar la depuración y el análisis de errores en tiempo real, alineados con las métricas definidas en el plan de monitoreo.\nIntegración y Pruebas Finales: Una vez desplegada y configurada la API en el nuevo entorno, el paso final es la integración con el sistema de Help Desk del CENACE (si aplica para la fase piloto). Se realizarán pruebas funcionales y de carga en este entorno para validar la correcta operación antes de ponerlo a disposición de los usuarios piloto.\n\nPara obtener las instrucciones técnicas detalladas, comandos específicos y configuraciones de ejemplo para cada una de estas etapas, consulte la sección “Manual de instalación y despliegue” en la Documentación Técnica (6_documentacion.qmd)."
  }
]